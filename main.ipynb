{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "74ab7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import pymupdf4llm\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# pip install llama_index\n",
    "\n",
    "# md_text = pymupdf4llm.to_markdown(fileinquestion)\n",
    "# pathlib.Path(\"output.md\").write_bytes(md_text.encode())\n",
    "# llama_reader = pymupdf4llm.LlamaMarkdownReader()\n",
    "# print(llama_reader())\n",
    "# llama_docs = llama_reader.load_data(fileinquestion)\n",
    "\n",
    "#tesseract ocr\n",
    "# english\n",
    "# russian\n",
    "# chinese (new)\n",
    "# mandarin (traditional)\n",
    "# japanese\n",
    "# Hindi\n",
    "# Arabic\n",
    "# French\n",
    "# Hebrew\n",
    "# German\n",
    "# Korean\n",
    "# Italian\n",
    "# Polish\n",
    "# Portugese\n",
    "# Spanish \n",
    "# Indonesian\n",
    "#turkish\n",
    "# Urdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "92c6f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_blocks_and_distributions(html_string): # Match each full <p> block\n",
    "    block_pattern = re.findall(\n",
    "        r'<p style=\"top:([\\d.]+)pt;left:([\\d.]+)pt;line-height:([\\d.]+)pt\">(.*?)</p>',\n",
    "        html_string,\n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "    results , font_sizes, line_heights = [],[],[]\n",
    "\n",
    "    for top, left, line_height, inner_html in block_pattern:\n",
    "        font_match = re.search(r'font-size:([\\d.]+)pt', inner_html)\n",
    "        font_size = float(font_match.group(1)) if font_match else None\n",
    "\n",
    "        font_family_match = re.search(r'font-family:([^;\"]+)', inner_html)\n",
    "        font_family = font_family_match.group(1).strip() if font_family_match else None\n",
    "\n",
    "        is_bold = bool(re.search(r'<b>', inner_html))\n",
    "        is_italic = bool(re.search(r'<i>', inner_html))\n",
    "\n",
    "        clean_text = re.sub(r'<[^>]+>', '', inner_html).strip()\n",
    "        word_count = len(clean_text.split())\n",
    "        word_density = word_count / float(line_height) if float(line_height) > 0 else 0\n",
    "\n",
    "        results.append({\n",
    "            \"top\": float(top),\n",
    "            \"left\": float(left),\n",
    "            \"line_height\": float(line_height),\n",
    "            \"font_size\": font_size,\n",
    "            \"font_family\": font_family,\n",
    "            \"bold\": is_bold,\n",
    "            \"italic\": is_italic,\n",
    "            \"text\": clean_text,\n",
    "            \"word_count\": word_count,\n",
    "            \"word_density\": word_density\n",
    "        })\n",
    "        line_heights.append(float(line_height))\n",
    "        if font_match:\n",
    "            font_size = float(font_match.group(1))\n",
    "            font_sizes.append(font_size)\n",
    "        else:\n",
    "            print(\"⚠️ Font size not found in:\", inner_html[:60])\n",
    "        line_heights.append(float(line_height))\n",
    "    return results, Counter(font_sizes), Counter(line_heights)\n",
    "\n",
    "heading_patterns = [\n",
    "    r'^\\d+\\.',                # 1.\n",
    "    r'^\\d+\\.\\d+(\\.\\d+)*',     # 1.1, 2.3.4\n",
    "    r'^\\d+\\)',                # 1)\n",
    "    r'^[A-Z]\\.',              # A.\n",
    "    r'^[a-z]\\)',              # a)\n",
    "    r'^[ivxlcdm]+\\)',         # i), ii), iv)\n",
    "    r'^[IVXLCDM]+\\)',         # I), II)\n",
    "    r'^[a-z]\\.',              # a.\n",
    "    r'^[IVXLCDM]+\\.',         # I.\n",
    "    r'^•\\s*',                 # • bullet\n",
    "    r'^-\\s*',                 # - bullet\n",
    "    r'^\\*\\s*'                 # * bullet\n",
    "]\n",
    "compiled_patterns = [re.compile(p) for p in heading_patterns]\n",
    "\n",
    "def extract_crucial_pattern_lines(dflist):\n",
    "    dfcrucial_pattern = []\n",
    "\n",
    "    for page_num, df in enumerate(dflist):\n",
    "        if df.empty or 'text' not in df.columns:\n",
    "            continue\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            first_word = row['text'].strip().split(' ')[0]\n",
    "            for pattern in compiled_patterns:\n",
    "                if pattern.match(first_word):\n",
    "                    row_copy = row.copy()\n",
    "                    row_copy['page'] = page_num\n",
    "                    dfcrucial_pattern.append(row_copy)\n",
    "                    break  # stop at first match\n",
    "\n",
    "    if dfcrucial_pattern:\n",
    "        return pd.DataFrame(dfcrucial_pattern).reset_index(drop=True)\n",
    "    else:\n",
    "        return pd.DataFrame()  # empty if nothing found\n",
    "\n",
    "def find_alternate_page_repeats_splitold(dflist, mid):\n",
    "    def get_matches(df_base, df1, df2):\n",
    "        matched = []\n",
    "        for _, row in df_base.iterrows():\n",
    "            text = row['text'].strip()\n",
    "            top = row['top']\n",
    "\n",
    "            def match(df):\n",
    "                return any(\n",
    "                    (abs(top - r['top']) <= 1.0) and (r['text'].strip() == text)\n",
    "                    for _, r in df.iterrows()\n",
    "                )\n",
    "\n",
    "            if match(df1) and match(df2):\n",
    "                matched.append(row)\n",
    "\n",
    "        return pd.DataFrame(matched)\n",
    "\n",
    "    even_matches, odd_matches = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Mid page even\n",
    "    if mid % 2 == 0:\n",
    "        page_even = dflist[mid]\n",
    "        before_even = dflist[mid - 2] if mid - 2 >= 0 else None\n",
    "        after_even = dflist[mid + 2] if mid + 2 < len(dflist) else None\n",
    "\n",
    "        if before_even is not None and after_even is not None:\n",
    "            even_matches = get_matches(page_even, before_even, after_even)\n",
    "        elif before_even is not None:\n",
    "            even_matches = get_matches(page_even, before_even)\n",
    "        elif after_even is not None:\n",
    "            even_matches = get_matches(page_even, after_even)\n",
    "\n",
    "    # Mid - 1 page odd\n",
    "    if mid - 1 >= 0 and (mid - 1) % 2 == 1:\n",
    "        odd_page_idx = mid - 1\n",
    "        page_odd = dflist[odd_page_idx]\n",
    "        before_odd = dflist[odd_page_idx - 2] if odd_page_idx - 2 >= 0 else None\n",
    "        after_odd = dflist[odd_page_idx + 2] if odd_page_idx + 2 < len(dflist) else None\n",
    "\n",
    "        if before_odd is not None and after_odd is not None:\n",
    "            odd_matches = get_matches(page_odd, before_odd, after_odd)\n",
    "        elif before_odd is not None:\n",
    "            odd_matches = get_matches(page_odd, before_odd)\n",
    "        elif after_odd is not None:\n",
    "            odd_matches = get_matches(page_odd, after_odd)\n",
    "    return even_matches, odd_matches\n",
    "\n",
    "def find_alternate_page_repeats_split(dflist, mid):\n",
    "    def is_valid_df(df):\n",
    "        return df is not None and not df.empty\n",
    "\n",
    "    def get_matches(df_base, *others):\n",
    "        matched = []\n",
    "        for _, row in df_base.iterrows():\n",
    "            text = row['text'].strip()\n",
    "            top = row['top']\n",
    "\n",
    "            def match(df):\n",
    "                return any(\n",
    "                    (abs(top - r['top']) <= 1.0) and (r['text'].strip() == text)\n",
    "                    for _, r in df.iterrows()\n",
    "                )\n",
    "\n",
    "            if all(match(df) for df in others if is_valid_df(df)):\n",
    "                matched.append(row)\n",
    "\n",
    "        return pd.DataFrame(matched)\n",
    "\n",
    "    even_matches, odd_matches = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Even page block\n",
    "    if mid % 2 == 0 and mid < len(dflist):\n",
    "        page_even = dflist[mid]\n",
    "        before_even = dflist[mid - 2] if mid - 2 >= 0 else None\n",
    "        after_even = dflist[mid + 2] if mid + 2 < len(dflist) else None\n",
    "\n",
    "        if is_valid_df(before_even) and is_valid_df(after_even):\n",
    "            even_matches = get_matches(page_even, before_even, after_even)\n",
    "        elif is_valid_df(before_even):\n",
    "            even_matches = get_matches(page_even, before_even)\n",
    "        elif is_valid_df(after_even):\n",
    "            even_matches = get_matches(page_even, after_even)\n",
    "\n",
    "    # Odd page block\n",
    "    odd_page_idx = mid - 1\n",
    "    if odd_page_idx >= 0 and odd_page_idx % 2 == 1 and odd_page_idx < len(dflist):\n",
    "        page_odd = dflist[odd_page_idx]\n",
    "        before_odd = dflist[odd_page_idx - 2] if odd_page_idx - 2 >= 0 else None\n",
    "        after_odd = dflist[odd_page_idx + 2] if odd_page_idx + 2 < len(dflist) else None\n",
    "\n",
    "        if is_valid_df(before_odd) and is_valid_df(after_odd):\n",
    "            odd_matches = get_matches(page_odd, before_odd, after_odd)\n",
    "        elif is_valid_df(before_odd):\n",
    "            odd_matches = get_matches(page_odd, before_odd)\n",
    "        elif is_valid_df(after_odd):\n",
    "            odd_matches = get_matches(page_odd, after_odd)\n",
    "\n",
    "    return even_matches.reset_index(drop=True), odd_matches.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "def clean_pages_of_even_odd_repeats(dflist, even_df, odd_df, tolerance=1.0):\n",
    "    \"\"\"\n",
    "    Cleans even-indexed pages using even_df and odd-indexed pages using odd_df.\n",
    "    If even_df == odd_df, treats all pages the same.\n",
    "    \"\"\"\n",
    "    def to_clean_set(df):\n",
    "        return set((row['text'].strip(), round(row['top'], 1)) for _, row in df.iterrows())\n",
    "\n",
    "    even_set = to_clean_set(even_df)\n",
    "    odd_set = to_clean_set(odd_df)\n",
    "    same_repeats = even_set == odd_set\n",
    "\n",
    "    cleaned_pages = []\n",
    "\n",
    "    for idx, df in enumerate(dflist):\n",
    "        compare_set = even_set if same_repeats or idx % 2 == 0 else odd_set\n",
    "\n",
    "        mask = df.apply(\n",
    "            lambda row: not any(\n",
    "                (abs(row['top'] - rep_top) <= tolerance) and (row['text'].strip() == rep_text)\n",
    "                for rep_text, rep_top in compare_set\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        cleaned_df = df[mask].reset_index(drop=True)\n",
    "        cleaned_pages.append(cleaned_df)\n",
    "    return cleaned_pages\n",
    "def merge_paragraphs_by_font(\n",
    "    df,\n",
    "    font_min=8.0,\n",
    "    font_max=14.0,\n",
    "    font_tolerance=1.0,\n",
    "    line_gap=2.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Groups consecutive lines (in original order) with similar font size and small vertical gap into paragraphs.\n",
    "    No sorting is applied — assumes df is already in reading order.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with 'top', 'font_size', 'line_height', 'text' columns.\n",
    "        font_min (float): Minimum font size to allow.\n",
    "        font_max (float): Maximum font size to allow.\n",
    "        font_tolerance (float): Allowed deviation in font size between lines.\n",
    "        line_gap (float): Maximum allowed vertical gap to group lines.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: List of merged paragraph dictionaries.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return []\n",
    "\n",
    "    # Filter font range\n",
    "    df = df[(df['font_size'] >= font_min) & (df['font_size'] <= font_max)].reset_index(drop=True)\n",
    "    if df.empty:\n",
    "        return []\n",
    "\n",
    "    paragraphs = []\n",
    "    current_para = {\n",
    "        'top': df.loc[0, 'top'],\n",
    "        'font_size': df.loc[0, 'font_size'],\n",
    "        'text': df.loc[0, 'text'],\n",
    "        'line_count': 1\n",
    "    }\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        prev = df.loc[i - 1]\n",
    "        curr = df.loc[i]\n",
    "    \n",
    "        same_font = abs(curr['font_size'] - prev['font_size']) <= font_tolerance\n",
    "        small_gap = abs(curr['top'] - prev['top']) <= (prev['line_height'] + line_gap)\n",
    "\n",
    "        if same_font and small_gap:\n",
    "            current_para['text'] += ' ' + curr['text']\n",
    "            current_para['line_count'] += 1\n",
    "        else:\n",
    "            paragraphs.append(current_para)\n",
    "            current_para = {\n",
    "                'top': curr['top'],\n",
    "                'font_size': curr['font_size'],\n",
    "                'text': curr['text'],\n",
    "                'line_count': 1\n",
    "            }\n",
    "\n",
    "    paragraphs.append(current_para)\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "def get_top_fonts(font_counter, top_n=3):\n",
    "    \"\"\"Returns the top_n most frequent font sizes in descending order.\"\"\"\n",
    "    font_freq = font_counter.most_common(top_n)\n",
    "    return [size for size, _ in font_freq]\n",
    "def create_font_level_map(font_counter):\n",
    "    \"\"\"Maps largest font sizes to H1, H2, H3; everything else is Body.\"\"\"\n",
    "    sorted_fonts = sorted(font_counter.keys(), reverse=True)\n",
    "    return {\n",
    "        sorted_fonts[0]: \"H1\" if len(sorted_fonts) > 0 else None,\n",
    "        sorted_fonts[1]: \"H2\" if len(sorted_fonts) > 1 else None,\n",
    "        sorted_fonts[2]: \"H3\" if len(sorted_fonts) > 2 else None\n",
    "    }\n",
    "def classify_font_size(size, font_to_level):\n",
    "    \"\"\"Returns level for a given font size using pre-defined mapping.\"\"\"\n",
    "    return font_to_level.get(size, \"Body\")\n",
    "def label_font_levels(cleaned_dflist, font_to_level):\n",
    "    \"\"\"Adds a 'level' column to each cleaned page's DataFrame.\"\"\"\n",
    "    for i, df in enumerate(cleaned_dflist):\n",
    "        if 'font_size' not in df.columns:\n",
    "            continue\n",
    "        cleaned_dflist[i]['level'] = df['font_size'].apply(lambda s: classify_font_size(s, font_to_level))\n",
    "    return cleaned_dflist\n",
    "\n",
    "def extract_heading_summary(cleaned_dflist, levels=('H1', 'H2')):\n",
    "    \"\"\"Extracts rows marked as H1 or H2 and returns a summary DataFrame.\"\"\"\n",
    "    heading_rows = []\n",
    "\n",
    "    for page_num, df in enumerate(cleaned_dflist):\n",
    "        if 'level' not in df.columns:\n",
    "            continue\n",
    "        headings_df = df[df['level'].isin(levels)].copy()\n",
    "        headings_df['page_num'] = page_num\n",
    "        heading_rows.append(headings_df[['page_num', 'level', 'text']])\n",
    "\n",
    "    return pd.concat(heading_rows, ignore_index=True) if heading_rows else pd.DataFrame()\n",
    "\n",
    "def filter_gibberish_rows(df, text_column='text', min_alpha_ratio=0.3, min_length=3):\n",
    "    \"\"\"\n",
    "    Removes rows from df where the text is mostly non-alphabetic (e.g., --------, ...., ====),\n",
    "    or is too short to be meaningful.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame with a text column.\n",
    "        text_column (str): Column name that contains the text.\n",
    "        min_alpha_ratio (float): Minimum ratio of alphabetic chars to keep the row.\n",
    "        min_length (int): Minimum total length of string to keep it.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with gibberish lines removed.\n",
    "    \"\"\"\n",
    "    def is_gibberish(text):\n",
    "        if len(text.strip()) < min_length:\n",
    "            return True\n",
    "        alpha_count = sum(c.isalpha() for c in text)\n",
    "        return (alpha_count / len(text)) < min_alpha_ratio\n",
    "\n",
    "    mask = df[text_column].apply(lambda t: not is_gibberish(t))\n",
    "    return df[mask].reset_index(drop=True)\n",
    "\n",
    "def extract_table_with_titleold(page, max_title_distance=50): #################Old\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Extracts the first table and its potential title from a PyMuPDF page.\n",
    "\n",
    "    Parameters:\n",
    "        page (fitz.Page): The PDF page object.\n",
    "        max_title_distance (float): Max vertical distance (in pt) to search above the table for a title.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: The extracted table (if found), else None.\n",
    "            - str: The possible table title (if found), else None.\n",
    "    \"\"\"\n",
    "    tables = page.find_tables()\n",
    "    print(f\"{len(tables.tables)} table(s) found.\")\n",
    "\n",
    "    if not tables.tables:\n",
    "        return None, None\n",
    "\n",
    "    table = tables[0]\n",
    "    table_data = table.extract()\n",
    "    table_df = pd.DataFrame(table_data)\n",
    "\n",
    "    print(\"First table extracted:\")\n",
    "    pprint(table_data)\n",
    "\n",
    "    # Look for text just above the table\n",
    "    bbox = table.bbox\n",
    "    text_dict = page.get_text(\"dict\")\n",
    "    possible_titles = []\n",
    "\n",
    "    for block in text_dict.get(\"blocks\", []):\n",
    "        if \"lines\" in block and block[\"bbox\"][3] < bbox[1]:  # Above the table\n",
    "            if abs(block[\"bbox\"][3] - bbox[1]) <= max_title_distance:\n",
    "                text = \" \".join(\n",
    "                    span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"]\n",
    "                ).strip()\n",
    "                if text:\n",
    "                    possible_titles.append((block[\"bbox\"][1], text))\n",
    "\n",
    "    # Closest first\n",
    "    possible_titles.sort(key=lambda x: -x[0])\n",
    "\n",
    "    if possible_titles:\n",
    "        title = possible_titles[0][1]\n",
    "        print(\"Possible table title:\", title)\n",
    "    else:\n",
    "        title = None\n",
    "        print(\"No clear title found above the table.\")\n",
    "\n",
    "    return table_df, title\n",
    "\n",
    "def extract_table_with_title(page, max_distance=50):\n",
    "    \"\"\"\n",
    "    Extracts the first table and possible title text (above or below),\n",
    "    along with formatting metadata (font size, bold, italic, underline).\n",
    "\n",
    "    Parameters:\n",
    "        page (fitz.Page): The PDF page object.\n",
    "        max_distance (float): Max vertical distance in points to search above or below the table.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: The extracted table.\n",
    "            - dict: Info about possible title (or footer), with formatting.\n",
    "    \"\"\"\n",
    "    tables = page.find_tables()\n",
    "    print(f\"{len(tables.tables)} table(s) found.\")\n",
    "\n",
    "    if not tables.tables:\n",
    "        return None, None\n",
    "\n",
    "    table = tables[0]\n",
    "    table_data = table.extract()\n",
    "    table_df = pd.DataFrame(table_data)\n",
    "    print(\"First table extracted:\")\n",
    "    pprint(table_data)\n",
    "\n",
    "    bbox = table.bbox  # (x0, y0, x1, y1)\n",
    "    text_dict = page.get_text(\"dict\")\n",
    "\n",
    "    def collect_nearby_text(blocks, ref_y, direction=\"above\"):\n",
    "        nearby = []\n",
    "        for block in blocks:\n",
    "            if \"lines\" not in block:\n",
    "                continue\n",
    "            block_y = block[\"bbox\"][3] if direction == \"above\" else block[\"bbox\"][1]\n",
    "            is_above = direction == \"above\" and block_y < ref_y and (ref_y - block_y) <= max_distance\n",
    "            is_below = direction == \"below\" and block_y > ref_y and (block_y - ref_y) <= max_distance\n",
    "            if is_above or is_below:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        text = span[\"text\"].strip()\n",
    "                        if text:\n",
    "                            nearby.append({\n",
    "                                \"y\": block_y,\n",
    "                                \"text\": text,\n",
    "                                \"font_size\": span.get(\"size\"),\n",
    "                                \"bold\": \"bold\" in span.get(\"font\", \"\").lower(),\n",
    "                                \"italic\": \"italic\" in span.get(\"font\", \"\").lower(),\n",
    "                                \"underline\": span.get(\"flags\", 0) & 4 != 0\n",
    "                            })\n",
    "        return nearby\n",
    "\n",
    "    above_texts = collect_nearby_text(text_dict.get(\"blocks\", []), ref_y=bbox[1], direction=\"above\")\n",
    "    below_texts = collect_nearby_text(text_dict.get(\"blocks\", []), ref_y=bbox[3], direction=\"below\")\n",
    "\n",
    "    def rank_candidates(candidates):\n",
    "        # Prioritize bold or larger font entries\n",
    "        sorted_cand = sorted(\n",
    "            candidates,\n",
    "            key=lambda x: (x[\"bold\"], x[\"font_size\"] or 0, -abs(x[\"y\"])),  # bold > font size > closest\n",
    "            reverse=True\n",
    "        )\n",
    "        return sorted_cand[0] if sorted_cand else None\n",
    "\n",
    "    title_above = rank_candidates(above_texts)\n",
    "    title_below = rank_candidates(below_texts)\n",
    "\n",
    "    # Choose the better one by some priority (e.g. favor above)\n",
    "    chosen_title = title_above or title_below\n",
    "\n",
    "    if chosen_title:\n",
    "        print(\"Possible table label:\")\n",
    "        pprint(chosen_title)\n",
    "    else:\n",
    "        print(\"No meaningful title/label found near the table.\")\n",
    "\n",
    "    return table_df, chosen_title\n",
    "\n",
    "def merge_consecutive_same_font_and_leftoldnew(dflist, target_fonts):\n",
    "    \"\"\"\n",
    "    Merges lines in-place within each DataFrame in dflist where:\n",
    "    - font_size ∈ target_fonts\n",
    "    - consecutive lines share same font_size and left\n",
    "    Adds a 'parabool' column to mark whether a line is a merged paragraph.\n",
    "    \n",
    "    Parameters:\n",
    "        dflist (list of pd.DataFrame): Each DataFrame is a page.\n",
    "        target_fonts (list): List of font sizes to consider for paragraph merging.\n",
    "    \n",
    "    Returns:\n",
    "        list of pd.DataFrame: Modified DataFrames with merged paragraphs and 'parabool' flag.\n",
    "    \"\"\"\n",
    "    updated_dflist = []\n",
    "\n",
    "    for df in dflist:\n",
    "        if df.empty or 'left' not in df.columns or 'font_size' not in df.columns:\n",
    "            df['parabool'] = False\n",
    "            updated_dflist.append(df)\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df['parabool'] = False\n",
    "        drop_indices = set()\n",
    "\n",
    "        for left_value in sorted(df['left'].unique()):\n",
    "            # Get indices where left matches\n",
    "            group_df = df[(df['left'] == left_value) & (df['font_size'].isin(target_fonts))]\n",
    "            indices = group_df.index.tolist()\n",
    "            i = 0\n",
    "\n",
    "            while i < len(indices):\n",
    "                current_idx = indices[i]\n",
    "                current_font = df.loc[current_idx, 'font_size']\n",
    "                current_text = df.loc[current_idx, 'text']\n",
    "\n",
    "                merged = False\n",
    "                j = i + 1\n",
    "\n",
    "                while j < len(indices):\n",
    "                    next_idx = indices[j]\n",
    "                    next_font = df.loc[next_idx, 'font_size']\n",
    "\n",
    "                    if next_font == current_font:\n",
    "                        # Merge text\n",
    "                        df.at[current_idx, 'text'] += ' ' + df.loc[next_idx, 'text']\n",
    "                        drop_indices.add(next_idx)\n",
    "                        merged = True\n",
    "                        j += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if merged:\n",
    "                    df.at[current_idx, 'parabool'] = True\n",
    "                else:\n",
    "                    df.at[current_idx, 'parabool'] = False\n",
    "\n",
    "                i = j  # skip merged rows\n",
    "\n",
    "        # Drop merged lines\n",
    "        df = df.drop(index=list(drop_indices)).reset_index(drop=True)\n",
    "        updated_dflist.append(df)\n",
    "\n",
    "    return updated_dflist\n",
    "def merge_consecutive_same_font_and_left(dflist, target_fonts):\n",
    "    \"\"\"\n",
    "    Merges lines in-place within each DataFrame in dflist where:\n",
    "    - font_size ∈ target_fonts\n",
    "    - consecutive lines share same font_size and left\n",
    "    - lines are NOT bold or italic\n",
    "    Adds a 'parabool' column to mark whether a line is a merged paragraph.\n",
    "\n",
    "    Parameters:\n",
    "        dflist (list of pd.DataFrame): Each DataFrame is a page.\n",
    "        target_fonts (list): List of font sizes to consider for paragraph merging.\n",
    "\n",
    "    Returns:\n",
    "        list of pd.DataFrame: Modified DataFrames with merged paragraphs and 'parabool' flag.\n",
    "    \"\"\"\n",
    "    updated_dflist = []\n",
    "\n",
    "    for df in dflist:\n",
    "        if df.empty or 'left' not in df.columns or 'font_size' not in df.columns:\n",
    "            df['parabool'] = False\n",
    "            updated_dflist.append(df)\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df['parabool'] = False\n",
    "        drop_indices = set()\n",
    "\n",
    "        for left_value in sorted(df['left'].unique()):\n",
    "            group_df = df[(df['left'] == left_value) & (df['font_size'].isin(target_fonts))]\n",
    "            indices = group_df.index.tolist()\n",
    "            i = 0\n",
    "\n",
    "            while i < len(indices):\n",
    "                current_idx = indices[i]\n",
    "                current_font = df.loc[current_idx, 'font_size']\n",
    "\n",
    "                # Skip bold/italic rows\n",
    "                if df.loc[current_idx, 'bold'] : #or df.loc[current_idx, 'italic']: #ignore the italics for now\n",
    "                    df.at[current_idx, 'parabool'] = False\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                j = i + 1\n",
    "                merged = False\n",
    "\n",
    "                while j < len(indices):\n",
    "                    next_idx = indices[j]\n",
    "                    next_font = df.loc[next_idx, 'font_size']\n",
    "\n",
    "                    # Stop if font differs or next is bold/italic\n",
    "                    if next_font != current_font or df.loc[next_idx, 'bold'] or df.loc[next_idx, 'italic']:\n",
    "                        break\n",
    "\n",
    "                    # Merge\n",
    "                    df.at[current_idx, 'text'] += ' ' + df.loc[next_idx, 'text']\n",
    "                    drop_indices.add(next_idx)\n",
    "                    merged = True\n",
    "                    j += 1\n",
    "\n",
    "                df.at[current_idx, 'parabool'] = merged\n",
    "                i = j\n",
    "\n",
    "        # Drop merged rows\n",
    "        df = df.drop(index=list(drop_indices)).reset_index(drop=True)\n",
    "        updated_dflist.append(df)\n",
    "\n",
    "    return updated_dflist\n",
    "\n",
    "\n",
    "def generate_font_stats(dflist):\n",
    "    all_stats = []\n",
    "\n",
    "    for i, df in enumerate(dflist):\n",
    "        if df.empty or 'font_size' not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Group by font size and family\n",
    "        grouped = df.groupby(['font_size', 'font_family'])\n",
    "\n",
    "        for (font_size, font_family), group in grouped:\n",
    "            total_rows = len(group)\n",
    "            total_words = group['word_count'].sum() if 'word_count' in group.columns else 0\n",
    "            avg_word_density = group['word_density'].mean() if 'word_density' in group.columns else 0\n",
    "\n",
    "            all_stats.append({\n",
    "                'page_number': i,\n",
    "                'font_size': font_size,\n",
    "                'font_family': font_family,\n",
    "                'count': total_rows,\n",
    "                'total_words': total_words,\n",
    "                'avg_word_density': round(avg_word_density, 2)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(all_stats)\n",
    "\n",
    "def get_global_font_counter(font_stats_df):\n",
    "    \"\"\"\n",
    "    Aggregates font counts across all pages from font_stats_df\n",
    "    to recreate a usable font_counter object.\n",
    "    \"\"\"\n",
    "    return Counter(dict(\n",
    "        font_stats_df.groupby('font_size')['count'].sum()\n",
    "    ))\n",
    "def recalculate_word_stats(dflist):\n",
    "    \"\"\"\n",
    "    Recalculates word count and word density (words per font size)\n",
    "    for each row in each DataFrame of the given list.\n",
    "\n",
    "    Parameters:\n",
    "        dflist (list of pd.DataFrame): Each DataFrame should have at least\n",
    "                                       'text' and 'font_size' columns.\n",
    "\n",
    "    Returns:\n",
    "        list of pd.DataFrame: Updated DataFrames with recalculated columns:\n",
    "                              'word_count' and 'word_density'.\n",
    "    \"\"\"\n",
    "    updated_dflist = []\n",
    "\n",
    "    for df in dflist:\n",
    "        if 'text' not in df.columns or 'font_size' not in df.columns:\n",
    "            updated_dflist.append(df)  # skip pages without required info\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df['word_count'] = df['text'].apply(lambda t: len(str(t).split()))\n",
    "        df['word_density'] = df.apply(\n",
    "            lambda row: row['word_count'] / row['font_size'] if row['font_size'] else 0,\n",
    "            axis=1\n",
    "        )\n",
    "        updated_dflist.append(df)\n",
    "\n",
    "    return updated_dflist\n",
    "\n",
    "\n",
    "\n",
    "def get_rare_large_fonts(font_counter, method='rms', freq_filter='average'):\n",
    "    \"\"\"\n",
    "    Identifies large fonts (likely headings) that are:\n",
    "      - larger than the body font size\n",
    "      - used less frequently than a frequency threshold\n",
    "\n",
    "    Parameters:\n",
    "        font_counter (Counter): font_size -> count\n",
    "        method (str): 'rms' or 'average' to compute body font size\n",
    "        freq_filter (str|float): 'average', 'median', or numeric threshold\n",
    "\n",
    "    Returns:\n",
    "        body_font (float): Computed body font size\n",
    "        rare_large_fonts (list): Sorted font sizes likely used for headings\n",
    "    \"\"\"\n",
    "    font_sizes = np.array(list(font_counter.keys()), dtype=float)\n",
    "    counts = np.array([int(font_counter[fs]) for fs in font_sizes])\n",
    "\n",
    "    # Step 1: Compute the body font\n",
    "    if method == 'rms':\n",
    "        body_font = np.sqrt(np.sum((font_sizes ** 2) * counts) / np.sum(counts))\n",
    "    else:  # 'average'\n",
    "        body_font = np.sum(font_sizes * counts) / np.sum(counts)\n",
    "\n",
    "    # Step 2: Determine frequency threshold\n",
    "    if freq_filter == 'average':\n",
    "        freq_threshold = np.mean(counts)\n",
    "    elif freq_filter == 'median':\n",
    "        freq_threshold = np.median(counts)\n",
    "    elif isinstance(freq_filter, (int, float)):\n",
    "        freq_threshold = freq_filter\n",
    "    else:\n",
    "        raise ValueError(\"Invalid freq_filter. Use 'average', 'median', or a numeric value.\")\n",
    "\n",
    "    # Step 3: Filter large, rare fonts\n",
    "    rare_large_fonts = [\n",
    "        float(fs) for fs in font_counter\n",
    "        if fs > body_font and font_counter[fs] < freq_threshold\n",
    "    ]\n",
    "\n",
    "    return round(body_font, 2), sorted(rare_large_fonts, reverse=True)\n",
    "\n",
    "def map_fonts_to_heading_levels(rare_fonts):\n",
    "    \"\"\"\n",
    "    Maps a list of rare large fonts to H1, H2, H3 levels.\n",
    "\n",
    "    Logic:\n",
    "    - 1 → H1\n",
    "    - 2 → H1, H2\n",
    "    - 3 → H1, H2, H3\n",
    "    - 4 → H1, H2, H3, H3\n",
    "    - 5 → H1, H2, H2, H3, H3\n",
    "    - 6 → H1, H1, H2, H2, H3, H3\n",
    "    - 7+ → Only first 6 fonts are used as per 6-rule\n",
    "\n",
    "    Parameters:\n",
    "        rare_fonts (list[float]): Sorted list of large rare font sizes (descending)\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping {font_size: heading_level}\n",
    "    \"\"\"\n",
    "    rare_fonts = sorted(rare_fonts, reverse=True)[:6]  # cap at 6 fonts\n",
    "    n = len(rare_fonts)\n",
    "    heading_map = {}\n",
    "\n",
    "    if n == 1:\n",
    "        heading_levels = ['H1']\n",
    "    elif n == 2:\n",
    "        heading_levels = ['H1', 'H2']\n",
    "    elif n == 3:\n",
    "        heading_levels = ['H1', 'H2', 'H3']\n",
    "    elif n == 4:\n",
    "        heading_levels = ['H1', 'H2', 'H3', 'H3']\n",
    "    elif n == 5:\n",
    "        heading_levels = ['H1', 'H2', 'H2', 'H3', 'H3']\n",
    "    else:  # n == 6\n",
    "        heading_levels = ['H1', 'H1', 'H2', 'H2', 'H3', 'H3']\n",
    "\n",
    "    for fs, level in zip(rare_fonts, heading_levels):\n",
    "        heading_map[fs] = level\n",
    "\n",
    "    return heading_map\n",
    "\n",
    "def analyze_word_density_patterns(dflist, method='word_density', stat='average', threshold_factor=1.5):\n",
    "    \"\"\"\n",
    "    Analyzes word density in a list of DataFrames to find lines that are unusually dense or sparse.\n",
    "\n",
    "    Parameters:\n",
    "        dflist (list of pd.DataFrame): List of page DataFrames with 'word_count', 'font_size', 'line_height'.\n",
    "        method (str): Mode of analysis. One of:\n",
    "            - 'word_density'     : word_count / line_height\n",
    "            - 'density_by_font'  : word_count / font_size\n",
    "            - 'weighted_density' : (word_count ** 2) / font_size\n",
    "            - 'font_scaled'      : word_density * font_size\n",
    "        stat (str): How to compute the central reference ('average', 'median').\n",
    "        threshold_factor (float): How far from the mean is considered rare/unusual.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            'metric_name': str,\n",
    "            'central_value': float,\n",
    "            'dense_lines': list of (page_idx, row_idx),\n",
    "            'sparse_lines': list of (page_idx, row_idx)\n",
    "        }\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    locations = []\n",
    "\n",
    "    for page_idx, df in enumerate(dflist):\n",
    "        for row_idx, row in df.iterrows():\n",
    "            wc, fs, lh = row.get('word_count'), row.get('font_size'), row.get('line_height')\n",
    "            if not all([wc, fs, lh]) or fs == 0 or lh == 0:\n",
    "                continue\n",
    "\n",
    "            # Compute metric based on mode\n",
    "            if method == 'word_density':\n",
    "                score = wc / lh\n",
    "            elif method == 'density_by_font':\n",
    "                score = wc / fs\n",
    "            elif method == 'weighted_density':\n",
    "                score = (wc ** 2) / fs\n",
    "            elif method == 'font_scaled':\n",
    "                score = (wc / lh) * fs\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid method: {method}\")\n",
    "\n",
    "            scores.append(score)\n",
    "            locations.append((page_idx, row_idx))\n",
    "\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    # Central tendency\n",
    "    if stat == 'average':\n",
    "        central = np.mean(scores)\n",
    "    elif stat == 'median':\n",
    "        central = np.median(scores)\n",
    "    else:\n",
    "        raise ValueError(\"Stat must be 'average' or 'median'\")\n",
    "\n",
    "    lower_thresh = central / threshold_factor\n",
    "    upper_thresh = central * threshold_factor\n",
    "\n",
    "    # Collect dense/sparse locations\n",
    "    dense_lines = [loc for score, loc in zip(scores, locations) if score > upper_thresh]\n",
    "    sparse_lines = [loc for score, loc in zip(scores, locations) if score < lower_thresh]\n",
    "\n",
    "    return {\n",
    "        'metric_name': method,\n",
    "        'central_value': round(central, 3),\n",
    "        'dense_lines': dense_lines,\n",
    "        'sparse_lines': sparse_lines\n",
    "    }\n",
    "\n",
    "def is_center_aligned(left, text, page_width, font_size, tolerance=15): ######make a freaking plot on desmos if required!?!?!?!?!\n",
    "    tolerance=page_width*(tolerance/100)\n",
    "    avg_char_width = 0.5 * font_size  # rough estimate\n",
    "    text_width = len(text) * avg_char_width\n",
    "    center_text = left + text_width / 2\n",
    "    center_page = page_width / 2\n",
    "    return abs(center_page - center_text) #<= tolerance\n",
    "# is_center_aligned(78.2, dflist[0].iloc[0]['text'], width, 14.3 )\n",
    "# dflist[0]\n",
    "# is_center_aligned(121.8, dflist[0].iloc[0]['text'], width, 10)\n",
    "\n",
    "# is_center_aligned(141.3, dflist[0].iloc[0]['text'], width, 6)\n",
    "# is_center_aligned(236.7, dflist[0].iloc[0]['text'], width, 14.3)\n",
    "\n",
    "# body_font, large_fonts = get_large_fonts(font_counter, method='rms')\n",
    "# page = doc[2]\n",
    "# table_df, table_label = extract_table_with_title(page)\n",
    "\n",
    "# if table_label:\n",
    "#     print(\"Detected Table Title:\", table_label[\"text\"])\n",
    "# extract_blocks_and_distributions,\n",
    "# find_alternate_page_repeats_split,\n",
    "# clean_pages_of_even_odd_repeats,\n",
    "# merge_paragraphs_by_font\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "d4ac1bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: density_by_font\n",
      "Central Value: 0.821\n",
      "Dense Lines: 55, Sparse Lines: 54\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "top",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "left",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "line_height",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "font_size",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "font_family",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "bold",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "italic",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word_density",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "parabool",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "level",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "c3a65784-964e-4fbc-8932-77f239ae8a39",
       "rows": [
        [
         "0",
         "71.0",
         "172.2",
         "20.0",
         "20.0",
         "RotisSansSerif,serif",
         "True",
         "False",
         "Belgium and Sri Lanka",
         "4",
         "0.2",
         "False",
         "H3"
        ],
        [
         "1",
         "203.1",
         "57.8",
         "12.0",
         "12.0",
         "Times New Roman,serif",
         "False",
         "False",
         "I have a simple equation in mind. Sharing power = dividing power = weakening the country. Why do we start by talking of this?",
         "24",
         "2.0",
         "True",
         "Body"
        ],
        [
         "2",
         "591.7",
         "49.8",
         "10.0",
         "10.0",
         "GaramondKursivHalbfett,serif",
         "True",
         "True",
         "Ethnic:  A social",
         "3",
         "0.3",
         "False",
         "Body"
        ],
        [
         "3",
         "603.7",
         "49.8",
         "10.0",
         "10.0",
         "GaramondAntiqua,serif",
         "False",
         "False",
         "division based on shared culture. People belonging to the same ethnic group believe in their common descent because of similarities of physical type or of culture or both. They need not always have the same religion or nationality.",
         "38",
         "3.8",
         "True",
         "Body"
        ],
        [
         "4",
         "97.8",
         "172.2",
         "11.5",
         "11.5",
         "GaramondAntiqua,serif",
         "False",
         "False",
         "Belgium is a small country in Europe, smaller in area than the state of Haryana. It has borders with France, the Netherlands, Germany and Luxembourg. It has a population of a little over one crore, about half the population of Haryana. The ethnic composition of this small country is very complex. Of the country&#x2019;s total population, 59 per cent lives in the Flemish region and speaks Dutch language. Another 40 per cent people live in the Wallonia region and speak French. Remaining one per cent of the Belgians speak German. In the capital city Brussels, 80 per cent people speak French while 20 per cent are Dutch-speaking. community was relatively rich and powerful. This was resented by the Dutch-speaking community who got the benefit of economic development and education much later. This led",
         "133",
         "11.565217391304348",
         "True",
         "Body"
        ],
        [
         "5",
         "371.4",
         "190.2",
         "11.5",
         "11.5",
         "GaramondAntiqua,serif",
         "False",
         "False",
         "The minority French-speaking",
         "3",
         "0.2608695652173913",
         "False",
         "Body"
        ],
        [
         "6",
         "97.8",
         "355.8",
         "11.5",
         "11.5",
         "GaramondAntiqua,serif",
         "False",
         "False",
         "to tensions between the Dutch- speaking and French-speaking communities during the 1950s and 1960s. The tension between the two communities was more acute in Brussels. Brussels presented a special problem: the Dutch-speaking people constituted a majority in the country, but a minority in the capital. situation in another country. Sri Lanka is an island nation, just a few kilometres off the southern coast of Tamil Nadu. It has about two crore people, about the same as in Haryana. Like other nations in the South Asia region, Sri Lanka has a diverse population. The major social groups are the Sinhala-speakers (74 per cent) and the Tamil-speakers (18 per cent). Among Tamils there are two sub-groups. Tamil natives of the country are called &#x2018;Sri Lankan",
         "123",
         "10.695652173913043",
         "True",
         "Body"
        ],
        [
         "7",
         "251.3",
         "373.8",
         "11.5",
         "11.5",
         "GaramondAntiqua,serif",
         "False",
         "False",
         "Let us compare this to the",
         "6",
         "0.5217391304347826",
         "False",
         "Body"
        ],
        [
         "8",
         "468.1",
         "447.4",
         "14.0",
         "14.0",
         "Arial Narrow,sans-serif",
         "False",
         "False",
         "Communities and regions of Belgium",
         "5",
         "0.35714285714285715",
         "True",
         "Body"
        ],
        [
         "9",
         "665.8",
         "215.6",
         "7.0",
         "7.0",
         "Arial,sans-serif",
         "False",
         "False",
         "Walloon (French-speaking)",
         "2",
         "0.2857142857142857",
         "False",
         "Body"
        ],
        [
         "10",
         "685.2",
         "215.5",
         "7.0",
         "7.0",
         "Arial,sans-serif",
         "False",
         "False",
         "Flemish (Dutch-speaking) German-speaking",
         "3",
         "0.42857142857142855",
         "True",
         "Body"
        ],
        [
         "11",
         "647.2",
         "215.2",
         "7.0",
         "7.0",
         "Arial,sans-serif",
         "False",
         "False",
         "Brussels-Capital Region",
         "2",
         "0.2857142857142857",
         "False",
         "Body"
        ],
        [
         "12",
         "701.0",
         "311.0",
         "9.0",
         "9.0",
         "Arial,sans-serif",
         "False",
         "True",
         "Look at the maps of Belgium and Sri Lanka. In",
         "10",
         "1.1111111111111112",
         "False",
         "Body"
        ],
        [
         "13",
         "711.8",
         "311.0",
         "9.0",
         "9.0",
         "Arial,sans-serif",
         "False",
         "True",
         "which region, do you find concentration of different",
         "8",
         "0.8888888888888888",
         "False",
         "Body"
        ],
        [
         "14",
         "722.6",
         "311.0",
         "9.0",
         "9.0",
         "Arial,sans-serif",
         "False",
         "True",
         "communities?",
         "1",
         "0.1111111111111111",
         "False",
         "Body"
        ],
        [
         "15",
         "621.3",
         "177.2",
         "8.0",
         "8.0",
         "Arial,sans-serif",
         "False",
         "False",
         "&#xa9;&#xa0; Wikipedia",
         "2",
         "0.25",
         "False",
         "Body"
        ],
        [
         "16",
         "735.4",
         "341.8",
         "8.0",
         "8.0",
         "Arial,sans-serif",
         "False",
         "False",
         "For more details, visit https://www.belgium.be/en",
         "5",
         "0.625",
         "False",
         "Body"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 17
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top</th>\n",
       "      <th>left</th>\n",
       "      <th>line_height</th>\n",
       "      <th>font_size</th>\n",
       "      <th>font_family</th>\n",
       "      <th>bold</th>\n",
       "      <th>italic</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>parabool</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71.0</td>\n",
       "      <td>172.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>RotisSansSerif,serif</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Belgium and Sri Lanka</td>\n",
       "      <td>4</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>False</td>\n",
       "      <td>H3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203.1</td>\n",
       "      <td>57.8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Times New Roman,serif</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>I have a simple equation in mind. Sharing powe...</td>\n",
       "      <td>24</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>591.7</td>\n",
       "      <td>49.8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>GaramondKursivHalbfett,serif</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Ethnic:  A social</td>\n",
       "      <td>3</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>603.7</td>\n",
       "      <td>49.8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>GaramondAntiqua,serif</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>division based on shared culture. People belon...</td>\n",
       "      <td>38</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>True</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97.8</td>\n",
       "      <td>172.2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>GaramondAntiqua,serif</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Belgium is a small country in Europe, smaller ...</td>\n",
       "      <td>133</td>\n",
       "      <td>11.565217</td>\n",
       "      <td>True</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>371.4</td>\n",
       "      <td>190.2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>GaramondAntiqua,serif</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>The minority French-speaking</td>\n",
       "      <td>3</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>97.8</td>\n",
       "      <td>355.8</td>\n",
       "      <td>11.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>GaramondAntiqua,serif</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>to tensions between the Dutch- speaking and Fr...</td>\n",
       "      <td>123</td>\n",
       "      <td>10.695652</td>\n",
       "      <td>True</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>251.3</td>\n",
       "      <td>373.8</td>\n",
       "      <td>11.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>GaramondAntiqua,serif</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Let us compare this to the</td>\n",
       "      <td>6</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>468.1</td>\n",
       "      <td>447.4</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Arial Narrow,sans-serif</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Communities and regions of Belgium</td>\n",
       "      <td>5</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>True</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>665.8</td>\n",
       "      <td>215.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Arial,sans-serif</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Walloon (French-speaking)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>685.2</td>\n",
       "      <td>215.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Arial,sans-serif</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Flemish (Dutch-speaking) German-speaking</td>\n",
       "      <td>3</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>True</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>647.2</td>\n",
       "      <td>215.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Arial,sans-serif</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Brussels-Capital Region</td>\n",
       "      <td>2</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>701.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Arial,sans-serif</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>Look at the maps of Belgium and Sri Lanka. In</td>\n",
       "      <td>10</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>711.8</td>\n",
       "      <td>311.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Arial,sans-serif</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>which region, do you find concentration of dif...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>722.6</td>\n",
       "      <td>311.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Arial,sans-serif</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>communities?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>621.3</td>\n",
       "      <td>177.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Arial,sans-serif</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>&amp;#xa9;&amp;#xa0; Wikipedia</td>\n",
       "      <td>2</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>735.4</td>\n",
       "      <td>341.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Arial,sans-serif</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>For more details, visit https://www.belgium.be/en</td>\n",
       "      <td>5</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      top   left  line_height  font_size                   font_family   bold  \\\n",
       "0    71.0  172.2         20.0       20.0          RotisSansSerif,serif   True   \n",
       "1   203.1   57.8         12.0       12.0         Times New Roman,serif  False   \n",
       "2   591.7   49.8         10.0       10.0  GaramondKursivHalbfett,serif   True   \n",
       "3   603.7   49.8         10.0       10.0         GaramondAntiqua,serif  False   \n",
       "4    97.8  172.2         11.5       11.5         GaramondAntiqua,serif  False   \n",
       "5   371.4  190.2         11.5       11.5         GaramondAntiqua,serif  False   \n",
       "6    97.8  355.8         11.5       11.5         GaramondAntiqua,serif  False   \n",
       "7   251.3  373.8         11.5       11.5         GaramondAntiqua,serif  False   \n",
       "8   468.1  447.4         14.0       14.0       Arial Narrow,sans-serif  False   \n",
       "9   665.8  215.6          7.0        7.0              Arial,sans-serif  False   \n",
       "10  685.2  215.5          7.0        7.0              Arial,sans-serif  False   \n",
       "11  647.2  215.2          7.0        7.0              Arial,sans-serif  False   \n",
       "12  701.0  311.0          9.0        9.0              Arial,sans-serif  False   \n",
       "13  711.8  311.0          9.0        9.0              Arial,sans-serif  False   \n",
       "14  722.6  311.0          9.0        9.0              Arial,sans-serif  False   \n",
       "15  621.3  177.2          8.0        8.0              Arial,sans-serif  False   \n",
       "16  735.4  341.8          8.0        8.0              Arial,sans-serif  False   \n",
       "\n",
       "    italic                                               text  word_count  \\\n",
       "0    False                              Belgium and Sri Lanka           4   \n",
       "1    False  I have a simple equation in mind. Sharing powe...          24   \n",
       "2     True                                  Ethnic:  A social           3   \n",
       "3    False  division based on shared culture. People belon...          38   \n",
       "4    False  Belgium is a small country in Europe, smaller ...         133   \n",
       "5    False                       The minority French-speaking           3   \n",
       "6    False  to tensions between the Dutch- speaking and Fr...         123   \n",
       "7    False                         Let us compare this to the           6   \n",
       "8    False                 Communities and regions of Belgium           5   \n",
       "9    False                          Walloon (French-speaking)           2   \n",
       "10   False           Flemish (Dutch-speaking) German-speaking           3   \n",
       "11   False                            Brussels-Capital Region           2   \n",
       "12    True      Look at the maps of Belgium and Sri Lanka. In          10   \n",
       "13    True  which region, do you find concentration of dif...           8   \n",
       "14    True                                       communities?           1   \n",
       "15   False                             &#xa9;&#xa0; Wikipedia           2   \n",
       "16   False  For more details, visit https://www.belgium.be/en           5   \n",
       "\n",
       "    word_density  parabool level  \n",
       "0       0.200000     False    H3  \n",
       "1       2.000000      True  Body  \n",
       "2       0.300000     False  Body  \n",
       "3       3.800000      True  Body  \n",
       "4      11.565217      True  Body  \n",
       "5       0.260870     False  Body  \n",
       "6      10.695652      True  Body  \n",
       "7       0.521739     False  Body  \n",
       "8       0.357143      True  Body  \n",
       "9       0.285714     False  Body  \n",
       "10      0.428571      True  Body  \n",
       "11      0.285714     False  Body  \n",
       "12      1.111111     False  Body  \n",
       "13      0.888889     False  Body  \n",
       "14      0.111111     False  Body  \n",
       "15      0.250000     False  Body  \n",
       "16      0.625000     False  Body  "
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileinquestion = \"C1A/input/E0CCG5S239.pdf\" #single page doc E0CCG5S239\n",
    "fileinquestion = \"C1A/input/TOPJUMP-PARTY-INVITATION-20161003-V01.pdf\"\n",
    "fileinquestion = \"C1A/input/STEMPathwaysFlyer.pdf\"\n",
    "fileinquestion = \"C1A/input/E0H1CM114.pdf\"\n",
    "fileinquestion = \"AAMine/killer.pdf\"\n",
    "fileinquestion = \"C1A/input/E0H1CM114.pdf\"\n",
    "fileinquestion = \"AAMine/killer.pdf\"\n",
    "fileinquestion = \"C1A/input/E0CCG5S312.pdf\"\n",
    "fileinquestion = \"AAMine/jess401.pdf\"\n",
    "\n",
    "\n",
    "#E0CCG5S312 eazy 12 pages\n",
    "#hard E0H1CM114\n",
    "doc = pymupdf.open(fileinquestion)  \n",
    "page = doc[0]  # first page\n",
    "\n",
    "width = page.rect.width\n",
    "height = page.rect.height\n",
    "doctoc = doc.get_toc()\n",
    "docmetadata = doc.metadata\n",
    "totalpage= doc.page_count\n",
    "\n",
    "dflist , repeated_all, all_font_sizes , font_counter = [], [] ,set() , Counter()\n",
    "mid = totalpage//2\n",
    "\n",
    "#if dig.dig is followed then try to use the flow...\n",
    "for i in range(totalpage):\n",
    "    page = doc.load_page(i)\n",
    "    html = page.get_text(\"html\")\n",
    "    blocks, font_counter, line_counter = extract_blocks_and_distributions(html)\n",
    "    dflist.append(pd.DataFrame(blocks))\n",
    "dfl = dflist\n",
    "dfcrucial = extract_crucial_pattern_lines(dflist)\n",
    "dfcrucial############################################################################################################################\n",
    "\n",
    "dflist[0]\n",
    "get_top_fonts(font_counter)\n",
    "\n",
    "even_df, odd_df =       find_alternate_page_repeats_split(dflist, mid=totalpage // 2)\n",
    "dflist =                clean_pages_of_even_odd_repeats(dflist, even_df, odd_df)\n",
    "dflist =        [filter_gibberish_rows(df) for df in dflist]\n",
    "\n",
    "\n",
    "\n",
    "font_stats_df = generate_font_stats(dflist)\n",
    "font_stats_df.sort_values(by=['page_number', 'font_size'], ascending=[True, False]).reset_index(drop=True)\n",
    "font_counter = get_global_font_counter(font_stats_df)\n",
    "topfonts = get_top_fonts(font_counter)\n",
    "# font_level_map =  create_font_level_map(font_counter) #classify_font_size(16, font_level_map)\n",
    "\n",
    "target_fonts = list(font_counter.keys())\n",
    "\n",
    "#merging it now\n",
    "dflist = merge_consecutive_same_font_and_left(dflist, target_fonts)\n",
    "dflist = recalculate_word_stats(dflist)\n",
    "\n",
    "\n",
    "font_counter\n",
    "body_font, large_fonts = get_rare_large_fonts(font_counter, method='rms', freq_filter='average')\n",
    "\n",
    "# even_df, odd_df = find_alternate_page_repeats_split(dflist, mid=totalpage // 2)\n",
    "# dflist = clean_pages_of_even_odd_repeats(dflist, even_df, odd_df)\n",
    "# font_level_map =  create_font_level_map(font_counter) #classify_font_size(16, font_level_map)\n",
    "font_heading_map = map_fonts_to_heading_levels(large_fonts)\n",
    "\n",
    "dflist = label_font_levels(dflist, font_heading_map) #returns the dflist\n",
    "result = analyze_word_density_patterns(\n",
    "    dflist,\n",
    "    method='density_by_font',\n",
    "    stat='median',\n",
    "    threshold_factor=1.8\n",
    ")\n",
    "\n",
    "print(f\"Metric: {result['metric_name']}\")\n",
    "print(f\"Central Value: {result['central_value']}\")\n",
    "print(f\"Dense Lines: {len(result['dense_lines'])}, Sparse Lines: {len(result['sparse_lines'])}\")\n",
    "\n",
    "# for i in range(totalpage):\n",
    "#     extract_table_with_title(doc[i])\n",
    "dflist[0]\n",
    "dfl[0]\n",
    "dfcrucial.loc[dfcrucial.bold==True]\n",
    "\n",
    "dfcrucial\n",
    "extract_heading_summary(dflist, levels=(['H1','H2','H3']))\n",
    "dflist[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78925123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "top",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "left",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "line_height",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "font_size",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "font_family",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "bold",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "italic",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "word_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "word_density",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "parabool",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "level",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "page",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "dfe0ebfb-5406-459c-90d6-f65c2390241a",
       "rows": [
        [
         "0",
         "391.5",
         "72.0",
         "10.0",
         "10.0",
         "NimbusRomNo9L,serif",
         "True",
         "False",
         "1. Introduction",
         "2",
         "0.2",
         "False",
         "Body",
         "0"
        ],
        [
         "1",
         "499.8",
         "72.0",
         "10.0",
         "10.0",
         "NimbusRomNo9L,serif",
         "True",
         "False",
         "2. Hermite Wavelet",
         "3",
         "0.3",
         "False",
         "Body",
         "1"
        ],
        [
         "2",
         "670.1",
         "72.0",
         "10.0",
         "10.0",
         "NimbusRomNo9L,serif",
         "True",
         "False",
         "3. Method For Solution",
         "4",
         "0.4",
         "False",
         "Body",
         "1"
        ],
        [
         "3",
         "137.4",
         "72.0",
         "10.0",
         "10.0",
         "NimbusRomNo9L,serif",
         "True",
         "False",
         "4. Convergence Analysis",
         "3",
         "0.3",
         "False",
         "Body",
         "2"
        ],
        [
         "4",
         "207.3",
         "72.0",
         "10.0",
         "10.0",
         "NimbusRomNo9L,serif",
         "True",
         "False",
         "5. Simulations and Results",
         "4",
         "0.4",
         "False",
         "Body",
         "2"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top</th>\n",
       "      <th>left</th>\n",
       "      <th>line_height</th>\n",
       "      <th>font_size</th>\n",
       "      <th>font_family</th>\n",
       "      <th>bold</th>\n",
       "      <th>italic</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>parabool</th>\n",
       "      <th>level</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391.5</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NimbusRomNo9L,serif</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1. Introduction</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>499.8</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NimbusRomNo9L,serif</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2. Hermite Wavelet</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>670.1</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NimbusRomNo9L,serif</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>3. Method For Solution</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137.4</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NimbusRomNo9L,serif</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4. Convergence Analysis</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>207.3</td>\n",
       "      <td>72.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NimbusRomNo9L,serif</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>5. Simulations and Results</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>False</td>\n",
       "      <td>Body</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     top  left  line_height  font_size          font_family  bold  italic  \\\n",
       "0  391.5  72.0         10.0       10.0  NimbusRomNo9L,serif  True   False   \n",
       "1  499.8  72.0         10.0       10.0  NimbusRomNo9L,serif  True   False   \n",
       "2  670.1  72.0         10.0       10.0  NimbusRomNo9L,serif  True   False   \n",
       "3  137.4  72.0         10.0       10.0  NimbusRomNo9L,serif  True   False   \n",
       "4  207.3  72.0         10.0       10.0  NimbusRomNo9L,serif  True   False   \n",
       "\n",
       "                         text  word_count  word_density  parabool level  page  \n",
       "0             1. Introduction           2           0.2     False  Body     0  \n",
       "1          2. Hermite Wavelet           3           0.3     False  Body     1  \n",
       "2      3. Method For Solution           4           0.4     False  Body     1  \n",
       "3     4. Convergence Analysis           3           0.3     False  Body     2  \n",
       "4  5. Simulations and Results           4           0.4     False  Body     2  "
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "56aef184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "page_num",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "level",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "86d402d1-23ba-4ad9-97e1-7b823143d02f",
       "rows": [
        [
         "0",
         "2",
         "H1",
         "Revision History"
        ],
        [
         "1",
         "3",
         "H1",
         "Table of Contents"
        ],
        [
         "2",
         "4",
         "H1",
         "Acknowledgements"
        ],
        [
         "3",
         "5",
         "H1",
         "1. Introduction to the Foundation Level Extensions"
        ],
        [
         "4",
         "6",
         "H1",
         "2. Introduction to Foundation Level Agile Tester Extension"
        ],
        [
         "5",
         "6",
         "H2",
         "2.1 Intended Audience"
        ],
        [
         "6",
         "6",
         "H2",
         "2.2 Career Paths for Testers"
        ],
        [
         "7",
         "6",
         "H2",
         "2.3 Learning Objectives"
        ],
        [
         "8",
         "7",
         "H2",
         "2.4 Entry Requirements"
        ],
        [
         "9",
         "7",
         "H2",
         "2.5 Structure and Course Duration"
        ],
        [
         "10",
         "8",
         "H2",
         "2.6 Keeping It Current"
        ],
        [
         "11",
         "9",
         "H1",
         "3. Overview of the Foundation Level Extension &#x2013; Agile Tester"
        ],
        [
         "12",
         "9",
         "H1",
         "Syllabus"
        ],
        [
         "13",
         "9",
         "H2",
         "3.1 Business Outcomes"
        ],
        [
         "14",
         "9",
         "H2",
         "3.2 Content"
        ],
        [
         "15",
         "11",
         "H1",
         "4. References"
        ],
        [
         "16",
         "11",
         "H2",
         "4.1 Trademarks"
        ],
        [
         "17",
         "11",
         "H2",
         "4.2 Documents and Web Sites"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 18
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>level</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>H1</td>\n",
       "      <td>Revision History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>H1</td>\n",
       "      <td>Table of Contents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>H1</td>\n",
       "      <td>Acknowledgements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>H1</td>\n",
       "      <td>1. Introduction to the Foundation Level Extens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>H1</td>\n",
       "      <td>2. Introduction to Foundation Level Agile Test...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>H2</td>\n",
       "      <td>2.1 Intended Audience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>H2</td>\n",
       "      <td>2.2 Career Paths for Testers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>H2</td>\n",
       "      <td>2.3 Learning Objectives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>H2</td>\n",
       "      <td>2.4 Entry Requirements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>H2</td>\n",
       "      <td>2.5 Structure and Course Duration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>H2</td>\n",
       "      <td>2.6 Keeping It Current</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9</td>\n",
       "      <td>H1</td>\n",
       "      <td>3. Overview of the Foundation Level Extension ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>H1</td>\n",
       "      <td>Syllabus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9</td>\n",
       "      <td>H2</td>\n",
       "      <td>3.1 Business Outcomes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9</td>\n",
       "      <td>H2</td>\n",
       "      <td>3.2 Content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11</td>\n",
       "      <td>H1</td>\n",
       "      <td>4. References</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11</td>\n",
       "      <td>H2</td>\n",
       "      <td>4.1 Trademarks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11</td>\n",
       "      <td>H2</td>\n",
       "      <td>4.2 Documents and Web Sites</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page_num level                                               text\n",
       "0          2    H1                                   Revision History\n",
       "1          3    H1                                  Table of Contents\n",
       "2          4    H1                                   Acknowledgements\n",
       "3          5    H1  1. Introduction to the Foundation Level Extens...\n",
       "4          6    H1  2. Introduction to Foundation Level Agile Test...\n",
       "5          6    H2                              2.1 Intended Audience\n",
       "6          6    H2                       2.2 Career Paths for Testers\n",
       "7          6    H2                            2.3 Learning Objectives\n",
       "8          7    H2                             2.4 Entry Requirements\n",
       "9          7    H2                  2.5 Structure and Course Duration\n",
       "10         8    H2                             2.6 Keeping It Current\n",
       "11         9    H1  3. Overview of the Foundation Level Extension ...\n",
       "12         9    H1                                           Syllabus\n",
       "13         9    H2                              3.1 Business Outcomes\n",
       "14         9    H2                                        3.2 Content\n",
       "15        11    H1                                      4. References\n",
       "16        11    H2                                     4.1 Trademarks\n",
       "17        11    H2                        4.2 Documents and Web Sites"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dflist = []\n",
    "for i in range(totalpage):\n",
    "    page = doc.load_page(i)\n",
    "    html = page.get_text(\"html\")\n",
    "    blocks, font_counter, line_counter = extract_blocks_and_distributions(html)\n",
    "    dflist.append(pd.DataFrame(blocks))\n",
    "\n",
    "even_df, odd_df = find_alternate_page_repeats_split(dflist, mid=totalpage // 2)\n",
    "cleaned_dflist = clean_pages_of_even_odd_repeats(dflist, even_df, odd_df)\n",
    "\n",
    "def find_repeating_rows(dflist, center_page, offsets=[1, 2], tolerance=1.0):#########depriciated\n",
    "    \"\"\"\n",
    "    Finds text lines (rows) in the center page that repeat in neighboring pages at given offsets.\n",
    "    Returns a DataFrame of repeated rows likely to be headers/footers.\n",
    "    \"\"\"\n",
    "    repeated_all = []\n",
    "    df_mid = dflist[center_page]\n",
    "\n",
    "    for offset in offsets:\n",
    "        before, after = center_page - offset, center_page + offset\n",
    "        if before < 0 or after >= len(dflist):\n",
    "            continue\n",
    "\n",
    "        df_before, df_after = dflist[before], dflist[after]\n",
    "\n",
    "        for _, row in df_mid.iterrows():\n",
    "            text, top = row['text'].strip(), row['top']\n",
    "\n",
    "            def is_match(df):\n",
    "                return any(\n",
    "                    (abs(top - r['top']) <= tolerance) and (r['text'].strip() == text)\n",
    "                    for _, r in df.iterrows()\n",
    "                )\n",
    "\n",
    "            if is_match(df_before) and is_match(df_after):\n",
    "                repeated_all.append(row)\n",
    "\n",
    "    # Drop duplicates and return as DataFrame\n",
    "    return pd.DataFrame(repeated_all).drop_duplicates(subset=[\"text\", \"top\", \"font_size\"])\n",
    "\n",
    "repeated_rows_df = find_repeating_rows(dflist, center_page=mid)\n",
    "# cleaned_dflist = clean_pages_of_repeats(dflist, repeated_rows_df)\n",
    "# repeated_rows_df\n",
    "# cleaned_dflist\n",
    "# # for i,clean in enumerate(cleaned_dflist):\n",
    "#     # clean.to_csv(f\"analysis/hope12/f{i}.csv\",index=False,encoding=\"utf-8\")\n",
    "\n",
    "cleaned_dflist = [filter_gibberish_rows(df) for df in cleaned_dflist]\n",
    "get_top_fonts(font_counter)\n",
    "font_level_map =  create_font_level_map(font_counter) #classify_font_size(16, font_level_map)\n",
    "cleaned_dflist = label_font_levels(cleaned_dflist, font_level_map)\n",
    "extract_heading_summary(cleaned_dflist, levels=(['H1','H2']))\n",
    "font_counter\n",
    "i=0\n",
    "# repeated_rows_df\n",
    "cleaned_dflist[1]\n",
    "font_level_map\n",
    "extract_heading_summary(cleaned_dflist, levels=(['H1','H2']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d0d0d788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "page_num",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "level",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f42853df-8c54-4739-b911-3435f552eb2a",
       "rows": [
        [
         "0",
         "0",
         "H2",
         "The Ontario Digital Library will make Ontario a better place to study, work and live by ensuring that"
        ],
        [
         "1",
         "0",
         "H2",
         "all Ontario citizens have access to the knowledge and learning supports needed to be life-long"
        ],
        [
         "2",
         "0",
         "H2",
         "learners and effective contributors towards Ontario&#x2019;s prosperity."
        ],
        [
         "3",
         "1",
         "H1",
         "Summary"
        ],
        [
         "4",
         "1",
         "H2",
         "St., Suite 303, Toronto, ON  M5C 1M3. Proposals must be received by Noon on Monday,"
        ],
        [
         "5",
         "2",
         "H1",
         "Background"
        ],
        [
         "6",
         "5",
         "H1",
         "The Business Plan to be Developed"
        ],
        [
         "7",
         "6",
         "H1",
         "Milestones"
        ],
        [
         "8",
         "6",
         "H1",
         "Approach and Specific Proposal Requirements"
        ],
        [
         "9",
         "7",
         "H1",
         "Evaluation and Awarding of Contract"
        ],
        [
         "10",
         "8",
         "H1",
         "Appendix A:  ODL Envisioned Phases &amp; Funding"
        ],
        [
         "11",
         "8",
         "H2",
         "Phase I:  Business Planning"
        ],
        [
         "12",
         "8",
         "H2",
         "Timeline: March 2003 &#x2013; September 2003"
        ],
        [
         "13",
         "8",
         "H2",
         "Funding Requested: ~$100,000 jointly funded by stakeholder groups and the provincial government."
        ],
        [
         "14",
         "8",
         "H2",
         "Result: The ODL business plan"
        ],
        [
         "15",
         "8",
         "H2",
         "The first phase will be to build the ODL business plan. This plan will clearly define the ODL&#x2019;s services,"
        ],
        [
         "16",
         "8",
         "H2",
         "funding and governance structures, as well as implementation plans for 2003-2005. It will also secure"
        ],
        [
         "17",
         "8",
         "H2",
         "the full commitment of all stakeholders and scope the operational plan for ODL for 2006 and beyond."
        ],
        [
         "18",
         "8",
         "H2",
         "Given the number and diversity of stakeholders involved, the business planning process must be a fully"
        ],
        [
         "19",
         "8",
         "H2",
         "consultative approach. To ensure that the planning results in a workable plan with the full commitment"
        ],
        [
         "20",
         "8",
         "H2",
         "of all stakeholders it must have competent, dedicated staffing and monies for the travel and"
        ],
        [
         "21",
         "8",
         "H2",
         "communication components so critical in a consultative process."
        ],
        [
         "22",
         "8",
         "H2",
         "Phase II: Implementing and Transitioning"
        ],
        [
         "23",
         "8",
         "H2",
         "Timeline: April 2004 &#x2013; December 2006"
        ],
        [
         "24",
         "8",
         "H2",
         "Funding Requested: Funding from other states and provinces suggest that the ODL could receive"
        ],
        [
         "25",
         "8",
         "H2",
         "funding of up to $50 Million (over 3 years). Funding to be provided partnership of government, library"
        ],
        [
         "26",
         "8",
         "H2",
         "stakeholders and other interested parties."
        ],
        [
         "27",
         "8",
         "H2",
         "Result: The ODL is implemented and validated"
        ],
        [
         "28",
         "8",
         "H2",
         "The second phase will be to implement the ODL based on the business plan. This phase recognizes"
        ],
        [
         "29",
         "8",
         "H2",
         "that for ODL to be optimally successful libraries must transition to a new way of doing business and"
        ],
        [
         "30",
         "8",
         "H2",
         "providing services. The transition must occur while libraries continue to provide existing services and"
        ],
        [
         "31",
         "8",
         "H2",
         "respond to current challenges. This implementation phase will funded by a partnership of government,"
        ],
        [
         "32",
         "8",
         "H2",
         "library stakeholders and other interested parties as a means to quickly jumpstart the ODL."
        ],
        [
         "33",
         "8",
         "H2",
         "The seed money requested will allow libraries to realign their budgets and services as the infrastructure"
        ],
        [
         "34",
         "8",
         "H2",
         "and content of the ODL is created and secured. Some of the funding will be new money, although there"
        ],
        [
         "35",
         "8",
         "H2",
         "is every indication that existing budgets and methods of operating may be modified as a result of"
        ],
        [
         "36",
         "8",
         "H2",
         "recommendations. During this phase the polices, procedures, governance structures and accountability"
        ],
        [
         "37",
         "8",
         "H2",
         "mechanisms of the ODL will be put in place. Pilot projects will be initiated, evaluated and expanded."
        ],
        [
         "38",
         "8",
         "H2",
         "Information resources will be identified, contracts for these resources will be negotiated and resources"
        ],
        [
         "39",
         "8",
         "H2",
         "will be made deployed through the ODL. Regular evaluation during this phase will ensure the ODL is"
        ],
        [
         "40",
         "8",
         "H2",
         "achieving its objectives and is accountable to its key communities."
        ],
        [
         "41",
         "8",
         "H2",
         "Phase III: Operating and Growing the ODL"
        ],
        [
         "42",
         "8",
         "H2",
         "Timeline: January 2007 -"
        ],
        [
         "43",
         "8",
         "H2",
         "Funding: $50 Million annually ($35 Million requested from government)"
        ],
        [
         "44",
         "8",
         "H2",
         "Result: The ODL is fully operational and sustainable"
        ],
        [
         "45",
         "8",
         "H2",
         "In the third phase the ODL moves into the operational stage. Ontarians will experience the full benefits"
        ],
        [
         "46",
         "8",
         "H2",
         "of the initiative and libraries will consolidate support around the ODL to grow resources and extend"
        ],
        [
         "47",
         "9",
         "H2",
         "capabilities. The challenge is to secure resources sufficient to both sustain the original investments and"
        ],
        [
         "48",
         "9",
         "H2",
         "to enhance the ODL."
        ],
        [
         "49",
         "9",
         "H2",
         "The ongoing funding of the ODL will be based on a partnership model involving all the key participants"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 222
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>level</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>H2</td>\n",
       "      <td>The Ontario Digital Library will make Ontario ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>H2</td>\n",
       "      <td>all Ontario citizens have access to the knowle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>H2</td>\n",
       "      <td>learners and effective contributors towards On...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>H1</td>\n",
       "      <td>Summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>H2</td>\n",
       "      <td>St., Suite 303, Toronto, ON  M5C 1M3. Proposal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>13</td>\n",
       "      <td>H2</td>\n",
       "      <td>-study guides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>13</td>\n",
       "      <td>H2</td>\n",
       "      <td>-study skill development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>13</td>\n",
       "      <td>H2</td>\n",
       "      <td>-web-based curricula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>13</td>\n",
       "      <td>H2</td>\n",
       "      <td>-internet search guides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>13</td>\n",
       "      <td>H2</td>\n",
       "      <td>4. Journals, books, maps, music etc.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     page_num level                                               text\n",
       "0           0    H2  The Ontario Digital Library will make Ontario ...\n",
       "1           0    H2  all Ontario citizens have access to the knowle...\n",
       "2           0    H2  learners and effective contributors towards On...\n",
       "3           1    H1                                            Summary\n",
       "4           1    H2  St., Suite 303, Toronto, ON  M5C 1M3. Proposal...\n",
       "..        ...   ...                                                ...\n",
       "217        13    H2                                      -study guides\n",
       "218        13    H2                           -study skill development\n",
       "219        13    H2                               -web-based curricula\n",
       "220        13    H2                            -internet search guides\n",
       "221        13    H2               4. Journals, books, maps, music etc.\n",
       "\n",
       "[222 rows x 3 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileinquestion = \"C1A/input/E0H1CM114.pdf\"\n",
    "#E0CCG5S312 eazy 12 pages\n",
    "doc = pymupdf.open(fileinquestion)  \n",
    "doctoc = doc.get_toc()\n",
    "docmetadata = doc.metadata\n",
    "totalpage= doc.page_count\n",
    "\n",
    "dflist , repeated_all, all_font_sizes , font_counter = [], [] ,set() , Counter()\n",
    "mid = totalpage//2\n",
    "\n",
    "#if dig.dig is followed then try to use the flow...\n",
    "\n",
    "for i in range(totalpage):\n",
    "    page = doc.load_page(i)\n",
    "    html = page.get_text(\"html\")\n",
    "    blocks, font_counter, line_counter = extract_blocks_and_distributions(html)\n",
    "    dflist.append(pd.DataFrame(blocks))\n",
    "\n",
    "even_df, odd_df = find_alternate_page_repeats_split(dflist, mid=totalpage // 2)\n",
    "cleaned_dflist = clean_pages_of_even_odd_repeats(dflist, even_df, odd_df)\n",
    "\n",
    "paragraphs = []\n",
    "for i, df in enumerate(cleaned_dflist):\n",
    "    merged = merge_paragraphs_by_font(df, font_min=8.0, font_max=14.0)  # You can define ranges\n",
    "    for para in merged:\n",
    "        para['page'] = i\n",
    "    paragraphs.extend(para)\n",
    "\n",
    "paragraph_df = pd.DataFrame(paragraphs)\n",
    "merged = merge_paragraphs_by_font(df, font_min=8.0, font_max=14.0, font_tolerance=1.0, line_gap=3.0)\n",
    "\n",
    "# repeated_rows_df = find_repeating_rows(dflist, center_page=mid)\n",
    "# cleaned_dflist = clean_pages_of_repeats(dflist, repeated_rows_df)\n",
    "# repeated_rows_df\n",
    "# cleaned_dflist\n",
    "# # for i,clean in enumerate(cleaned_dflist):\n",
    "#     # clean.to_csv(f\"analysis/hope12/f{i}.csv\",index=False,encoding=\"utf-8\")\n",
    "\n",
    "cleaned_dflist = [filter_gibberish_rows(df) for df in cleaned_dflist]\n",
    "get_top_fonts(font_counter)\n",
    "font_level_map =  create_font_level_map(font_counter) #classify_font_size(16, font_level_map)\n",
    "cleaned_dflist = label_font_levels(cleaned_dflist, font_level_map)\n",
    "extract_heading_summary(cleaned_dflist, levels=(['H1','H2']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, page_df in enumerate(cleaned_dflist):\n",
    "    paras = merge_paragraphs_by_font(page_df)\n",
    "    for p in paras:\n",
    "        p['page'] = i  # Add page number\n",
    "    merged_paras.extend(paras)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "paragraph_df = pd.DataFrame(merged_paras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3293315b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 table(s) found.\n",
      "0 table(s) found.\n",
      "1 table(s) found.\n",
      "First table extracted:\n",
      "[['Version', 'Date', 'Remarks'],\n",
      " ['0.1', '18 JUNE 2013', 'Initial version'],\n",
      " ['0.2', '23 JULY 2013', 'WG reviewed and confirmed'],\n",
      " ['0.3', '6 NOV 2013', 'amended population and diagram'],\n",
      " ['0.7', '11 DEC 2013', 'Amended Business Outcomes and Chapters matching'],\n",
      " ['0.8', '20 DEC 2013', 'Working group updates on 0.7'],\n",
      " ['1.0', '31 MAY 2014', 'GA release for Agile Extension']]\n",
      "Possible table title: Revision History\n",
      "\n",
      "Title: Revision History\n",
      "\n",
      "         0             1                                                2\n",
      "0  Version          Date                                          Remarks\n",
      "1      0.1  18 JUNE 2013                                  Initial version\n",
      "2      0.2  23 JULY 2013                        WG reviewed and confirmed\n",
      "3      0.3    6 NOV 2013                   amended population and diagram\n",
      "4      0.7   11 DEC 2013  Amended Business Outcomes and Chapters matching\n",
      "0 table(s) found.\n",
      "0 table(s) found.\n",
      "0 table(s) found.\n",
      "0 table(s) found.\n",
      "1 table(s) found.\n",
      "First table extracted:\n",
      "[['Syllabus', 'Days'],\n",
      " ['Baseline: Foundation', '3'],\n",
      " ['Extension: Agile Tester', '2']]\n",
      "Possible table title: The syllabi must be taught in the following minimum number of days:\n",
      "\n",
      "Title: The syllabi must be taught in the following minimum number of days:\n",
      "\n",
      "                         0     1\n",
      "0                 Syllabus  Days\n",
      "1     Baseline: Foundation     3\n",
      "2  Extension: Agile Tester     2\n",
      "0 table(s) found.\n",
      "0 table(s) found.\n",
      "0 table(s) found.\n",
      "0 table(s) found.\n"
     ]
    }
   ],
   "source": [
    "#############EXP\n",
    "tablezdf\n",
    "for i in range(totalpage):\n",
    "    page = doc[i]\n",
    "    table_df, title = extract_table_with_title(page)\n",
    "\n",
    "    if table_df is not None:\n",
    "        print(f\"\\nTitle: {title}\\n\")\n",
    "        print(table_df.head())\n",
    "        tabs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRAVEYARD\n",
    "\n",
    "# page = doc[1] #not index, its page\n",
    "# for page in reversed(doc):\n",
    "# for page in doc.pages(start, stop, step):\n",
    "locationn= \"analysis/hope1\"\n",
    "alldfs,thetexts , linkstotal , linkstotalnext , annotstotal , html = [] , [] , [] , [] , [] , []\n",
    "def dumpshit(doc):\n",
    "    for i,page in enumerate(doc): # iterate the document pages\n",
    "        text = page.get_text() # get plain text encoded as UTF-8\n",
    "        thetexts.append(text)\n",
    "        links = page.get_links()\n",
    "        linkstotal.append(links)\n",
    "        link = page.first_link  # a `Link` object or `None`\n",
    "        linktemp =[]\n",
    "        while link: \n",
    "            link = link.next # get next link, last one has `None` in its `next`\n",
    "            linktemp.append(link)\n",
    "        linkstotalnext.append(linktemp)\n",
    "        # for annot in page.annots():\n",
    "            # print(f'Annotation on page: {page.number} with type: {annot.type} and rect: {annot.rect}')\n",
    "        # for field in page.widgets():\n",
    "            # print(f'Widget on page: {page.number} with type: {field.type} and rect: {field.rect}')\n",
    "        text = page.get_text(\"html\")\n",
    "            # Use one of the following strings for opt to obtain different formats [2]:\n",
    "            # “text”: (default) plain text with line breaks. No formatting, no text position details, no images.\n",
    "            # “blocks”: generate a list of text blocks (= paragraphs).\n",
    "            # “words”: generate a list of words (strings not containing spaces).\n",
    "            # “html”: creates a full visual version of the page including any images. This can be displayed with your internet browser.\n",
    "            # “dict” / “json”: same information level as HTML, but provided as a Python dictionary or resp. JSON string. See TextPage.extractDICT() for details of its structure.\n",
    "            # “rawdict” / “rawjson”: a super-set of “dict” / “json”. It additionally provides character detail information like XML. See TextPage.extractRAWDICT() for details of its structure.\n",
    "            # “xhtml”: text information level as the TEXT version but includes images. Can also be displayed by internet browsers.\n",
    "            # “xml”: contains no images, but full position and font information down to each single text character. Use an XML module to interpret.\n",
    "        # with open(\"output.html\", \"w\") as f:\n",
    "        text = re.sub(r'<img[^>]*>', '<image>', text)\n",
    "        html.append(text)\n",
    "        with open(f\"analysis/hope1/f{i}.html\",'w',encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    " \n",
    "        # Font\n",
    "        # Alignment\n",
    "        # cases\n",
    "        #indentation, new pages, \n",
    "        # 1 1.0\n",
    "        #toc/index - \n",
    "        # blocks of code\n",
    "        # \n",
    "# for i in range(12)\n",
    "page= doc[0]\n",
    "text = page.get_text(\"html\")\n",
    "# pprint(text)\n",
    "print(doc[1].get_text(\"blocks\") == doc[1].get_text(\"blocks\"))\n",
    "htmx=(doc[0].get_text(\"html\"))\n",
    "with open (\"html.html\", \"w\") as f:\n",
    "    f.write(htmx)\n",
    "htmx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d236ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLDDDDD\n",
    "def clean_pages_of_even_odd_repeats(dflist, even_df, odd_df, tolerance=1.0):\n",
    "    \"\"\"\n",
    "    Cleans even-indexed pages using even_df and odd-indexed pages using odd_df.\n",
    "    If even_df == odd_df, treats all pages the same.\n",
    "    \"\"\"\n",
    "    cleaned_pages = []\n",
    "    # Check if both are the same\n",
    "    same_repeats = ( set((r['text'].strip(), round(r['top'], 1)) for _, r in even_df.iterrows()) == set((r['text'].strip(), round(r['top'], 1)) for _, r in odd_df.iterrows()) )\n",
    "    for idx, df in enumerate(dflist):\n",
    "        if same_repeats:\n",
    "            compare_df = even_df  # or odd_df, doesn't matter\n",
    "        else:\n",
    "            compare_df = even_df if idx % 2 == 0 else odd_df\n",
    "        mask = df.apply(\n",
    "            lambda row: not any(\n",
    "                (abs(row['top'] - rep['top']) <= tolerance)\n",
    "                and (row['text'].strip() == rep['text'].strip())\n",
    "                for _, rep in compare_df.iterrows()\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        cleaned_df = df[mask].reset_index(drop=True)\n",
    "        cleaned_pages.append(cleaned_df)\n",
    "    return cleaned_pages\n",
    "\n",
    "def clean_pages_of_repeats(dflist, repeated_df, tolerance=1.0): ##################depreciated\n",
    "    \"\"\"\n",
    "    Removes rows from each page in dflist that match (in position and text) any row in repeated_df.\n",
    "    Useful for cleaning common headers/footers.\n",
    "    \"\"\"\n",
    "    cleaned_pages = []\n",
    "\n",
    "    for df in dflist:\n",
    "        mask = df.apply(\n",
    "            lambda row: not any(\n",
    "                (abs(row['top'] - rep['top']) <= tolerance) and (row['text'].strip() == rep['text'].strip())\n",
    "                for _, rep in repeated_df.iterrows()\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        cleaned_df = df[mask].reset_index(drop=True)\n",
    "        cleaned_pages.append(cleaned_df)\n",
    "\n",
    "    return cleaned_pages\n",
    "\n",
    "\n",
    "def find_repeating_rows(dflist, center_page, offsets=[1, 2], tolerance=1.0):#########depriciated\n",
    "    \"\"\"\n",
    "    Finds text lines (rows) in the center page that repeat in neighboring pages at given offsets.\n",
    "    Returns a DataFrame of repeated rows likely to be headers/footers.\n",
    "    \"\"\"\n",
    "    repeated_all = []\n",
    "    df_mid = dflist[center_page]\n",
    "\n",
    "    for offset in offsets:\n",
    "        before, after = center_page - offset, center_page + offset\n",
    "        if before < 0 or after >= len(dflist):\n",
    "            continue\n",
    "\n",
    "        df_before, df_after = dflist[before], dflist[after]\n",
    "\n",
    "        for _, row in df_mid.iterrows():\n",
    "            text, top = row['text'].strip(), row['top']\n",
    "\n",
    "            def is_match(df):\n",
    "                return any(\n",
    "                    (abs(top - r['top']) <= tolerance) and (r['text'].strip() == text)\n",
    "                    for _, r in df.iterrows()\n",
    "                )\n",
    "\n",
    "            if is_match(df_before) and is_match(df_after):\n",
    "                repeated_all.append(row)\n",
    "\n",
    "    # Drop duplicates and return as DataFrame\n",
    "    return pd.DataFrame(repeated_all).drop_duplicates(subset=[\"text\", \"top\", \"font_size\"])\n",
    "\n",
    "def merge_paragraphs_by_font0(df, font_tolerance=1, line_gap=2.0):\n",
    "    \"\"\"\n",
    "    Groups consecutive lines with similar font size and small vertical gap into paragraphs.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): A single-page DataFrame containing 'top', 'font_size', 'text' columns.\n",
    "        font_tolerance (float): Allowed deviation in font size to consider lines as same style.\n",
    "        line_gap (float): Max vertical gap between lines to be considered part of the same paragraph.\n",
    "\n",
    "    Returns:\n",
    "        List of dicts: Each dict represents a merged paragraph block.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return []\n",
    "\n",
    "    # Sort by vertical position\n",
    "    # df_sorted = df.sort_values(by='top').reset_index(drop=True)\n",
    "    df_sorted = df\n",
    "    paragraphs = []\n",
    "    current_para = {\n",
    "        'top': df_sorted.loc[0, 'top'],\n",
    "        'font_size': df_sorted.loc[0, 'font_size'],\n",
    "        'text': df_sorted.loc[0, 'text']\n",
    "    }\n",
    "\n",
    "    for i in range(1, len(df_sorted)):\n",
    "        prev = df_sorted.loc[i - 1]\n",
    "        curr = df_sorted.loc[i]\n",
    "\n",
    "        same_font = (abs(curr['font_size'] - prev['font_size']) <= font_tolerance)\n",
    "        # close_enough = abs(curr['top'] - prev['top']) <= (prev['line_height'] + line_gap)\n",
    "\n",
    "        if same_font : #and close_enough:\n",
    "            current_para['text'] += ' ' + curr['text']\n",
    "        else:\n",
    "            paragraphs.append(current_para)\n",
    "            current_para = {\n",
    "                'top': curr['top'],\n",
    "                'font_size': curr['font_size'],\n",
    "                'text': curr['text']\n",
    "            }\n",
    "\n",
    "    paragraphs.append(current_para)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ec8f68fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div id=\"page0\" style=\"width:612.0pt;height:792.0pt\">\n",
      "<p style=\"top:94.6pt;left:90.0pt;line-height:20.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:20.0pt;color:#000000\">Ontario&#x2019;s Digital Library </span></p>\n",
      "<p style=\"top:122.0pt;left:90.0pt;line-height:16.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:16.0pt;color:#000000\">A Critical Component for Implementing Ontario&#x2019;s Road Map to </span></p>\n",
      "<p style=\"top:140.3pt;left:90.0pt;line-height:16.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:16.0pt;color:#000000\">Prosperity Strategy </span></p>\n",
      "<p style=\"top:199.3pt;left:96.0pt;line-height:12.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:12.0pt;color:#000000\">Summary </span></p>\n",
      "<p style=\"top:229.7pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">The purpose of this </span><b><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Request for Proposal</span></b><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\"> (</span><b><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">RFP</span></b><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">) is to invite firms and/or consultants to </span></p>\n",
      "<p style=\"top:242.3pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">present a proposal for developing the business plan for the </span><b><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Ontario Digital Library (ODL). </span></b></p>\n",
      "<p style=\"top:255.0pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">The ODL will deliver high-quality library electronic content to all Ontario residents in order to </span></p>\n",
      "<p style=\"top:267.8pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">assist people as they learn, work, and enhance their quality of life. The business plan to be </span></p>\n",
      "<p style=\"top:280.4pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">developed is to document and clearly communicate: </span></p>\n",
      "<p style=\"top:305.1pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">how the ODL will be implemented, including the timeline </span></p>\n",
      "<p style=\"top:317.7pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">the financial plan for the implementation </span></p>\n",
      "<p style=\"top:330.3pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">the financial plan for the first 2 operating years, including capital and operating costs, </span></p>\n",
      "<p style=\"top:343.0pt;left:132.6pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">revenues, etc. </span></p>\n",
      "<p style=\"top:355.6pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">a financial forecast for the succeeding 2 operating years </span></p>\n",
      "<p style=\"top:368.3pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">the services and products to be delivered by the ODL </span></p>\n",
      "<p style=\"top:380.9pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">how the ODL will operate and be managed following the implementation </span></p>\n",
      "<p style=\"top:393.5pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">who will be involved, and what their role/responsibility will be, for both the </span></p>\n",
      "<p style=\"top:406.2pt;left:132.6pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">implementation and operational stages </span></p>\n",
      "<p style=\"top:418.8pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">the marketing and communications plan for the ODL </span></p>\n",
      "<p style=\"top:444.1pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">This business plan must be completed and approved by the ODL Steering Committee no </span></p>\n",
      "<p style=\"top:456.7pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">later than September 30, 2003 </span></p>\n",
      "<p style=\"top:481.5pt;left:96.0pt;line-height:11.0pt\"><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Timeline: </span></i></p>\n",
      "<p style=\"top:506.0pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Those firms/consultants intended to submit a proposal to this RFP must indicate their </span></p>\n",
      "<p style=\"top:518.8pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">intention to do so in an e-mail to Michael Ridley (</span><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#0000ff\">mridley@uoguelph.ca</span><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">) by </span><b><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">April 11</span></i></b><sup><b><i><span style=\"font-family:Arial,sans-serif;font-size:7.0pt;color:#000000\">th</span></i></b></sup><b><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">. </span></i></b></p>\n",
      "<p style=\"top:543.4pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Proposals may be e-mailed, mailed, couriered or faxed to: Larry Moore </span></p>\n",
      "<p style=\"top:556.0pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">(</span><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#0000ff\">lmoore@accessola.com</span><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">), Executive Director, The Ontario Library Association, </span><span style=\"font-family:Arial,sans-serif;font-size:10.0pt;color:#000000\">100 Lombard </span></p>\n",
      "<p style=\"top:569.6pt;left:96.0pt;line-height:10.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:10.0pt;color:#000000\">St., Suite 303, Toronto, ON  M5C 1M3. </span><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Proposals must be received by </span><b><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Noon on Monday, </span></i></b></p>\n",
      "<p style=\"top:581.3pt;left:96.0pt;line-height:11.0pt\"><b><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">April 21, 2003. </span></i></b></p>\n",
      "<p style=\"top:606.0pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Those proposals that are short-listed will be invited to discuss their proposal during the week </span></p>\n",
      "<p style=\"top:618.6pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">of </span><b><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">April 28, 2003</span></i></b><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">. No presentation will be expected. Firms or consultants invited to an </span></p>\n",
      "<p style=\"top:631.4pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">interview will be expected to discuss the project and their approach with the selection </span></p>\n",
      "<p style=\"top:644.0pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">committee</span><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">. </span></i></p>\n",
      "<p style=\"top:668.6pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Contracts with the firm/consultant will be signed the week of </span><b><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">May 5, 2003</span></i></b><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\"> with the work to </span></p>\n",
      "<p style=\"top:681.3pt;left:96.1pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">commence as soon as possible thereafter. </span></p>\n",
      "<p style=\"top:734.8pt;left:90.0pt;line-height:7.6pt\"><b><i><span style=\"font-family:Arial,sans-serif;font-size:7.6pt;color:#000000\">RFP: To Develop the Ontario Digital Library Business Plan</span></i></b></p>\n",
      "<p style=\"top:734.8pt;left:296.7pt;line-height:7.6pt\"><b><i><span style=\"font-family:Arial,sans-serif;font-size:7.6pt;color:#000000\"> March 2003 </span></i></b></p>\n",
      "<p style=\"top:733.7pt;left:516.6pt;line-height:9.0pt\"><i><span style=\"font-family:Arial,sans-serif;font-size:9.0pt;color:#000000\">2</span></i><span style=\"font-family:Times New Roman,serif;font-size:9.0pt;color:#000000\"> </span></p>\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page = doc[1]\n",
    "print(page.get_text('html'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
