{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "# import pymupdf4llm\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import re\n",
    "from collections import Counter\n",
    "from extractandclean    import extract_blocks_and_distributions , extract_heading_summary , extract_crucial_pattern_lines , filter_and_clean_gibberish\n",
    "from merge              import merge_consecutive_same_font_and_left\n",
    "from headerfooter       import find_alternate_page_repeats_splitv2 , find_alternate_page_repeats_split , clean_pages_of_even_odd_repeats\n",
    "from tables             import extract_table_with_title\n",
    "from fontstat           import generate_font_stats , get_global_font_counter , get_rare_large_fonts , map_fonts_to_heading_levels , get_significant_large_fonts , create_font_level_map , get_top_fonts , classify_font_size , extract_heading_summary\n",
    "from analyzeandcompute  import unify_close_left_values , analyze_left_distribution , analyze_left_clusters , analyze_line_spacing , compute_word_count_stats , compute_line_spacing_per_page\n",
    "# md_text = pymupdf4llm.to_markdown(fileinquestion)\n",
    "# pathlib.Path(\"output.md\").write_bytes(md_text.encode())\n",
    "# llama_reader = pymupdf4llm.LlamaMarkdownReader()\n",
    "# print(llama_reader())\n",
    "# llama_docs = llama_reader.load_data(fileinquestion)\n",
    "\n",
    "#tesseract ocr\n",
    "# english\n",
    "# russian\n",
    "# chinese (new)\n",
    "# mandarin (traditional)\n",
    "# japanese\n",
    "# Hindi\n",
    "# Arabic\n",
    "# French\n",
    "# Hebrew\n",
    "# German\n",
    "# Korean\n",
    "# Italian\n",
    "# Polish\n",
    "# Portugese\n",
    "# Spanish \n",
    "# Indonesian3\n",
    "#turkish\n",
    "# Urdu\n",
    "#pip install pymupdf numpy pandas sentence-transformers\n",
    "#################### workflow\n",
    "#parallel processing\n",
    "#trigger - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "###################################### something for future\n",
    "# import re\n",
    "\n",
    "# def is_gibberish(text):\n",
    "#     text = text.strip()\n",
    "#     if len(text) < min_length:\n",
    "#         return True\n",
    "#     if re.fullmatch(r'[\\W_]{2,}', text):  # full of symbols\n",
    "#         return True\n",
    "#     alpha_count = sum(c.isalpha() for c in text)\n",
    "#     return (alpha_count / len(text)) < min_alpha_ratio\n",
    "# from langdetect import detect, DetectorFactory\n",
    "# DetectorFactory.seed = 0\n",
    "\n",
    "# def is_gibberish(text): #textstat or langdetect for readability or language detection\n",
    "#     text = text.strip()\n",
    "#     if len(text) < min_length:\n",
    "#         return True\n",
    "#     try:\n",
    "#         lang = detect(text)\n",
    "#         return False  # it's a real language\n",
    "#     except:\n",
    "#         return True  # detection failed = likely \n",
    "    \n",
    "# from nltk.corpus import words\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# word_set = set(words.words())\n",
    "\n",
    "# def is_gibberish(text): # Word Tokenization and Dictionary Matching\n",
    "#     tokens = word_tokenize(text.lower())\n",
    "#     meaningful_words = [t for t in tokens if t in word_set]\n",
    "#     if len(text.strip()) < min_length:\n",
    "#         return True\n",
    "#     if len(meaningful_words) / (len(tokens) + 1e-6) < 0.3:\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "# def is_gibberish(text): #Multiple Heuristics\n",
    "#     text = text.strip()\n",
    "#     if len(text) < min_length:\n",
    "#         return True\n",
    "#     if re.fullmatch(r'[\\W_]{2,}', text):\n",
    "#         return True\n",
    "#     alpha_ratio = sum(c.isalpha() for c in text) / (len(text) + 1e-6)\n",
    "#     if alpha_ratio < min_alpha_ratio:\n",
    "#         return True\n",
    "#     try:\n",
    "#         lang = detect(text)\n",
    "#         if lang not in ['en']:  # restrict to English\n",
    "#             return True\n",
    "#     except:\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "\n",
    "def is_center_aligned(left, text, page_width, font_size, tolerance=15): ######make a freaking plot on desmos if required!?!?!?!?!\n",
    "    tolerance=page_width*(tolerance/100)\n",
    "    avg_char_width = 0.5 * font_size  # rough estimate\n",
    "    text_width = len(text) * avg_char_width\n",
    "    center_text = left + text_width / 2\n",
    "    center_page = page_width / 2\n",
    "    return abs(center_page - center_text) #<= tolerance\n",
    "# is_center_aligned(78.2, dflist[0].iloc[0]['text'], width, 14.3 )\n",
    "# dflist[0]\n",
    "# is_center_aligned(121.8, dflist[0].iloc[0]['text'], width, 10)\n",
    "\n",
    "# is_center_aligned(141.3, dflist[0].iloc[0]['text'], width, 6)\n",
    "# is_center_aligned(236.7, dflist[0].iloc[0]['text'], width, 14.3)\n",
    "\n",
    "# body_font, large_fonts = get_large_fonts(font_counter, method='rms')\n",
    "# page = doc[2]\n",
    "# table_df, table_label = extract_table_with_title(page)\n",
    "\n",
    "# if table_label:\n",
    "#     print(\"Detected Table Title:\", table_label[\"text\"])\n",
    "# extract_blocks_and_distributions,\n",
    "# find_alternate_page_repeats_split,\n",
    "# clean_pages_of_even_odd_repeats,\n",
    "# merge_paragraphs_by_font\n",
    "\n",
    "\n",
    "def combine_dflist_to_master_df(dflist):\n",
    "    \"\"\"\n",
    "    Combines a list of page-level DataFrames into a single master DataFrame.\n",
    "    Adds a 'page_number' column to indicate the source page for each row.\n",
    "    Parameters:\n",
    "        dflist (list of pd.DataFrame): A list where each DataFrame corresponds to one PDF page.\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with an added 'page_number' column.\n",
    "    \"\"\"\n",
    "    combined_rows = []\n",
    "    for page_number, df in enumerate(dflist):\n",
    "        if df is not None and not df.empty:\n",
    "            df_copy = df.copy()\n",
    "            df_copy['page_number'] = page_number\n",
    "            combined_rows.append(df_copy)\n",
    "\n",
    "    if not combined_rows:\n",
    "        return pd.DataFrame()  # Return empty DataFrame if nothing valid\n",
    "\n",
    "    master_df = pd.concat(combined_rows, ignore_index=True)\n",
    "    return master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "fileinquestion = \"C1A/input/E0CCG5S239.pdf\" #single page doc E0CCG5S239\n",
    "fileinquestion = \"C1A/input/TOPJUMP-PARTY-INVITATION-20161003-V01.pdf\"\n",
    "fileinquestion = \"C1A/input/STEMPathwaysFlyer.pdf\"\n",
    "fileinquestion = \"CustomPDFs/killer.pdf\"\n",
    "fileinquestion = \"CustomPDFs/jess401.pdf\"\n",
    "fileinquestion = \"C1A/input/E0CCG5S312.pdf\"   #easy\n",
    "fileinquestion = \"C1A/input/E0H1CM114.pdf\" ######### hard\n",
    "doc = pymupdf.open(fileinquestion)  \n",
    "page = doc[0]  # first page\n",
    "width , height = page.rect.width , page.rect.height\n",
    "doctoc = doc.get_toc()\n",
    "docmetadata = doc.metadata\n",
    "totalpage= doc.page_count\n",
    "dflist   = []\n",
    "mid = totalpage//2\n",
    "\n",
    "for i in range(totalpage):\n",
    "    page = doc.load_page(i)\n",
    "    html = page.get_text(\"html\")\n",
    "    blocks= extract_blocks_and_distributions(html)\n",
    "    dflist.append(pd.DataFrame(blocks))############### to dict if we do async\n",
    "dfl = dflist\n",
    "even_df, odd_df =   find_alternate_page_repeats_splitv2(dflist, mid=totalpage // 2)\n",
    "dflist          =   clean_pages_of_even_odd_repeats(dflist, even_df, odd_df)\n",
    "dflist          =   [filter_and_clean_gibberish(df, min_alnum_ratio=0.15, min_length=1) for df in dflist] \n",
    "averagewords = compute_word_count_stats(dflist)['global_avg_words_per_line']\n",
    "font_stats_df = generate_font_stats(dflist)\n",
    "\n",
    "dflist = unify_close_left_values(dflist, tolerance=width*0.01, min_left=0, max_left=width)\n",
    "\n",
    "################add it back\n",
    "#['page_number', 'font_size', 'font_family', 'count', 'total_words','avg_word_density']\n",
    "# fontfamily_counter = Counter(font_stats_df['font_family'])\n",
    "# fontsize_counter = Counter(font_stats_df['font_size'])\n",
    "\n",
    "font_word_counts = (font_stats_df.groupby('font_size')['total_words'].sum().sort_values(ascending=False) )\n",
    "# master_df = combine_dflist_to_master_df(dflist)\n",
    "target_fonts = get_significant_large_fonts(font_word_counts, z_thresh=0.5)['large_fonts']\n",
    "# print(\"Large fonts:\", result['large_fonts']) #print(\"Z-scores:\\n\", result['z_scores']) #print(\"Body font:\", result['body_font'])\n",
    "\n",
    "spacing_list = compute_line_spacing_per_page(dflist)\n",
    "all_spacings = np.concatenate(spacing_list)# If you want to concatenate all spacing into a single array:\n",
    "spacing_stats = analyze_line_spacing(spacing_list)\n",
    "meansp =  spacing_stats['mean_spacing']\n",
    "\n",
    "dflist = merge_consecutive_same_font_and_left(dflist,list(font_word_counts.keys()),[meansp*0.4,meansp*1.75])\n",
    "leftdf = analyze_left_distribution(dflist)['left_stats_df'] # 'left_counter': left_counter,# 'dominant_lefts': dominant_lefts,# 'left_stats_df': left_stats_df\n",
    "leftdf.loc[(leftdf.left<=204) & (leftdf.percentage>=1)]\n",
    "\n",
    "master_df = combine_dflist_to_master_df(dflist)\n",
    "\n",
    "column_order = ['page_number',  'left',\"top\", 'end' ,\"text\",'line_height','font_size','font_family','bold','italic','word_count','word_density','parabool']\n",
    "master_df[column_order]\n",
    "master_df['left'] = master_df['left'].round(2)\n",
    "\n",
    "sortmm = (master_df.loc[master_df.left<width*0.35])\n",
    "unique_lefts = sorted(set(val for val in sortmm['left'].unique()))\n",
    "unique_lefts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Usage\n",
    "df_summary = analyze_left_clusters([master_df], unique_lefts) #uniql are ascending\n",
    "df_summary\n",
    "# pprint(master_df.loc[(master_df.left==unique_lefts[0])& (master_df.word_count<=10)][['top','line_height','font_size','bold','italic','text','word_count','parabool']])\n",
    "print(font_word_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac1bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "fileinquestion = \"C1A/input/E0CCG5S239.pdf\" #single page doc E0CCG5S239\n",
    "fileinquestion = \"C1A/input/TOPJUMP-PARTY-INVITATION-20161003-V01.pdf\"\n",
    "fileinquestion = \"C1A/input/STEMPathwaysFlyer.pdf\"\n",
    "fileinquestion = \"C1A/input/E0H1CM114.pdf\"\n",
    "fileinquestion = \"AAMine/killer.pdf\"\n",
    "fileinquestion = \"C1A/input/E0H1CM114.pdf\"\n",
    "fileinquestion = \"AAMine/killer.pdf\"\n",
    "fileinquestion = \"C1A/input/E0CCG5S312.pdf\"\n",
    "fileinquestion = \"AAMine/jess401.pdf\"\n",
    "fileinquestion = \"C1A/input/E0CCG5S312.pdf\" #single page doc E0CCG5S239 #easy\n",
    "\n",
    "\n",
    "#E0CCG5S312 eazy 12 pages\n",
    "#hard E0H1CM114\n",
    "doc = pymupdf.open(fileinquestion)  \n",
    "page = doc[0]  # first page\n",
    "\n",
    "width = page.rect.width\n",
    "height = page.rect.height\n",
    "doctoc = doc.get_toc()\n",
    "docmetadata = doc.metadata\n",
    "totalpage= doc.page_count\n",
    "\n",
    "dflist , repeated_all, all_font_sizes , font_counter = [], [] ,set() , Counter()\n",
    "mid = totalpage//2\n",
    "\n",
    "#if dig.dig is followed then try to use the flow...\n",
    "for i in range(totalpage):\n",
    "    page = doc.load_page(i)\n",
    "    html = page.get_text(\"html\")\n",
    "    blocks, font_counter, line_counter = extract_blocks_and_distributions(html)\n",
    "    dflist.append(pd.DataFrame(blocks))\n",
    "dfl = dflist\n",
    "\n",
    "dfcrucial = extract_crucial_pattern_lines(dflist)\n",
    "dfcrucial############################################################################################################################\n",
    "dfcrucial.loc[dfcrucial.bold==True]\n",
    "\n",
    "get_top_fonts(font_counter)\n",
    "\n",
    "even_df, odd_df =       find_alternate_page_repeats_split(dflist, mid=totalpage // 2)\n",
    "dflist =                clean_pages_of_even_odd_repeats(dflist, even_df, odd_df)\n",
    "dflist =        [filter_gibberish_rows(df) for df in dflist]\n",
    "\n",
    "#mode range \n",
    "font_stats_df = generate_font_stats(dflist)\n",
    "font_stats_df.sort_values(by=['page_number', 'font_size'], ascending=[True, False]).reset_index(drop=True)\n",
    "font_counter = get_global_font_counter(font_stats_df)\n",
    "topfonts = get_top_fonts(font_counter)\n",
    "# font_level_map =  create_font_level_map(font_counter) #classify_font_size(16, font_level_map)\n",
    "\n",
    "target_fonts = list(font_counter.keys())\n",
    "\n",
    "#merging it now\n",
    "dflist = merge_consecutive_same_font_and_left(dflist, target_fonts)\n",
    "dflist = recalculate_word_stats(dflist)\n",
    "\n",
    "\n",
    "font_counter\n",
    "body_font, large_fonts = get_rare_large_fonts(font_counter, method='rms', freq_filter='average')\n",
    "\n",
    "# even_df, odd_df = find_alternate_page_repeats_split(dflist, mid=totalpage // 2)\n",
    "# dflist = clean_pages_of_even_odd_repeats(dflist, even_df, odd_df)\n",
    "# font_level_map =  create_font_level_map(font_counter) #classify_font_size(16, font_level_map)\n",
    "font_heading_map = map_fonts_to_heading_levels(large_fonts)\n",
    "\n",
    "dflist = label_font_levels(dflist, font_heading_map) #returns the dflist\n",
    "result = analyze_word_density_patterns(\n",
    "    dflist,\n",
    "    method='density_by_font',\n",
    "    stat='median',\n",
    "    threshold_factor=1.8\n",
    ")\n",
    "\n",
    "# print(f\"Metric: {result['metric_name']}\")\n",
    "# print(f\"Central Value: {result['central_value']}\")\n",
    "# print(f\"Dense Lines: {len(result['dense_lines'])}, Sparse Lines: {len(result['sparse_lines'])}\")\n",
    "\n",
    "# for i in range(totalpage):\n",
    "#     extract_table_with_title(doc[i])\n",
    "dfcrucial\n",
    "extract_heading_summary(dflist, levels=(['H1','H2','H3']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71623e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileinquestion = \"C1A/input/E0CCG5S312.pdf\" #single page doc E0CCG5S239\n",
    "#E0CCG5S312 eazy 12 pages\n",
    "#hard E0H1CM114\n",
    "doc = pymupdf.open(fileinquestion)  \n",
    "page = doc[3]  # first page\n",
    "\n",
    "            # “text”: (default) plain text with line breaks. No formatting, no text position details, no images.\n",
    "            # “blocks”: generate a list of text blocks (= paragraphs).\n",
    "            # “words”: generate a list of words (strings not containing spaces).\n",
    "            # “html”: creates a full visual version of the page including any images. This can be displayed with your internet browser.\n",
    "            # “dict” / “json”: same information level as HTML, but provided as a Python dictionary or resp. JSON string. See TextPage.extractDICT() for details of its structure.\n",
    "            # “rawdict” / “rawjson”: a super-set of “dict” / “json”. It additionally provides character detail information like XML. See TextPage.extractRAWDICT() for details of its structure.\n",
    "            # “xhtml”: text information level as the TEXT version but includes images. Can also be displayed by internet browsers.\n",
    "            # “xml”: contains no images, but full position and font information down to each single text character. Use an XML module to interpret.\n",
    "ab = page.get_text('xhtml')\n",
    "pprint(ab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78925123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### async\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import time\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import Counter\n",
    "fileinquestion = \"C1A/input/E0H1CM114.pdf\" ######### hard\n",
    "\n",
    "# Dummy version of your function (replace with your real one)\n",
    "def extract_blocks_and_distributions(html):\n",
    "    from random import randint\n",
    "    # Dummy block list\n",
    "    blocks = [{\"text\": f\"Dummy {i}\", \"font\": \"FontX\", \"size\": randint(10, 14)} for i in range(randint(5, 15))]\n",
    "    font_counter = Counter({f\"FontX-{randint(10,14)}\": randint(1, 5)})\n",
    "    line_counter = Counter()\n",
    "    return blocks, font_counter, line_counter\n",
    "\n",
    "# ---- SEQUENTIAL VERSION ----\n",
    "def process_sequential(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    dflist = []\n",
    "    font_counter_total = Counter()\n",
    "    for i in range(doc.page_count):\n",
    "        page = doc.load_page(i)\n",
    "        html = page.get_text(\"html\")\n",
    "        blocks, font_counter, line_counter = extract_blocks_and_distributions(html)\n",
    "        dflist.append(pd.DataFrame(blocks))\n",
    "        font_counter_total.update(font_counter)\n",
    "    return dflist, font_counter_total\n",
    "\n",
    "# ---- ASYNC MULTITHREAD VERSION ----\n",
    "def process_page_threaded(doc_path, page_num):\n",
    "    with fitz.open(doc_path) as doc:\n",
    "        page = doc.load_page(page_num)\n",
    "        html = page.get_text(\"html\")\n",
    "        blocks, font_counter, line_counter = extract_blocks_and_distributions(html)\n",
    "        return pd.DataFrame(blocks), font_counter\n",
    "\n",
    "async def process_async(file_path, max_workers=4):\n",
    "    loop = asyncio.get_running_loop()\n",
    "    dflist = []\n",
    "    font_counter_total = Counter()\n",
    "    total_pages = fitz.open(file_path).page_count\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        tasks = [\n",
    "            loop.run_in_executor(executor, process_page_threaded, file_path, i)\n",
    "            for i in range(total_pages)\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for df, counter in results:\n",
    "        dflist.append(df)\n",
    "        font_counter_total.update(counter)\n",
    "\n",
    "    return dflist, font_counter_total\n",
    "\n",
    "# ---- TIMING TEST ----\n",
    "fileinquestion = fileinquestion  # Change this to your file path\n",
    "\n",
    "# Time sequential\n",
    "start_seq = time.time()\n",
    "dflist_seq, font_counter_seq = process_sequential(fileinquestion)\n",
    "end_seq = time.time()\n",
    "\n",
    "# Time async\n",
    "start_async = time.time()\n",
    "dflist_async, font_counter_async = asyncio.run(process_async(fileinquestion, max_workers=8))\n",
    "end_async = time.time()\n",
    "\n",
    "# ---- RESULT COMPARISON ----\n",
    "print(f\"⏱ Sequential time:      {end_seq - start_seq:.4f} seconds\")\n",
    "print(f\"⚡ Async (Threaded) time: {end_async - start_async:.4f} seconds\")\n",
    "\n",
    "# Optional: verify correctness\n",
    "print(f\"\\nPages (Sequential): {len(dflist_seq)}, Pages (Async): {len(dflist_async)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aef184",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflist = []\n",
    "for i in range(totalpage):\n",
    "    page = doc.load_page(i)\n",
    "    html = page.get_text(\"html\")\n",
    "    blocks, font_counter, line_counter = extract_blocks_and_distributions(html)\n",
    "    dflist.append(pd.DataFrame(blocks))\n",
    "\n",
    "even_df, odd_df = find_alternate_page_repeats_split(dflist, mid=totalpage // 2)\n",
    "dflist = clean_pages_of_even_odd_repeats(dflist, even_df, odd_df)\n",
    "\n",
    "def find_repeating_rows(dflist, center_page, offsets=[1, 2], tolerance=1.0):#########depriciated\n",
    "    \"\"\"\n",
    "    Finds text lines (rows) in the center page that repeat in neighboring pages at given offsets.\n",
    "    Returns a DataFrame of repeated rows likely to be headers/footers.\n",
    "    \"\"\"\n",
    "    repeated_all = []\n",
    "    df_mid = dflist[center_page]\n",
    "\n",
    "    for offset in offsets:\n",
    "        before, after = center_page - offset, center_page + offset\n",
    "        if before < 0 or after >= len(dflist):\n",
    "            continue\n",
    "\n",
    "        df_before, df_after = dflist[before], dflist[after]\n",
    "\n",
    "        for _, row in df_mid.iterrows():\n",
    "            text, top = row['text'].strip(), row['top']\n",
    "\n",
    "            def is_match(df):\n",
    "                return any(\n",
    "                    (abs(top - r['top']) <= tolerance) and (r['text'].strip() == text)\n",
    "                    for _, r in df.iterrows()\n",
    "                )\n",
    "\n",
    "            if is_match(df_before) and is_match(df_after):\n",
    "                repeated_all.append(row)\n",
    "\n",
    "    # Drop duplicates and return as DataFrame\n",
    "    return pd.DataFrame(repeated_all).drop_duplicates(subset=[\"text\", \"top\", \"font_size\"])\n",
    "\n",
    "repeated_rows_df = find_repeating_rows(dflist, center_page=mid)\n",
    "# dflist_pages_of_repeats(dflist, repeated_rows_df)\n",
    "# repeated_rows_df\n",
    "# dflist\n",
    "# # for i,clean in enumerate(dflist):\n",
    "#     # clean.to_csv(f\"analysis/hope12/f{i}.csv\",index=False,encoding=\"utf-8\")\n",
    "\n",
    "dflister_gibberish_rows(df) for df in dflist\n",
    "get_top_fonts(font_counter)\n",
    "font_level_map =  create_font_level_map(font_counter) #classify_font_size(16, font_level_map)\n",
    "dflist_font_levels(dflist)\n",
    "extract_heading_summary(dflist, levels=(['H1','H2']))\n",
    "font_counter\n",
    "i=0\n",
    "# repeated_rows_df\n",
    "dflist[1]\n",
    "font_level_map\n",
    "extract_heading_summary(dflist, levels=(['H1','H2']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileinquestion = \"C1A/input/E0H1CM114.pdf\"\n",
    "#E0CCG5S312 eazy 12 pages\n",
    "doc = pymupdf.open(fileinquestion)  \n",
    "doctoc = doc.get_toc()\n",
    "docmetadata = doc.metadata\n",
    "totalpage= doc.page_count\n",
    "\n",
    "dflist , repeated_all, all_font_sizes , font_counter = [], [] ,set() , Counter()\n",
    "mid = totalpage//2\n",
    "\n",
    "#if dig.dig is followed then try to use the flow...\n",
    "\n",
    "for i in range(totalpage):\n",
    "    page = doc.load_page(i)\n",
    "    html = page.get_text(\"html\")\n",
    "    blocks, font_counter, line_counter = extract_blocks_and_distributions(html)\n",
    "    dflist.append(pd.DataFrame(blocks))\n",
    "\n",
    "even_df, odd_df = find_alternate_page_repeats_split(dflist, mid=totalpage // 2)\n",
    "dflist_pages_of_even_odd_repeats(dflist, even_df, odd_df)\n",
    "\n",
    "paragraphs = []\n",
    "for i, df in enumerate(dflist\n",
    "    merged = merge_paragraphs_by_font(df, font_min=8.0, font_max=14.0)  # You can define ranges\n",
    "    for para in merged:\n",
    "        para['page'] = i\n",
    "    paragraphs.extend(para)\n",
    "\n",
    "paragraph_df = pd.DataFrame(paragraphs)\n",
    "merged = merge_paragraphs_by_font(df, font_min=8.0, font_max=14.0, font_tolerance=1.0, line_gap=3.0)\n",
    "\n",
    "# repeated_rows_df = find_repeating_rows(dflist, center_page=mid)\n",
    "# dflist_pages_of_repeats(dflist, repeated_rows_df)\n",
    "# repeated_rows_df\n",
    "# dflist\n",
    "# # for i,clean in enumerate(dflist\n",
    "#     # clean.to_csv(f\"analysis/hope12/f{i}.csv\",index=False,encoding=\"utf-8\")\n",
    "\n",
    "dflister_gibberish_rows(df) for df in dflist\n",
    "get_top_fonts(font_counter)\n",
    "font_level_map =  create_font_level_map(font_counter) #classify_font_size(16, font_level_map)\n",
    "dflist = label_font_levels(dflist, font_level_map)\n",
    "extract_heading_summary(dflist=(['H1','H2']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, page_df in enumerate(dflist\n",
    "    paras = merge_paragraphs_by_font(page_df)\n",
    "    for p in paras:\n",
    "        p['page'] = i  # Add page number\n",
    "    merged_paras.extend(paras)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "paragraph_df = pd.DataFrame(merged_paras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############EXP\n",
    "tablezdf\n",
    "for i in range(totalpage):\n",
    "    page = doc[i]\n",
    "    table_df, title = extract_table_with_title(page)\n",
    "\n",
    "    if table_df is not None:\n",
    "        print(f\"\\nTitle: {title}\\n\")\n",
    "        print(table_df.head())\n",
    "        tabs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRAVEYARD\n",
    "\n",
    "# page = doc[1] #not index, its page\n",
    "# for page in reversed(doc):\n",
    "# for page in doc.pages(start, stop, step):\n",
    "locationn= \"analysis/hope1\"\n",
    "alldfs,thetexts , linkstotal , linkstotalnext , annotstotal , html = [] , [] , [] , [] , [] , []\n",
    "def dumpshit(doc):\n",
    "    for i,page in enumerate(doc): # iterate the document pages\n",
    "        text = page.get_text() # get plain text encoded as UTF-8\n",
    "        thetexts.append(text)\n",
    "        links = page.get_links()\n",
    "        linkstotal.append(links)\n",
    "        link = page.first_link  # a `Link` object or `None`\n",
    "        linktemp =[]\n",
    "        while link: \n",
    "            link = link.next # get next link, last one has `None` in its `next`\n",
    "            linktemp.append(link)\n",
    "        linkstotalnext.append(linktemp)\n",
    "        # for annot in page.annots():\n",
    "            # print(f'Annotation on page: {page.number} with type: {annot.type} and rect: {annot.rect}')\n",
    "        # for field in page.widgets():\n",
    "            # print(f'Widget on page: {page.number} with type: {field.type} and rect: {field.rect}')\n",
    "        text = page.get_text(\"html\")\n",
    "            # Use one of the following strings for opt to obtain different formats [2]:\n",
    "            # “text”: (default) plain text with line breaks. No formatting, no text position details, no images.\n",
    "            # “blocks”: generate a list of text blocks (= paragraphs).\n",
    "            # “words”: generate a list of words (strings not containing spaces).\n",
    "            # “html”: creates a full visual version of the page including any images. This can be displayed with your internet browser.\n",
    "            # “dict” / “json”: same information level as HTML, but provided as a Python dictionary or resp. JSON string. See TextPage.extractDICT() for details of its structure.\n",
    "            # “rawdict” / “rawjson”: a super-set of “dict” / “json”. It additionally provides character detail information like XML. See TextPage.extractRAWDICT() for details of its structure.\n",
    "            # “xhtml”: text information level as the TEXT version but includes images. Can also be displayed by internet browsers.\n",
    "            # “xml”: contains no images, but full position and font information down to each single text character. Use an XML module to interpret.\n",
    "        # with open(\"output.html\", \"w\") as f:\n",
    "        text = re.sub(r'<img[^>]*>', '<image>', text)\n",
    "        html.append(text)\n",
    "        with open(f\"analysis/hope1/f{i}.html\",'w',encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    " \n",
    "        # Font\n",
    "        # Alignment\n",
    "        # cases\n",
    "        #indentation, new pages, \n",
    "        # 1 1.0\n",
    "        #toc/index - \n",
    "        # blocks of code\n",
    "        # \n",
    "# for i in range(12)\n",
    "page= doc[0]\n",
    "text = page.get_text(\"html\")\n",
    "# pprint(text)\n",
    "print(doc[1].get_text(\"blocks\") == doc[1].get_text(\"blocks\"))\n",
    "htmx=(doc[0].get_text(\"html\"))\n",
    "with open (\"html.html\", \"w\") as f:\n",
    "    f.write(htmx)\n",
    "htmx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d236ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Depreciated \n",
    "def clean_pages_of_even_odd_repeats(dflist, even_df, odd_df, tolerance=1.0):\n",
    "    \"\"\"\n",
    "    Cleans even-indexed pages using even_df and odd-indexed pages using odd_df.\n",
    "    If even_df == odd_df, treats all pages the same.\n",
    "    \"\"\"\n",
    "    cleaned_pages = []\n",
    "    # Check if both are the same\n",
    "    same_repeats = ( set((r['text'].strip(), round(r['top'], 1)) for _, r in even_df.iterrows()) == set((r['text'].strip(), round(r['top'], 1)) for _, r in odd_df.iterrows()) )\n",
    "    for idx, df in enumerate(dflist):\n",
    "        if same_repeats:\n",
    "            compare_df = even_df  # or odd_df, doesn't matter\n",
    "        else:\n",
    "            compare_df = even_df if idx % 2 == 0 else odd_df\n",
    "        mask = df.apply(\n",
    "            lambda row: not any(\n",
    "                (abs(row['top'] - rep['top']) <= tolerance)\n",
    "                and (row['text'].strip() == rep['text'].strip())\n",
    "                for _, rep in compare_df.iterrows()\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        cleaned_df = df[mask].reset_index(drop=True)\n",
    "        cleaned_pages.append(cleaned_df)\n",
    "    return cleaned_pages\n",
    "\n",
    "def clean_pages_of_repeats(dflist, repeated_df, tolerance=1.0): ##################depreciated\n",
    "    \"\"\"\n",
    "    Removes rows from each page in dflist that match (in position and text) any row in repeated_df.\n",
    "    Useful for cleaning common headers/footers.\n",
    "    \"\"\"\n",
    "    cleaned_pages = []\n",
    "\n",
    "    for df in dflist:\n",
    "        mask = df.apply(\n",
    "            lambda row: not any(\n",
    "                (abs(row['top'] - rep['top']) <= tolerance) and (row['text'].strip() == rep['text'].strip())\n",
    "                for _, rep in repeated_df.iterrows()\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        cleaned_df = df[mask].reset_index(drop=True)\n",
    "        cleaned_pages.append(cleaned_df)\n",
    "\n",
    "    return cleaned_pages\n",
    "\n",
    "\n",
    "def find_repeating_rows(dflist, center_page, offsets=[1, 2], tolerance=1.0):#########depriciated\n",
    "    \"\"\"\n",
    "    Finds text lines (rows) in the center page that repeat in neighboring pages at given offsets.\n",
    "    Returns a DataFrame of repeated rows likely to be headers/footers.\n",
    "    \"\"\"\n",
    "    repeated_all = []\n",
    "    df_mid = dflist[center_page]\n",
    "\n",
    "    for offset in offsets:\n",
    "        before, after = center_page - offset, center_page + offset\n",
    "        if before < 0 or after >= len(dflist):\n",
    "            continue\n",
    "\n",
    "        df_before, df_after = dflist[before], dflist[after]\n",
    "\n",
    "        for _, row in df_mid.iterrows():\n",
    "            text, top = row['text'].strip(), row['top']\n",
    "\n",
    "            def is_match(df):\n",
    "                return any(\n",
    "                    (abs(top - r['top']) <= tolerance) and (r['text'].strip() == text)\n",
    "                    for _, r in df.iterrows()\n",
    "                )\n",
    "\n",
    "            if is_match(df_before) and is_match(df_after):\n",
    "                repeated_all.append(row)\n",
    "\n",
    "    # Drop duplicates and return as DataFrame\n",
    "    return pd.DataFrame(repeated_all).drop_duplicates(subset=[\"text\", \"top\", \"font_size\"])\n",
    "\n",
    "def find_alternate_page_repeats_splitold(dflist, mid):\n",
    "    def get_matches(df_base, df1, df2):\n",
    "        matched = []\n",
    "        for _, row in df_base.iterrows():\n",
    "            text = row['text'].strip()\n",
    "            top = row['top']\n",
    "\n",
    "            def match(df):\n",
    "                return any(\n",
    "                    (abs(top - r['top']) <= 1.0) and (r['text'].strip() == text)\n",
    "                    for _, r in df.iterrows()\n",
    "                )\n",
    "            if match(df1) and match(df2):\n",
    "                matched.append(row)\n",
    "        return pd.DataFrame(matched)\n",
    "    even_matches, odd_matches = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Mid page even\n",
    "    if mid % 2 == 0:\n",
    "        page_even = dflist[mid]\n",
    "        before_even = dflist[mid - 2] if mid - 2 >= 0 else None\n",
    "        after_even = dflist[mid + 2] if mid + 2 < len(dflist) else None\n",
    "\n",
    "        if before_even is not None and after_even is not None:\n",
    "            even_matches = get_matches(page_even, before_even, after_even)\n",
    "        elif before_even is not None:\n",
    "            even_matches = get_matches(page_even, before_even)\n",
    "        elif after_even is not None:\n",
    "            even_matches = get_matches(page_even, after_even)\n",
    "\n",
    "    # Mid - 1 page odd\n",
    "    if mid - 1 >= 0 and (mid - 1) % 2 == 1:\n",
    "        odd_page_idx = mid - 1\n",
    "        page_odd = dflist[odd_page_idx]\n",
    "        before_odd = dflist[odd_page_idx - 2] if odd_page_idx - 2 >= 0 else None\n",
    "        after_odd = dflist[odd_page_idx + 2] if odd_page_idx + 2 < len(dflist) else None\n",
    "\n",
    "        if before_odd is not None and after_odd is not None:\n",
    "            odd_matches = get_matches(page_odd, before_odd, after_odd)\n",
    "        elif before_odd is not None:\n",
    "            odd_matches = get_matches(page_odd, before_odd)\n",
    "        elif after_odd is not None:\n",
    "            odd_matches = get_matches(page_odd, after_odd)\n",
    "    return even_matches, odd_matches\n",
    "\n",
    "def merge_paragraphs_by_font0(df, font_tolerance=1, line_gap=2.0):\n",
    "    \"\"\"\n",
    "    Groups consecutive lines with similar font size and small vertical gap into paragraphs.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): A single-page DataFrame containing 'top', 'font_size', 'text' columns.\n",
    "        font_tolerance (float): Allowed deviation in font size to consider lines as same style.\n",
    "        line_gap (float): Max vertical gap between lines to be considered part of the same paragraph.\n",
    "\n",
    "    Returns:\n",
    "        List of dicts: Each dict represents a merged paragraph block.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return []\n",
    "\n",
    "    # Sort by vertical position\n",
    "    # df_sorted = df.sort_values(by='top').reset_index(drop=True)\n",
    "    df_sorted = df\n",
    "    paragraphs = []\n",
    "    current_para = {\n",
    "        'top': df_sorted.loc[0, 'top'],\n",
    "        'font_size': df_sorted.loc[0, 'font_size'],\n",
    "        'text': df_sorted.loc[0, 'text']\n",
    "    }\n",
    "\n",
    "    for i in range(1, len(df_sorted)):\n",
    "        prev = df_sorted.loc[i - 1]\n",
    "        curr = df_sorted.loc[i]\n",
    "\n",
    "        same_font = (abs(curr['font_size'] - prev['font_size']) <= font_tolerance)\n",
    "        # close_enough = abs(curr['top'] - prev['top']) <= (prev['line_height'] + line_gap)\n",
    "\n",
    "        if same_font : #and close_enough:\n",
    "            current_para['text'] += ' ' + curr['text']\n",
    "        else:\n",
    "            paragraphs.append(current_para)\n",
    "            current_para = {\n",
    "                'top': curr['top'],\n",
    "                'font_size': curr['font_size'],\n",
    "                'text': curr['text']\n",
    "            }\n",
    "\n",
    "    paragraphs.append(current_para)\n",
    "    return \n",
    "\n",
    "\n",
    "def merge_consecutive_same_font_and_leftoldnew(dflist, target_fonts):\n",
    "    \"\"\"\n",
    "    Merges lines in-place within each DataFrame in dflist where:\n",
    "    - font_size ∈ target_fonts\n",
    "    - consecutive lines share same font_size and left\n",
    "    Adds a 'parabool' column to mark whether a line is a merged paragraph.\n",
    "    \n",
    "    Parameters:\n",
    "        dflist (list of pd.DataFrame): Each DataFrame is a page.\n",
    "        target_fonts (list): List of font sizes to consider for paragraph merging.\n",
    "    \n",
    "    Returns:\n",
    "        list of pd.DataFrame: Modified DataFrames with merged paragraphs and 'parabool' flag.\n",
    "    \"\"\"\n",
    "    updated_dflist = []\n",
    "\n",
    "    for df in dflist:\n",
    "        if df.empty or 'left' not in df.columns or 'font_size' not in df.columns:\n",
    "            df['parabool'] = False\n",
    "            updated_dflist.append(df)\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df['parabool'] = False\n",
    "        drop_indices = set()\n",
    "\n",
    "        for left_value in sorted(df['left'].unique()):\n",
    "            # Get indices where left matches\n",
    "            group_df = df[(df['left'] == left_value) & (df['font_size'].isin(target_fonts))]\n",
    "            indices = group_df.index.tolist()\n",
    "            i = 0\n",
    "\n",
    "            while i < len(indices):\n",
    "                current_idx = indices[i]\n",
    "                current_font = df.loc[current_idx, 'font_size']\n",
    "                current_text = df.loc[current_idx, 'text']\n",
    "\n",
    "                merged = False\n",
    "                j = i + 1\n",
    "\n",
    "                while j < len(indices):\n",
    "                    next_idx = indices[j]\n",
    "                    next_font = df.loc[next_idx, 'font_size']\n",
    "\n",
    "                    if next_font == current_font:\n",
    "                        # Merge text\n",
    "                        df.at[current_idx, 'text'] += ' ' + df.loc[next_idx, 'text']\n",
    "                        drop_indices.add(next_idx)\n",
    "                        merged = True\n",
    "                        j += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if merged:\n",
    "                    df.at[current_idx, 'parabool'] = True\n",
    "                else:\n",
    "                    df.at[current_idx, 'parabool'] = False\n",
    "\n",
    "                i = j  # skip merged rows\n",
    "\n",
    "        # Drop merged lines\n",
    "        df = df.drop(index=list(drop_indices)).reset_index(drop=True)\n",
    "        updated_dflist.append(df)\n",
    "\n",
    "    return updated_dflist\n",
    "\n",
    "\n",
    "def extract_table_with_titleold(page, max_title_distance=50): #################Old\n",
    "    \"\"\"\n",
    "    Extracts the first table and its potential title from a PyMuPDF page.\n",
    "\n",
    "    Parameters:\n",
    "        page (fitz.Page): The PDF page object.\n",
    "        max_title_distance (float): Max vertical distance (in pt) to search above the table for a title.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: The extracted table (if found), else None.\n",
    "            - str: The possible table title (if found), else None.\n",
    "    \"\"\"\n",
    "    tables = page.find_tables()\n",
    "    print(f\"{len(tables.tables)} table(s) found.\")\n",
    "\n",
    "    if not tables.tables:\n",
    "        return None, None\n",
    "\n",
    "    table = tables[0]\n",
    "    table_data = table.extract()\n",
    "    table_df = pd.DataFrame(table_data)\n",
    "\n",
    "    print(\"First table extracted:\")\n",
    "    pprint(table_data)\n",
    "\n",
    "    # Look for text just above the table\n",
    "    bbox = table.bbox\n",
    "    text_dict = page.get_text(\"dict\")\n",
    "    possible_titles = []\n",
    "\n",
    "    for block in text_dict.get(\"blocks\", []):\n",
    "        if \"lines\" in block and block[\"bbox\"][3] < bbox[1]:  # Above the table\n",
    "            if abs(block[\"bbox\"][3] - bbox[1]) <= max_title_distance:\n",
    "                text = \" \".join(\n",
    "                    span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"]\n",
    "                ).strip()\n",
    "                if text:\n",
    "                    possible_titles.append((block[\"bbox\"][1], text))\n",
    "\n",
    "    # Closest first\n",
    "    possible_titles.sort(key=lambda x: -x[0])\n",
    "\n",
    "    if possible_titles:\n",
    "        title = possible_titles[0][1]\n",
    "        print(\"Possible table title:\", title)\n",
    "    else:\n",
    "        title = None\n",
    "        print(\"No clear title found above the table.\")\n",
    "\n",
    "    return table_df, title\n",
    "\n",
    "\n",
    "def filter_and_clean_gibberish(df, text_column='text', min_alnum_ratio=0.1, min_length=1):\n",
    "    \"\"\"\n",
    "    Replaces gibberish parts of the text with spaces. If the entire string is gibberish,\n",
    "    the row is removed.\n",
    "    Replaces gibberish parts of the text with spaces. Keeps tokens like '2.1', 'a.', '1.' etc.\n",
    "    Removes purely symbolic tokens. Drops rows if nothing meaningful remains.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame with a text column.\n",
    "        text_column (str): Column name that contains the text.\n",
    "        min_alnum_ratio (float): Minimum ratio of alphanumeric chars to keep a token.\n",
    "        min_length (int): Minimum total length of cleaned string to keep the row.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with gibberish removed or cleaned.\n",
    "    \"\"\"\n",
    "\n",
    "    def is_meaningful(token):\n",
    "        token = token.strip()\n",
    "        if not token:\n",
    "            return False\n",
    "\n",
    "        # Keep tokens like: 2.1, a., 1., v2.0, etc.\n",
    "        if re.match(r'^[a-zA-Z0-9]+\\.$', token):     # a. or 1.\n",
    "            return True\n",
    "        if re.match(r'^[a-zA-Z]?[0-9]*\\.[0-9]+$', token):  # 2.1, 0.3, v2.0\n",
    "            return True\n",
    "\n",
    "        alnum_ratio = sum(c.isalnum() for c in token) / (len(token) + 1e-6)\n",
    "        return alnum_ratio >= min_alnum_ratio\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = str(text).strip()\n",
    "        if len(text) < min_length:\n",
    "            return None\n",
    "\n",
    "        # Split tokens but keep punctuation and symbols separated\n",
    "        tokens = re.split(r'(\\W+)', text)\n",
    "        cleaned_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if is_meaningful(token):\n",
    "                cleaned_tokens.append(token)\n",
    "            else:\n",
    "                cleaned_tokens.append(' ')\n",
    "\n",
    "        cleaned = ''.join(cleaned_tokens).strip()\n",
    "        if len(cleaned) < min_length or all(c.isspace() for c in cleaned):\n",
    "            return None\n",
    "        return cleaned\n",
    "\n",
    "    df[text_column] = df[text_column].apply(clean_text)\n",
    "    df = df[df[text_column].notna()].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_consecutive_same_font_and_leftv44(dflist, target_fonts, acceptable_spacing_range=[0.0, 2.0], min_words_required=3):\n",
    "    min_spacing, max_spacing = acceptable_spacing_range\n",
    "    updated_dflist = []\n",
    "\n",
    "    for df in dflist:\n",
    "        if df.empty or 'left' not in df.columns or 'font_size' not in df.columns:\n",
    "            df['parabool'] = False\n",
    "            updated_dflist.append(df)\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df['parabool'] = False\n",
    "        drop_indices = set()\n",
    "\n",
    "        for left_value in sorted(df['left'].unique()):\n",
    "            group_df = df[(df['left'] == left_value) & (df['font_size'].isin(target_fonts))]\n",
    "            indices = group_df.index.tolist()\n",
    "            i = 0\n",
    "\n",
    "            while i < len(indices):\n",
    "                current_idx = indices[i]\n",
    "                current_font = df.loc[current_idx, 'font_size']\n",
    "                current_bold = df.loc[current_idx, 'bold']\n",
    "                current_italic = df.loc[current_idx, 'italic']\n",
    "                current_top = df.loc[current_idx, 'top']\n",
    "                current_word_count = df.loc[current_idx, 'word_count']\n",
    "\n",
    "                j = i + 1\n",
    "                merged = False\n",
    "\n",
    "                while j < len(indices):\n",
    "                    next_idx = indices[j]\n",
    "                    next_font = df.loc[next_idx, 'font_size']\n",
    "                    next_bold = df.loc[next_idx, 'bold']\n",
    "                    next_italic = df.loc[next_idx, 'italic']\n",
    "                    next_top = df.loc[next_idx, 'top']\n",
    "\n",
    "                    if (\n",
    "                        next_font != current_font or\n",
    "                        (current_word_count > min_words_required and (\n",
    "                            next_bold != current_bold or\n",
    "                            next_italic != current_italic))\n",
    "                    ):\n",
    "                        break\n",
    "\n",
    "                    top_diff = abs(next_top - current_top)\n",
    "                    if not (min_spacing <= top_diff <= max_spacing):\n",
    "                        break\n",
    "\n",
    "                    df.at[current_idx, 'text'] += ' ' + df.loc[next_idx, 'text']\n",
    "                    df.at[current_idx, 'word_count'] += df.loc[next_idx, 'word_count']\n",
    "                    drop_indices.add(next_idx)\n",
    "                    merged = True\n",
    "                    j += 1\n",
    "                    current_top = next_top\n",
    "                    current_word_count = df.at[current_idx, 'word_count']\n",
    "\n",
    "                df.at[current_idx, 'parabool'] = merged\n",
    "                i = j\n",
    "\n",
    "        df = df.drop(index=list(drop_indices)).reset_index(drop=True)\n",
    "        updated_dflist.append(df)\n",
    "\n",
    "    return updated_dflist\n",
    "\n",
    "\n",
    "def merge_paragraphs_by_font(df, font_min=8.0, font_max=14.0, font_tolerance=1.0,line_gap=2.0): #maybe add a center align criteria\n",
    "    \"\"\"\n",
    "    Groups consecutive lines (in original order) with similar font size and small vertical gap into paragraphs.\n",
    "    No sorting is applied — assumes df is already in reading order.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with 'top', 'font_size', 'line_height', 'text' columns.\n",
    "        font_min (float): Minimum font size to allow.\n",
    "        font_max (float): Maximum font size to allow.\n",
    "        font_tolerance (float): Allowed deviation in font size between lines.\n",
    "        line_gap (float): Maximum allowed vertical gap to group lines.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: List of merged paragraph dictionaries.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return []\n",
    "    # Filter font range\n",
    "    df = df[(df['font_size'] >= font_min) & (df['font_size'] <= font_max)].reset_index(drop=True)\n",
    "    if df.empty:\n",
    "        return []\n",
    "\n",
    "    paragraphs = []\n",
    "    current_para = {\n",
    "        'top': df.loc[0, 'top'],\n",
    "        'font_size': df.loc[0, 'font_size'],\n",
    "        'text': df.loc[0, 'text'],\n",
    "        'line_count': 1\n",
    "    }\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        prev = df.loc[i - 1]\n",
    "        curr = df.loc[i]\n",
    "    \n",
    "        same_font = abs(curr['font_size'] - prev['font_size']) <= font_tolerance\n",
    "        small_gap = abs(curr['top'] - prev['top']) <= (prev['line_height'] + line_gap)\n",
    "\n",
    "        if same_font and small_gap:\n",
    "            current_para['text'] += ' ' + curr['text']\n",
    "            current_para['line_count'] += 1\n",
    "        else:\n",
    "            paragraphs.append(current_para)\n",
    "            current_para = {\n",
    "                'top': curr['top'],\n",
    "                'font_size': curr['font_size'],\n",
    "                'text': curr['text'],\n",
    "                'line_count': 1\n",
    "            }\n",
    "\n",
    "    paragraphs.append(current_para)\n",
    "    return paragraphs\n",
    "\n",
    "def filter_gibberish_rows(df, text_column='text', min_alpha_ratio=0.1, min_length=1):\n",
    "    \"\"\"\n",
    "    Removes rows from df where the text is mostly non-alphabetic (e.g., --------, ...., ====),\n",
    "    or is too short to be meaningful.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame with a text column.\n",
    "        text_column (str): Column name that contains the text.\n",
    "        min_alpha_ratio (float): Minimum ratio of alphabetic chars to keep the row.\n",
    "        min_length (int): Minimum total length of string to keep it.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with gibberish lines removed.\n",
    "    \"\"\"\n",
    "    def is_gibberish(text):\n",
    "        if len(text.strip()) < min_length:\n",
    "            return True\n",
    "        alpha_count = sum(c.isalpha() for c in text)\n",
    "        return (alpha_count / len(text)) < min_alpha_ratio\n",
    "\n",
    "    mask = df[text_column].apply(lambda t: not is_gibberish(t))\n",
    "    return df[mask].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "def extract_table_with_titleold(page, max_title_distance=50): #################Old\n",
    "    \"\"\"\n",
    "    Extracts the first table and its potential title from a PyMuPDF page.\n",
    "\n",
    "    Parameters:\n",
    "        page (fitz.Page): The PDF page object.\n",
    "        max_title_distance (float): Max vertical distance (in pt) to search above the table for a title.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: The extracted table (if found), else None.\n",
    "            - str: The possible table title (if found), else None.\n",
    "    \"\"\"\n",
    "    tables = page.find_tables()\n",
    "    print(f\"{len(tables.tables)} table(s) found.\")\n",
    "\n",
    "    if not tables.tables:\n",
    "        return None, None\n",
    "\n",
    "    table = tables[0]\n",
    "    table_data = table.extract()\n",
    "    table_df = pd.DataFrame(table_data)\n",
    "\n",
    "    print(\"First table extracted:\")\n",
    "    pprint(table_data)\n",
    "\n",
    "    # Look for text just above the table\n",
    "    bbox = table.bbox\n",
    "    text_dict = page.get_text(\"dict\")\n",
    "    possible_titles = []\n",
    "\n",
    "    for block in text_dict.get(\"blocks\", []):\n",
    "        if \"lines\" in block and block[\"bbox\"][3] < bbox[1]:  # Above the table\n",
    "            if abs(block[\"bbox\"][3] - bbox[1]) <= max_title_distance:\n",
    "                text = \" \".join(\n",
    "                    span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"]\n",
    "                ).strip()\n",
    "                if text:\n",
    "                    possible_titles.append((block[\"bbox\"][1], text))\n",
    "\n",
    "    # Closest first\n",
    "    possible_titles.sort(key=lambda x: -x[0])\n",
    "\n",
    "    if possible_titles:\n",
    "        title = possible_titles[0][1]\n",
    "        print(\"Possible table title:\", title)\n",
    "    else:\n",
    "        title = None\n",
    "        print(\"No clear title found above the table.\")\n",
    "\n",
    "    return table_df, title\n",
    "\n",
    "\n",
    "\n",
    "def merge_consecutive_same_font_and_leftv2(dflist, target_fonts):\n",
    "    \"\"\"\n",
    "    Merges lines within each DataFrame in dflist where:\n",
    "    - font_size ∈ target_fonts\n",
    "    - consecutive lines share same font_size, left alignment, and bold/italic state\n",
    "    Adds a 'parabool' column to mark whether a line is a merged paragraph.\n",
    "\n",
    "    Parameters:\n",
    "        dflist (list of pd.DataFrame): Each DataFrame is a page.\n",
    "        target_fonts (list): List of font sizes to consider for paragraph merging.\n",
    "\n",
    "    Returns:\n",
    "        list of pd.DataFrame: Modified DataFrames with merged paragraphs and 'parabool' flag.\n",
    "    \"\"\"\n",
    "    updated_dflist = []\n",
    "\n",
    "    for df in dflist:\n",
    "        if df.empty or 'left' not in df.columns or 'font_size' not in df.columns:\n",
    "            df['parabool'] = False\n",
    "            updated_dflist.append(df)\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df['parabool'] = False\n",
    "        drop_indices = set()\n",
    "\n",
    "        for left_value in sorted(df['left'].unique()):\n",
    "            group_df = df[(df['left'] == left_value) & (df['font_size'].isin(target_fonts))]\n",
    "            indices = group_df.index.tolist()\n",
    "            i = 0\n",
    "\n",
    "            while i < len(indices):\n",
    "                current_idx = indices[i]\n",
    "                current_font = df.loc[current_idx, 'font_size']\n",
    "                current_bold = df.loc[current_idx, 'bold']\n",
    "                current_italic = df.loc[current_idx, 'italic']\n",
    "\n",
    "                j = i + 1\n",
    "                merged = False\n",
    "\n",
    "                while j < len(indices):\n",
    "                    next_idx = indices[j]\n",
    "                    next_font = df.loc[next_idx, 'font_size']\n",
    "                    next_bold = df.loc[next_idx, 'bold']\n",
    "                    next_italic = df.loc[next_idx, 'italic']\n",
    "\n",
    "                    # Stop if font or bold/italic style mismatch\n",
    "                    if (\n",
    "                        next_font != current_font or\n",
    "                        next_bold != current_bold or\n",
    "                        next_italic != current_italic\n",
    "                    ):\n",
    "                        break\n",
    "\n",
    "                    # Merge text\n",
    "                    df.at[current_idx, 'text'] += ' ' + df.loc[next_idx, 'text']\n",
    "                    drop_indices.add(next_idx)\n",
    "                    merged = True\n",
    "                    j += 1\n",
    "\n",
    "                df.at[current_idx, 'parabool'] = merged\n",
    "                i = j\n",
    "\n",
    "        df = df.drop(index=list(drop_indices)).reset_index(drop=True)\n",
    "        updated_dflist.append(df)\n",
    "\n",
    "    return updated_dflist\n",
    "\n",
    "\n",
    "def merge_consecutive_same_font_and_leftv3(dflist, target_fonts, acceptable_spacing_range=[0.0, 2.0]):\n",
    "    \"\"\"\n",
    "    Merges lines within each DataFrame in dflist where:\n",
    "    - font_size ∈ target_fonts\n",
    "    - consecutive lines share same font_size, left alignment, and bold/italic state\n",
    "    - vertical spacing (top difference) is within acceptable_spacing_range\n",
    "\n",
    "    Adds:\n",
    "    - 'parabool' column: True if line is a merged paragraph.\n",
    "    - 'end' column: set to its own top if no merge, or the last merged line's top.\n",
    "\n",
    "    Parameters:\n",
    "        dflist (list of pd.DataFrame): Each DataFrame is a page.\n",
    "        target_fonts (list): Font sizes to consider for merging.\n",
    "        acceptable_spacing_range (list): [min_spacing, max_spacing] range to allow merging lines.\n",
    "\n",
    "    Returns:\n",
    "        list of pd.DataFrame: Modified DataFrames with merged paragraphs and 'parabool'/'end' flags.\n",
    "    \"\"\"\n",
    "    min_spacing, max_spacing = acceptable_spacing_range\n",
    "    updated_dflist = []\n",
    "\n",
    "    for df in dflist:\n",
    "        if df.empty or 'left' not in df.columns or 'font_size' not in df.columns:\n",
    "            df['parabool'] = False\n",
    "            df['end'] = df['top'] if 'top' in df.columns else 0\n",
    "            updated_dflist.append(df)\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df['parabool'] = False\n",
    "        df['end'] = df['top']\n",
    "        drop_indices = set()\n",
    "\n",
    "        for left_value in sorted(df['left'].unique()):\n",
    "            group_df = df[(df['left'] == left_value) & (df['font_size'].isin(target_fonts))]\n",
    "            indices = group_df.index.tolist()\n",
    "            i = 0\n",
    "\n",
    "            while i < len(indices):\n",
    "                current_idx = indices[i]\n",
    "                current_font = df.loc[current_idx, 'font_size']\n",
    "                current_bold = df.loc[current_idx, 'bold']\n",
    "                current_italic = df.loc[current_idx, 'italic']\n",
    "                current_top = df.loc[current_idx, 'top']\n",
    "                final_end = current_top\n",
    "\n",
    "                j = i + 1\n",
    "                merged = False\n",
    "\n",
    "                while j < len(indices):\n",
    "                    next_idx = indices[j]\n",
    "                    next_font = df.loc[next_idx, 'font_size']\n",
    "                    next_bold = df.loc[next_idx, 'bold']\n",
    "                    next_italic = df.loc[next_idx, 'italic']\n",
    "                    next_top = df.loc[next_idx, 'top']\n",
    "\n",
    "                    # Stop if font or style mismatch\n",
    "                    if (\n",
    "                        next_font != current_font or\n",
    "                        next_bold != current_bold or\n",
    "                        next_italic != current_italic\n",
    "                    ):\n",
    "                        break\n",
    "\n",
    "                    # Check top spacing constraint\n",
    "                    top_diff = abs(next_top - current_top)\n",
    "                    if not (min_spacing <= top_diff <= max_spacing):\n",
    "                        break\n",
    "\n",
    "                    # Merge text\n",
    "                    df.at[current_idx, 'text'] += ' ' + df.loc[next_idx, 'text']\n",
    "                    final_end = next_top\n",
    "                    drop_indices.add(next_idx)\n",
    "                    merged = True\n",
    "                    j += 1\n",
    "                    current_top = next_top  # update for cascading merge\n",
    "\n",
    "                df.at[current_idx, 'parabool'] = merged\n",
    "                df.at[current_idx, 'end'] = final_end\n",
    "                i = j\n",
    "\n",
    "        df = df.drop(index=list(drop_indices)).reset_index(drop=True)\n",
    "        updated_dflist.append(df)\n",
    "\n",
    "    return updated_dflist\n",
    "\n",
    "\n",
    "#spandan\n",
    "\n",
    "def merge_consecutive_same_font_and_leftv0(dflist, target_fonts):\n",
    "    \"\"\"\n",
    "    Merges lines in-place within each DataFrame in dflist where:\n",
    "    - font_size ∈ target_fonts\n",
    "    - consecutive lines share same font_size and left\n",
    "    - lines are NOT bold or italic\n",
    "    Adds a 'parabool' column to mark whether a line is a merged paragraph.\n",
    "\n",
    "    Parameters:\n",
    "        dflist (list of pd.DataFrame): Each DataFrame is a page.\n",
    "        target_fonts (list): List of font sizes to consider for paragraph merging.\n",
    "\n",
    "    Returns:\n",
    "        list of pd.DataFrame: Modified DataFrames with merged paragraphs and 'parabool' flag.\n",
    "    \"\"\"\n",
    "    updated_dflist = []\n",
    "\n",
    "    for df in dflist:\n",
    "        if df.empty or 'left' not in df.columns or 'font_size' not in df.columns:\n",
    "            df['parabool'] = False\n",
    "            updated_dflist.append(df)\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df['parabool'] = False\n",
    "        drop_indices = set()\n",
    "\n",
    "        for left_value in sorted(df['left'].unique()):\n",
    "            group_df = df[(df['left'] == left_value) & (df['font_size'].isin(target_fonts))]\n",
    "            indices = group_df.index.tolist()\n",
    "            i = 0\n",
    "\n",
    "            while i < len(indices):\n",
    "                current_idx = indices[i]\n",
    "                current_font = df.loc[current_idx, 'font_size']\n",
    "\n",
    "                # Skip bold/italic rows\n",
    "                if df.loc[current_idx, 'bold'] : #or df.loc[current_idx, 'italic']: #ignore the italics for now\n",
    "                    df.at[current_idx, 'parabool'] = False\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                j = i + 1\n",
    "                merged = False\n",
    "\n",
    "                while j < len(indices):\n",
    "                    next_idx = indices[j]\n",
    "                    next_font = df.loc[next_idx, 'font_size']\n",
    "\n",
    "                    # Stop if font differs or next is bold/italic\n",
    "                    if next_font != current_font or df.loc[next_idx, 'bold'] or df.loc[next_idx, 'italic']:\n",
    "                        break\n",
    "\n",
    "                    # Merge\n",
    "                    df.at[current_idx, 'text'] += ' ' + df.loc[next_idx, 'text']\n",
    "                    drop_indices.add(next_idx)\n",
    "                    merged = True\n",
    "                    j += 1\n",
    "\n",
    "                df.at[current_idx, 'parabool'] = merged\n",
    "                i = j\n",
    "\n",
    "        # Drop merged rows\n",
    "        df = df.drop(index=list(drop_indices)).reset_index(drop=True)\n",
    "        updated_dflist.append(df)\n",
    "\n",
    "    return updated_dflist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f68fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = doc[1]\n",
    "print(page.get_text('html'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
