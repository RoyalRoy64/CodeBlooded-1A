{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab7f39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'extract_heading_summary' from 'extractandclean' (c:\\Users\\Roy\\Desktop\\GIT\\Adobe\\CodeBlooded-1A\\extractandclean.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mextractandclean\u001b[39;00m\u001b[38;5;250m    \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_blocks_and_distributions , extract_heading_summary , extract_crucial_pattern_lines , filter_and_clean_gibberish\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmerge\u001b[39;00m\u001b[38;5;250m              \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge_consecutive_same_font_and_left\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mheaderfooter\u001b[39;00m\u001b[38;5;250m       \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_alternate_page_repeats_splitv2 , find_alternate_page_repeats_split , clean_pages_of_even_odd_repeats\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'extract_heading_summary' from 'extractandclean' (c:\\Users\\Roy\\Desktop\\GIT\\Adobe\\CodeBlooded-1A\\extractandclean.py)"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "# import pymupdf4llm\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import re\n",
    "from collections import Counter\n",
    "from extractandclean    import extract_blocks_and_distributions , extract_heading_summary , extract_crucial_pattern_lines , filter_and_clean_gibberish\n",
    "from merge              import merge_consecutive_same_font_and_left\n",
    "from headerfooter       import find_alternate_page_repeats_splitv2 , find_alternate_page_repeats_split , clean_pages_of_even_odd_repeats\n",
    "from tables             import extract_table_with_title\n",
    "from fontstat           import generate_font_stats , get_global_font_counter , get_rare_large_fonts , map_fonts_to_heading_levels , get_significant_large_fonts , create_font_level_map , get_top_fonts , classify_font_size , extract_heading_summary\n",
    "from analyzeandcompute  import unify_close_left_values , analyze_left_distribution , analyze_left_clusters , analyze_line_spacing , compute_word_count_stats , compute_line_spacing_per_page\n",
    "# md_text = pymupdf4llm.to_markdown(fileinquestion)\n",
    "# pathlib.Path(\"output.md\").write_bytes(md_text.encode())\n",
    "# llama_reader = pymupdf4llm.LlamaMarkdownReader()\n",
    "# print(llama_reader())\n",
    "# llama_docs = llama_reader.load_data(fileinquestion)\n",
    "\n",
    "#tesseract ocr\n",
    "# english\n",
    "# russian\n",
    "# chinese (new)\n",
    "# mandarin (traditional)\n",
    "# japanese\n",
    "# Hindi\n",
    "# Arabic\n",
    "# French\n",
    "# Hebrew\n",
    "# German\n",
    "# Korean\n",
    "# Italian\n",
    "# Polish\n",
    "# Portugese\n",
    "# Spanish \n",
    "# Indonesian3\n",
    "#turkish\n",
    "# Urdu\n",
    "#pip install pymupdf numpy pandas sentence-transformers\n",
    "#################### workflow\n",
    "#parallel processing\n",
    "#trigger - \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "###################################### something for future\n",
    "# import re\n",
    "\n",
    "# def is_gibberish(text):\n",
    "#     text = text.strip()\n",
    "#     if len(text) < min_length:\n",
    "#         return True\n",
    "#     if re.fullmatch(r'[\\W_]{2,}', text):  # full of symbols\n",
    "#         return True\n",
    "#     alpha_count = sum(c.isalpha() for c in text)\n",
    "#     return (alpha_count / len(text)) < min_alpha_ratio\n",
    "# from langdetect import detect, DetectorFactory\n",
    "# DetectorFactory.seed = 0\n",
    "\n",
    "# def is_gibberish(text): #textstat or langdetect for readability or language detection\n",
    "#     text = text.strip()\n",
    "#     if len(text) < min_length:\n",
    "#         return True\n",
    "#     try:\n",
    "#         lang = detect(text)\n",
    "#         return False  # it's a real language\n",
    "#     except:\n",
    "#         return True  # detection failed = likely \n",
    "    \n",
    "# from nltk.corpus import words\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# word_set = set(words.words())\n",
    "\n",
    "# def is_gibberish(text): # Word Tokenization and Dictionary Matching\n",
    "#     tokens = word_tokenize(text.lower())\n",
    "#     meaningful_words = [t for t in tokens if t in word_set]\n",
    "#     if len(text.strip()) < min_length:\n",
    "#         return True\n",
    "#     if len(meaningful_words) / (len(tokens) + 1e-6) < 0.3:\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "# def is_gibberish(text): #Multiple Heuristics\n",
    "#     text = text.strip()\n",
    "#     if len(text) < min_length:\n",
    "#         return True\n",
    "#     if re.fullmatch(r'[\\W_]{2,}', text):\n",
    "#         return True\n",
    "#     alpha_ratio = sum(c.isalpha() for c in text) / (len(text) + 1e-6)\n",
    "#     if alpha_ratio < min_alpha_ratio:\n",
    "#         return True\n",
    "#     try:\n",
    "#         lang = detect(text)\n",
    "#         if lang not in ['en']:  # restrict to English\n",
    "#             return True\n",
    "#     except:\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "\n",
    "def is_center_aligned(left, text, page_width, font_size, tolerance=15): ######make a freaking plot on desmos if required!?!?!?!?!\n",
    "    tolerance=page_width*(tolerance/100)\n",
    "    avg_char_width = 0.5 * font_size  # rough estimate\n",
    "    text_width = len(text) * avg_char_width\n",
    "    center_text = left + text_width / 2\n",
    "    center_page = page_width / 2\n",
    "    return abs(center_page - center_text) #<= tolerance\n",
    "# is_center_aligned(78.2, dflist[0].iloc[0]['text'], width, 14.3 )\n",
    "# dflist[0]\n",
    "# is_center_aligned(121.8, dflist[0].iloc[0]['text'], width, 10)\n",
    "\n",
    "# is_center_aligned(141.3, dflist[0].iloc[0]['text'], width, 6)\n",
    "# is_center_aligned(236.7, dflist[0].iloc[0]['text'], width, 14.3)\n",
    "\n",
    "# body_font, large_fonts = get_large_fonts(font_counter, method='rms')\n",
    "# page = doc[2]\n",
    "# table_df, table_label = extract_table_with_title(page)\n",
    "\n",
    "# if table_label:\n",
    "#     print(\"Detected Table Title:\", table_label[\"text\"])\n",
    "# extract_blocks_and_distributions,\n",
    "# find_alternate_page_repeats_split,\n",
    "# clean_pages_of_even_odd_repeats,\n",
    "# merge_paragraphs_by_font\n",
    "\n",
    "\n",
    "def combine_dflist_to_master_df(dflist):\n",
    "    \"\"\"\n",
    "    Combines a list of page-level DataFrames into a single master DataFrame.\n",
    "    Adds a 'page_number' column to indicate the source page for each row.\n",
    "    Parameters:\n",
    "        dflist (list of pd.DataFrame): A list where each DataFrame corresponds to one PDF page.\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with an added 'page_number' column.\n",
    "    \"\"\"\n",
    "    combined_rows = []\n",
    "    for page_number, df in enumerate(dflist):\n",
    "        if df is not None and not df.empty:\n",
    "            df_copy = df.copy()\n",
    "            df_copy['page_number'] = page_number\n",
    "            combined_rows.append(df_copy)\n",
    "\n",
    "    if not combined_rows:\n",
    "        return pd.DataFrame()  # Return empty DataFrame if nothing valid\n",
    "\n",
    "    master_df = pd.concat(combined_rows, ignore_index=True)\n",
    "    return master_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3b048",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m     page \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mload_page(i)\n\u001b[0;32m     20\u001b[0m     html \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mget_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m     blocks\u001b[38;5;241m=\u001b[39m \u001b[43mextract_blocks_and_distributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     dflist\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mDataFrame(blocks))\u001b[38;5;66;03m############### to dict if we do async\u001b[39;00m\n\u001b[0;32m     23\u001b[0m dfl \u001b[38;5;241m=\u001b[39m dflist\n",
      "File \u001b[1;32mc:\\Users\\Roy\\Desktop\\GIT\\Adobe\\CodeBlooded-1A\\extractandclean.py:3\u001b[0m, in \u001b[0;36mextract_blocks_and_distributions\u001b[1;34m(html_string)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_blocks_and_distributions\u001b[39m(html_string): \u001b[38;5;66;03m# Match each full <p> block\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     block_pattern \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39mfindall(\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<p style=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop:([\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md.]+)pt;left:([\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md.]+)pt;line-height:([\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md.]+)pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>(.*?)</p>\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m         html_string,\n\u001b[0;32m      6\u001b[0m         flags\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39mDOTALL\n\u001b[0;32m      7\u001b[0m     )\n\u001b[0;32m      8\u001b[0m     results , font_sizes, line_heights \u001b[38;5;241m=\u001b[39m [],[],[]\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m top, left, line_height, inner_html \u001b[38;5;129;01min\u001b[39;00m block_pattern:\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "fileinquestion = \"C1A/input/E0CCG5S239.pdf\" #single page doc E0CCG5S239\n",
    "fileinquestion = \"C1A/input/TOPJUMP-PARTY-INVITATION-20161003-V01.pdf\"\n",
    "fileinquestion = \"C1A/input/STEMPathwaysFlyer.pdf\"\n",
    "fileinquestion = \"CustomPDFs/killer.pdf\"\n",
    "fileinquestion = \"CustomPDFs/jess401.pdf\"\n",
    "fileinquestion = \"C1A/input/E0CCG5S312.pdf\"   #easy\n",
    "fileinquestion = \"C1A/input/E0H1CM114.pdf\" ######### hard\n",
    "doc = pymupdf.open(fileinquestion)  \n",
    "page = doc[0]  # first page\n",
    "width , height = page.rect.width , page.rect.height\n",
    "doctoc = doc.get_toc()\n",
    "docmetadata = doc.metadata\n",
    "totalpage= doc.page_count\n",
    "dflist   = []\n",
    "mid = totalpage//2\n",
    "\n",
    "for i in range(totalpage):\n",
    "    page = doc.load_page(i)\n",
    "    html = page.get_text(\"html\")\n",
    "    blocks= extract_blocks_and_distributions(html)\n",
    "    dflist.append(pd.DataFrame(blocks))############### to dict if we do async\n",
    "dfl = dflist\n",
    "even_df, odd_df =   find_alternate_page_repeats_splitv2(dflist, mid=totalpage // 2)\n",
    "dflist          =   clean_pages_of_even_odd_repeats(dflist, even_df, odd_df)\n",
    "dflist          =   [filter_and_clean_gibberish(df, min_alnum_ratio=0.15, min_length=1) for df in dflist] \n",
    "averagewords = compute_word_count_stats(dflist)['global_avg_words_per_line']\n",
    "font_stats_df = generate_font_stats(dflist)\n",
    "\n",
    "dflist = unify_close_left_values(dflist, tolerance=width*0.01, min_left=0, max_left=width)\n",
    "\n",
    "################add it back\n",
    "#['page_number', 'font_size', 'font_family', 'count', 'total_words','avg_word_density']\n",
    "# fontfamily_counter = Counter(font_stats_df['font_family'])\n",
    "# fontsize_counter = Counter(font_stats_df['font_size'])\n",
    "\n",
    "font_word_counts = (font_stats_df.groupby('font_size')['total_words'].sum().sort_values(ascending=False) )\n",
    "# master_df = combine_dflist_to_master_df(dflist)\n",
    "target_fonts = get_significant_large_fonts(font_word_counts, z_thresh=0.5)['large_fonts']\n",
    "# print(\"Large fonts:\", result['large_fonts']) #print(\"Z-scores:\\n\", result['z_scores']) #print(\"Body font:\", result['body_font'])\n",
    "\n",
    "spacing_list = compute_line_spacing_per_page(dflist)\n",
    "all_spacings = np.concatenate(spacing_list)# If you want to concatenate all spacing into a single array:\n",
    "spacing_stats = analyze_line_spacing(spacing_list)\n",
    "meansp =  spacing_stats['mean_spacing']\n",
    "\n",
    "dflist = merge_consecutive_same_font_and_left(dflist,list(font_word_counts.keys()),[meansp*0.4,meansp*1.75])\n",
    "leftdf = analyze_left_distribution(dflist)['left_stats_df'] # 'left_counter': left_counter,# 'dominant_lefts': dominant_lefts,# 'left_stats_df': left_stats_df\n",
    "leftdf.loc[(leftdf.left<=204) & (leftdf.percentage>=1)]\n",
    "\n",
    "master_df = combine_dflist_to_master_df(dflist)\n",
    "\n",
    "column_order = ['page_number',  'left',\"top\", 'end' ,\"text\",'line_height','font_size','font_family','bold','italic','word_count','word_density','parabool']\n",
    "master_df[column_order]\n",
    "master_df['left'] = master_df['left'].round(2)\n",
    "\n",
    "sortmm = (master_df.loc[master_df.left<width*0.35])\n",
    "unique_lefts = sorted(set(val for val in sortmm['left'].unique()))\n",
    "unique_lefts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Usage\n",
    "df_summary = analyze_left_clusters([master_df], unique_lefts) #uniql are ascending\n",
    "df_summary\n",
    "# pprint(master_df.loc[(master_df.left==unique_lefts[0])& (master_df.word_count<=10)][['top','line_height','font_size','bold','italic','text','word_count','parabool']])\n",
    "print(font_word_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac1bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157 ms ± 4.19 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "fileinquestion = \"C1A/input/E0CCG5S239.pdf\" #single page doc E0CCG5S239\n",
    "fileinquestion = \"C1A/input/TOPJUMP-PARTY-INVITATION-20161003-V01.pdf\"\n",
    "fileinquestion = \"C1A/input/STEMPathwaysFlyer.pdf\"\n",
    "fileinquestion = \"C1A/input/E0H1CM114.pdf\"\n",
    "fileinquestion = \"AAMine/killer.pdf\"\n",
    "fileinquestion = \"C1A/input/E0H1CM114.pdf\"\n",
    "fileinquestion = \"AAMine/killer.pdf\"\n",
    "fileinquestion = \"C1A/input/E0CCG5S312.pdf\"\n",
    "fileinquestion = \"AAMine/jess401.pdf\"\n",
    "fileinquestion = \"C1A/input/E0CCG5S312.pdf\" #single page doc E0CCG5S239 #easy\n",
    "\n",
    "\n",
    "#E0CCG5S312 eazy 12 pages\n",
    "#hard E0H1CM114\n",
    "doc = pymupdf.open(fileinquestion)  \n",
    "page = doc[0]  # first page\n",
    "\n",
    "width = page.rect.width\n",
    "height = page.rect.height\n",
    "doctoc = doc.get_toc()\n",
    "docmetadata = doc.metadata\n",
    "totalpage= doc.page_count\n",
    "\n",
    "dflist , repeated_all, all_font_sizes , font_counter = [], [] ,set() , Counter()\n",
    "mid = totalpage//2\n",
    "\n",
    "#if dig.dig is followed then try to use the flow...\n",
    "for i in range(totalpage):\n",
    "    page = doc.load_page(i)\n",
    "    html = page.get_text(\"html\")\n",
    "    blocks, font_counter, line_counter = extract_blocks_and_distributions(html)\n",
    "    dflist.append(pd.DataFrame(blocks))\n",
    "dfl = dflist\n",
    "\n",
    "dfcrucial = extract_crucial_pattern_lines(dflist)\n",
    "dfcrucial############################################################################################################################\n",
    "dfcrucial.loc[dfcrucial.bold==True]\n",
    "\n",
    "get_top_fonts(font_counter)\n",
    "\n",
    "even_df, odd_df =       find_alternate_page_repeats_split(dflist, mid=totalpage // 2)\n",
    "dflist =                clean_pages_of_even_odd_repeats(dflist, even_df, odd_df)\n",
    "dflist =        [filter_gibberish_rows(df) for df in dflist]\n",
    "\n",
    "#mode range \n",
    "font_stats_df = generate_font_stats(dflist)\n",
    "font_stats_df.sort_values(by=['page_number', 'font_size'], ascending=[True, False]).reset_index(drop=True)\n",
    "font_counter = get_global_font_counter(font_stats_df)\n",
    "topfonts = get_top_fonts(font_counter)\n",
    "# font_level_map =  create_font_level_map(font_counter) #classify_font_size(16, font_level_map)\n",
    "\n",
    "target_fonts = list(font_counter.keys())\n",
    "\n",
    "#merging it now\n",
    "dflist = merge_consecutive_same_font_and_left(dflist, target_fonts)\n",
    "dflist = recalculate_word_stats(dflist)\n",
    "\n",
    "\n",
    "font_counter\n",
    "body_font, large_fonts = get_rare_large_fonts(font_counter, method='rms', freq_filter='average')\n",
    "\n",
    "# even_df, odd_df = find_alternate_page_repeats_split(dflist, mid=totalpage // 2)\n",
    "# dflist = clean_pages_of_even_odd_repeats(dflist, even_df, odd_df)\n",
    "# font_level_map =  create_font_level_map(font_counter) #classify_font_size(16, font_level_map)\n",
    "font_heading_map = map_fonts_to_heading_levels(large_fonts)\n",
    "\n",
    "dflist = label_font_levels(dflist, font_heading_map) #returns the dflist\n",
    "result = analyze_word_density_patterns(\n",
    "    dflist,\n",
    "    method='density_by_font',\n",
    "    stat='median',\n",
    "    threshold_factor=1.8\n",
    ")\n",
    "\n",
    "# print(f\"Metric: {result['metric_name']}\")\n",
    "# print(f\"Central Value: {result['central_value']}\")\n",
    "# print(f\"Dense Lines: {len(result['dense_lines'])}, Sparse Lines: {len(result['sparse_lines'])}\")\n",
    "\n",
    "# for i in range(totalpage):\n",
    "#     extract_table_with_title(doc[i])\n",
    "dfcrucial\n",
    "extract_heading_summary(dflist, levels=(['H1','H2','H3']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71623e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileinquestion = \"C1A/input/E0CCG5S312.pdf\" #single page doc E0CCG5S239\n",
    "#E0CCG5S312 eazy 12 pages\n",
    "#hard E0H1CM114\n",
    "doc = pymupdf.open(fileinquestion)  \n",
    "page = doc[3]  # first page\n",
    "\n",
    "            # “text”: (default) plain text with line breaks. No formatting, no text position details, no images.\n",
    "            # “blocks”: generate a list of text blocks (= paragraphs).\n",
    "            # “words”: generate a list of words (strings not containing spaces).\n",
    "            # “html”: creates a full visual version of the page including any images. This can be displayed with your internet browser.\n",
    "            # “dict” / “json”: same information level as HTML, but provided as a Python dictionary or resp. JSON string. See TextPage.extractDICT() for details of its structure.\n",
    "            # “rawdict” / “rawjson”: a super-set of “dict” / “json”. It additionally provides character detail information like XML. See TextPage.extractRAWDICT() for details of its structure.\n",
    "            # “xhtml”: text information level as the TEXT version but includes images. Can also be displayed by internet browsers.\n",
    "            # “xml”: contains no images, but full position and font information down to each single text character. Use an XML module to interpret.\n",
    "ab = page.get_text('xhtml')\n",
    "pprint(ab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78925123",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Time async\u001b[39;00m\n\u001b[0;32m     67\u001b[0m start_async \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 68\u001b[0m dflist_async, font_counter_async \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileinquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m end_async \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# ---- RESULT COMPARISON ----\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "#### async\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import time\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import Counter\n",
    "fileinquestion = \"C1A/input/E0H1CM114.pdf\" ######### hard\n",
    "\n",
    "# Dummy version of your function (replace with your real one)\n",
    "def extract_blocks_and_distributions(html):\n",
    "    from random import randint\n",
    "    # Dummy block list\n",
    "    blocks = [{\"text\": f\"Dummy {i}\", \"font\": \"FontX\", \"size\": randint(10, 14)} for i in range(randint(5, 15))]\n",
    "    font_counter = Counter({f\"FontX-{randint(10,14)}\": randint(1, 5)})\n",
    "    line_counter = Counter()\n",
    "    return blocks, font_counter, line_counter\n",
    "\n",
    "# ---- SEQUENTIAL VERSION ----\n",
    "def process_sequential(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    dflist = []\n",
    "    font_counter_total = Counter()\n",
    "    for i in range(doc.page_count):\n",
    "        page = doc.load_page(i)\n",
    "        html = page.get_text(\"html\")\n",
    "        blocks, font_counter, line_counter = extract_blocks_and_distributions(html)\n",
    "        dflist.append(pd.DataFrame(blocks))\n",
    "        font_counter_total.update(font_counter)\n",
    "    return dflist, font_counter_total\n",
    "\n",
    "# ---- ASYNC MULTITHREAD VERSION ----\n",
    "def process_page_threaded(doc_path, page_num):\n",
    "    with fitz.open(doc_path) as doc:\n",
    "        page = doc.load_page(page_num)\n",
    "        html = page.get_text(\"html\")\n",
    "        blocks, font_counter, line_counter = extract_blocks_and_distributions(html)\n",
    "        return pd.DataFrame(blocks), font_counter\n",
    "\n",
    "async def process_async(file_path, max_workers=4):\n",
    "    loop = asyncio.get_running_loop()\n",
    "    dflist = []\n",
    "    font_counter_total = Counter()\n",
    "    total_pages = fitz.open(file_path).page_count\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        tasks = [\n",
    "            loop.run_in_executor(executor, process_page_threaded, file_path, i)\n",
    "            for i in range(total_pages)\n",
    "        ]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for df, counter in results:\n",
    "        dflist.append(df)\n",
    "        font_counter_total.update(counter)\n",
    "\n",
    "    return dflist, font_counter_total\n",
    "\n",
    "# ---- TIMING TEST ----\n",
    "fileinquestion = fileinquestion  # Change this to your file path\n",
    "\n",
    "# Time sequential\n",
    "start_seq = time.time()\n",
    "dflist_seq, font_counter_seq = process_sequential(fileinquestion)\n",
    "end_seq = time.time()\n",
    "\n",
    "# Time async\n",
    "start_async = time.time()\n",
    "dflist_async, font_counter_async = asyncio.run(process_async(fileinquestion, max_workers=8))\n",
    "end_async = time.time()\n",
    "\n",
    "# ---- RESULT COMPARISON ----\n",
    "print(f\"⏱ Sequential time:      {end_seq - start_seq:.4f} seconds\")\n",
    "print(f\"⚡ Async (Threaded) time: {end_async - start_async:.4f} seconds\")\n",
    "\n",
    "# Optional: verify correctness\n",
    "print(f\"\\nPages (Sequential): {len(dflist_seq)}, Pages (Async): {len(dflist_async)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aef184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "page_num",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "level",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "86d402d1-23ba-4ad9-97e1-7b823143d02f",
       "rows": [
        [
         "0",
         "2",
         "H1",
         "Revision History"
        ],
        [
         "1",
         "3",
         "H1",
         "Table of Contents"
        ],
        [
         "2",
         "4",
         "H1",
         "Acknowledgements"
        ],
        [
         "3",
         "5",
         "H1",
         "1. Introduction to the Foundation Level Extensions"
        ],
        [
         "4",
         "6",
         "H1",
         "2. Introduction to Foundation Level Agile Tester Extension"
        ],
        [
         "5",
         "6",
         "H2",
         "2.1 Intended Audience"
        ],
        [
         "6",
         "6",
         "H2",
         "2.2 Career Paths for Testers"
        ],
        [
         "7",
         "6",
         "H2",
         "2.3 Learning Objectives"
        ],
        [
         "8",
         "7",
         "H2",
         "2.4 Entry Requirements"
        ],
        [
         "9",
         "7",
         "H2",
         "2.5 Structure and Course Duration"
        ],
        [
         "10",
         "8",
         "H2",
         "2.6 Keeping It Current"
        ],
        [
         "11",
         "9",
         "H1",
         "3. Overview of the Foundation Level Extension &#x2013; Agile Tester"
        ],
        [
         "12",
         "9",
         "H1",
         "Syllabus"
        ],
        [
         "13",
         "9",
         "H2",
         "3.1 Business Outcomes"
        ],
        [
         "14",
         "9",
         "H2",
         "3.2 Content"
        ],
        [
         "15",
         "11",
         "H1",
         "4. References"
        ],
        [
         "16",
         "11",
         "H2",
         "4.1 Trademarks"
        ],
        [
         "17",
         "11",
         "H2",
         "4.2 Documents and Web Sites"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 18
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>level</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>H1</td>\n",
       "      <td>Revision History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>H1</td>\n",
       "      <td>Table of Contents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>H1</td>\n",
       "      <td>Acknowledgements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>H1</td>\n",
       "      <td>1. Introduction to the Foundation Level Extens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>H1</td>\n",
       "      <td>2. Introduction to Foundation Level Agile Test...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>H2</td>\n",
       "      <td>2.1 Intended Audience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>H2</td>\n",
       "      <td>2.2 Career Paths for Testers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>H2</td>\n",
       "      <td>2.3 Learning Objectives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>H2</td>\n",
       "      <td>2.4 Entry Requirements</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>H2</td>\n",
       "      <td>2.5 Structure and Course Duration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>H2</td>\n",
       "      <td>2.6 Keeping It Current</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9</td>\n",
       "      <td>H1</td>\n",
       "      <td>3. Overview of the Foundation Level Extension ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>H1</td>\n",
       "      <td>Syllabus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9</td>\n",
       "      <td>H2</td>\n",
       "      <td>3.1 Business Outcomes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9</td>\n",
       "      <td>H2</td>\n",
       "      <td>3.2 Content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11</td>\n",
       "      <td>H1</td>\n",
       "      <td>4. References</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11</td>\n",
       "      <td>H2</td>\n",
       "      <td>4.1 Trademarks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11</td>\n",
       "      <td>H2</td>\n",
       "      <td>4.2 Documents and Web Sites</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page_num level                                               text\n",
       "0          2    H1                                   Revision History\n",
       "1          3    H1                                  Table of Contents\n",
       "2          4    H1                                   Acknowledgements\n",
       "3          5    H1  1. Introduction to the Foundation Level Extens...\n",
       "4          6    H1  2. Introduction to Foundation Level Agile Test...\n",
       "5          6    H2                              2.1 Intended Audience\n",
       "6          6    H2                       2.2 Career Paths for Testers\n",
       "7          6    H2                            2.3 Learning Objectives\n",
       "8          7    H2                             2.4 Entry Requirements\n",
       "9          7    H2                  2.5 Structure and Course Duration\n",
       "10         8    H2                             2.6 Keeping It Current\n",
       "11         9    H1  3. Overview of the Foundation Level Extension ...\n",
       "12         9    H1                                           Syllabus\n",
       "13         9    H2                              3.1 Business Outcomes\n",
       "14         9    H2                                        3.2 Content\n",
       "15        11    H1                                      4. References\n",
       "16        11    H2                                     4.1 Trademarks\n",
       "17        11    H2                        4.2 Documents and Web Sites"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dflist = []\n",
    "for i in range(totalpage):\n",
    "    page = doc.load_page(i)\n",
    "    html = page.get_text(\"html\")\n",
    "    blocks, font_counter, line_counter = extract_blocks_and_distributions(html)\n",
    "    dflist.append(pd.DataFrame(blocks))\n",
    "\n",
    "even_df, odd_df = find_alternate_page_repeats_split(dflist, mid=totalpage // 2)\n",
    "dflist = clean_pages_of_even_odd_repeats(dflist, even_df, odd_df)\n",
    "\n",
    "def find_repeating_rows(dflist, center_page, offsets=[1, 2], tolerance=1.0):#########depriciated\n",
    "    \"\"\"\n",
    "    Finds text lines (rows) in the center page that repeat in neighboring pages at given offsets.\n",
    "    Returns a DataFrame of repeated rows likely to be headers/footers.\n",
    "    \"\"\"\n",
    "    repeated_all = []\n",
    "    df_mid = dflist[center_page]\n",
    "\n",
    "    for offset in offsets:\n",
    "        before, after = center_page - offset, center_page + offset\n",
    "        if before < 0 or after >= len(dflist):\n",
    "            continue\n",
    "\n",
    "        df_before, df_after = dflist[before], dflist[after]\n",
    "\n",
    "        for _, row in df_mid.iterrows():\n",
    "            text, top = row['text'].strip(), row['top']\n",
    "\n",
    "            def is_match(df):\n",
    "                return any(\n",
    "                    (abs(top - r['top']) <= tolerance) and (r['text'].strip() == text)\n",
    "                    for _, r in df.iterrows()\n",
    "                )\n",
    "\n",
    "            if is_match(df_before) and is_match(df_after):\n",
    "                repeated_all.append(row)\n",
    "\n",
    "    # Drop duplicates and return as DataFrame\n",
    "    return pd.DataFrame(repeated_all).drop_duplicates(subset=[\"text\", \"top\", \"font_size\"])\n",
    "\n",
    "repeated_rows_df = find_repeating_rows(dflist, center_page=mid)\n",
    "# dflist_pages_of_repeats(dflist, repeated_rows_df)\n",
    "# repeated_rows_df\n",
    "# dflist\n",
    "# # for i,clean in enumerate(dflist):\n",
    "#     # clean.to_csv(f\"analysis/hope12/f{i}.csv\",index=False,encoding=\"utf-8\")\n",
    "\n",
    "dflister_gibberish_rows(df) for df in dflist\n",
    "get_top_fonts(font_counter)\n",
    "font_level_map =  create_font_level_map(font_counter) #classify_font_size(16, font_level_map)\n",
    "dflist_font_levels(dflist)\n",
    "extract_heading_summary(dflist, levels=(['H1','H2']))\n",
    "font_counter\n",
    "i=0\n",
    "# repeated_rows_df\n",
    "dflist[1]\n",
    "font_level_map\n",
    "extract_heading_summary(dflist, levels=(['H1','H2']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0d788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "page_num",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "level",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f42853df-8c54-4739-b911-3435f552eb2a",
       "rows": [
        [
         "0",
         "0",
         "H2",
         "The Ontario Digital Library will make Ontario a better place to study, work and live by ensuring that"
        ],
        [
         "1",
         "0",
         "H2",
         "all Ontario citizens have access to the knowledge and learning supports needed to be life-long"
        ],
        [
         "2",
         "0",
         "H2",
         "learners and effective contributors towards Ontario&#x2019;s prosperity."
        ],
        [
         "3",
         "1",
         "H1",
         "Summary"
        ],
        [
         "4",
         "1",
         "H2",
         "St., Suite 303, Toronto, ON  M5C 1M3. Proposals must be received by Noon on Monday,"
        ],
        [
         "5",
         "2",
         "H1",
         "Background"
        ],
        [
         "6",
         "5",
         "H1",
         "The Business Plan to be Developed"
        ],
        [
         "7",
         "6",
         "H1",
         "Milestones"
        ],
        [
         "8",
         "6",
         "H1",
         "Approach and Specific Proposal Requirements"
        ],
        [
         "9",
         "7",
         "H1",
         "Evaluation and Awarding of Contract"
        ],
        [
         "10",
         "8",
         "H1",
         "Appendix A:  ODL Envisioned Phases &amp; Funding"
        ],
        [
         "11",
         "8",
         "H2",
         "Phase I:  Business Planning"
        ],
        [
         "12",
         "8",
         "H2",
         "Timeline: March 2003 &#x2013; September 2003"
        ],
        [
         "13",
         "8",
         "H2",
         "Funding Requested: ~$100,000 jointly funded by stakeholder groups and the provincial government."
        ],
        [
         "14",
         "8",
         "H2",
         "Result: The ODL business plan"
        ],
        [
         "15",
         "8",
         "H2",
         "The first phase will be to build the ODL business plan. This plan will clearly define the ODL&#x2019;s services,"
        ],
        [
         "16",
         "8",
         "H2",
         "funding and governance structures, as well as implementation plans for 2003-2005. It will also secure"
        ],
        [
         "17",
         "8",
         "H2",
         "the full commitment of all stakeholders and scope the operational plan for ODL for 2006 and beyond."
        ],
        [
         "18",
         "8",
         "H2",
         "Given the number and diversity of stakeholders involved, the business planning process must be a fully"
        ],
        [
         "19",
         "8",
         "H2",
         "consultative approach. To ensure that the planning results in a workable plan with the full commitment"
        ],
        [
         "20",
         "8",
         "H2",
         "of all stakeholders it must have competent, dedicated staffing and monies for the travel and"
        ],
        [
         "21",
         "8",
         "H2",
         "communication components so critical in a consultative process."
        ],
        [
         "22",
         "8",
         "H2",
         "Phase II: Implementing and Transitioning"
        ],
        [
         "23",
         "8",
         "H2",
         "Timeline: April 2004 &#x2013; December 2006"
        ],
        [
         "24",
         "8",
         "H2",
         "Funding Requested: Funding from other states and provinces suggest that the ODL could receive"
        ],
        [
         "25",
         "8",
         "H2",
         "funding of up to $50 Million (over 3 years). Funding to be provided partnership of government, library"
        ],
        [
         "26",
         "8",
         "H2",
         "stakeholders and other interested parties."
        ],
        [
         "27",
         "8",
         "H2",
         "Result: The ODL is implemented and validated"
        ],
        [
         "28",
         "8",
         "H2",
         "The second phase will be to implement the ODL based on the business plan. This phase recognizes"
        ],
        [
         "29",
         "8",
         "H2",
         "that for ODL to be optimally successful libraries must transition to a new way of doing business and"
        ],
        [
         "30",
         "8",
         "H2",
         "providing services. The transition must occur while libraries continue to provide existing services and"
        ],
        [
         "31",
         "8",
         "H2",
         "respond to current challenges. This implementation phase will funded by a partnership of government,"
        ],
        [
         "32",
         "8",
         "H2",
         "library stakeholders and other interested parties as a means to quickly jumpstart the ODL."
        ],
        [
         "33",
         "8",
         "H2",
         "The seed money requested will allow libraries to realign their budgets and services as the infrastructure"
        ],
        [
         "34",
         "8",
         "H2",
         "and content of the ODL is created and secured. Some of the funding will be new money, although there"
        ],
        [
         "35",
         "8",
         "H2",
         "is every indication that existing budgets and methods of operating may be modified as a result of"
        ],
        [
         "36",
         "8",
         "H2",
         "recommendations. During this phase the polices, procedures, governance structures and accountability"
        ],
        [
         "37",
         "8",
         "H2",
         "mechanisms of the ODL will be put in place. Pilot projects will be initiated, evaluated and expanded."
        ],
        [
         "38",
         "8",
         "H2",
         "Information resources will be identified, contracts for these resources will be negotiated and resources"
        ],
        [
         "39",
         "8",
         "H2",
         "will be made deployed through the ODL. Regular evaluation during this phase will ensure the ODL is"
        ],
        [
         "40",
         "8",
         "H2",
         "achieving its objectives and is accountable to its key communities."
        ],
        [
         "41",
         "8",
         "H2",
         "Phase III: Operating and Growing the ODL"
        ],
        [
         "42",
         "8",
         "H2",
         "Timeline: January 2007 -"
        ],
        [
         "43",
         "8",
         "H2",
         "Funding: $50 Million annually ($35 Million requested from government)"
        ],
        [
         "44",
         "8",
         "H2",
         "Result: The ODL is fully operational and sustainable"
        ],
        [
         "45",
         "8",
         "H2",
         "In the third phase the ODL moves into the operational stage. Ontarians will experience the full benefits"
        ],
        [
         "46",
         "8",
         "H2",
         "of the initiative and libraries will consolidate support around the ODL to grow resources and extend"
        ],
        [
         "47",
         "9",
         "H2",
         "capabilities. The challenge is to secure resources sufficient to both sustain the original investments and"
        ],
        [
         "48",
         "9",
         "H2",
         "to enhance the ODL."
        ],
        [
         "49",
         "9",
         "H2",
         "The ongoing funding of the ODL will be based on a partnership model involving all the key participants"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 222
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_num</th>\n",
       "      <th>level</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>H2</td>\n",
       "      <td>The Ontario Digital Library will make Ontario ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>H2</td>\n",
       "      <td>all Ontario citizens have access to the knowle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>H2</td>\n",
       "      <td>learners and effective contributors towards On...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>H1</td>\n",
       "      <td>Summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>H2</td>\n",
       "      <td>St., Suite 303, Toronto, ON  M5C 1M3. Proposal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>13</td>\n",
       "      <td>H2</td>\n",
       "      <td>-study guides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>13</td>\n",
       "      <td>H2</td>\n",
       "      <td>-study skill development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>13</td>\n",
       "      <td>H2</td>\n",
       "      <td>-web-based curricula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>13</td>\n",
       "      <td>H2</td>\n",
       "      <td>-internet search guides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>13</td>\n",
       "      <td>H2</td>\n",
       "      <td>4. Journals, books, maps, music etc.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     page_num level                                               text\n",
       "0           0    H2  The Ontario Digital Library will make Ontario ...\n",
       "1           0    H2  all Ontario citizens have access to the knowle...\n",
       "2           0    H2  learners and effective contributors towards On...\n",
       "3           1    H1                                            Summary\n",
       "4           1    H2  St., Suite 303, Toronto, ON  M5C 1M3. Proposal...\n",
       "..        ...   ...                                                ...\n",
       "217        13    H2                                      -study guides\n",
       "218        13    H2                           -study skill development\n",
       "219        13    H2                               -web-based curricula\n",
       "220        13    H2                            -internet search guides\n",
       "221        13    H2               4. Journals, books, maps, music etc.\n",
       "\n",
       "[222 rows x 3 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileinquestion = \"C1A/input/E0H1CM114.pdf\"\n",
    "#E0CCG5S312 eazy 12 pages\n",
    "doc = pymupdf.open(fileinquestion)  \n",
    "doctoc = doc.get_toc()\n",
    "docmetadata = doc.metadata\n",
    "totalpage= doc.page_count\n",
    "\n",
    "dflist , repeated_all, all_font_sizes , font_counter = [], [] ,set() , Counter()\n",
    "mid = totalpage//2\n",
    "\n",
    "#if dig.dig is followed then try to use the flow...\n",
    "\n",
    "for i in range(totalpage):\n",
    "    page = doc.load_page(i)\n",
    "    html = page.get_text(\"html\")\n",
    "    blocks, font_counter, line_counter = extract_blocks_and_distributions(html)\n",
    "    dflist.append(pd.DataFrame(blocks))\n",
    "\n",
    "even_df, odd_df = find_alternate_page_repeats_split(dflist, mid=totalpage // 2)\n",
    "dflist_pages_of_even_odd_repeats(dflist, even_df, odd_df)\n",
    "\n",
    "paragraphs = []\n",
    "for i, df in enumerate(dflist\n",
    "    merged = merge_paragraphs_by_font(df, font_min=8.0, font_max=14.0)  # You can define ranges\n",
    "    for para in merged:\n",
    "        para['page'] = i\n",
    "    paragraphs.extend(para)\n",
    "\n",
    "paragraph_df = pd.DataFrame(paragraphs)\n",
    "merged = merge_paragraphs_by_font(df, font_min=8.0, font_max=14.0, font_tolerance=1.0, line_gap=3.0)\n",
    "\n",
    "# repeated_rows_df = find_repeating_rows(dflist, center_page=mid)\n",
    "# dflist_pages_of_repeats(dflist, repeated_rows_df)\n",
    "# repeated_rows_df\n",
    "# dflist\n",
    "# # for i,clean in enumerate(dflist\n",
    "#     # clean.to_csv(f\"analysis/hope12/f{i}.csv\",index=False,encoding=\"utf-8\")\n",
    "\n",
    "dflister_gibberish_rows(df) for df in dflist\n",
    "get_top_fonts(font_counter)\n",
    "font_level_map =  create_font_level_map(font_counter) #classify_font_size(16, font_level_map)\n",
    "dflist = label_font_levels(dflist, font_level_map)\n",
    "extract_heading_summary(dflist=(['H1','H2']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, page_df in enumerate(dflist\n",
    "    paras = merge_paragraphs_by_font(page_df)\n",
    "    for p in paras:\n",
    "        p['page'] = i  # Add page number\n",
    "    merged_paras.extend(paras)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "paragraph_df = pd.DataFrame(merged_paras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3293315b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 table(s) found.\n",
      "0 table(s) found.\n",
      "1 table(s) found.\n",
      "First table extracted:\n",
      "[['Version', 'Date', 'Remarks'],\n",
      " ['0.1', '18 JUNE 2013', 'Initial version'],\n",
      " ['0.2', '23 JULY 2013', 'WG reviewed and confirmed'],\n",
      " ['0.3', '6 NOV 2013', 'amended population and diagram'],\n",
      " ['0.7', '11 DEC 2013', 'Amended Business Outcomes and Chapters matching'],\n",
      " ['0.8', '20 DEC 2013', 'Working group updates on 0.7'],\n",
      " ['1.0', '31 MAY 2014', 'GA release for Agile Extension']]\n",
      "Possible table title: Revision History\n",
      "\n",
      "Title: Revision History\n",
      "\n",
      "         0             1                                                2\n",
      "0  Version          Date                                          Remarks\n",
      "1      0.1  18 JUNE 2013                                  Initial version\n",
      "2      0.2  23 JULY 2013                        WG reviewed and confirmed\n",
      "3      0.3    6 NOV 2013                   amended population and diagram\n",
      "4      0.7   11 DEC 2013  Amended Business Outcomes and Chapters matching\n",
      "0 table(s) found.\n",
      "0 table(s) found.\n",
      "0 table(s) found.\n",
      "0 table(s) found.\n",
      "1 table(s) found.\n",
      "First table extracted:\n",
      "[['Syllabus', 'Days'],\n",
      " ['Baseline: Foundation', '3'],\n",
      " ['Extension: Agile Tester', '2']]\n",
      "Possible table title: The syllabi must be taught in the following minimum number of days:\n",
      "\n",
      "Title: The syllabi must be taught in the following minimum number of days:\n",
      "\n",
      "                         0     1\n",
      "0                 Syllabus  Days\n",
      "1     Baseline: Foundation     3\n",
      "2  Extension: Agile Tester     2\n",
      "0 table(s) found.\n",
      "0 table(s) found.\n",
      "0 table(s) found.\n",
      "0 table(s) found.\n"
     ]
    }
   ],
   "source": [
    "#############EXP\n",
    "tablezdf\n",
    "for i in range(totalpage):\n",
    "    page = doc[i]\n",
    "    table_df, title = extract_table_with_title(page)\n",
    "\n",
    "    if table_df is not None:\n",
    "        print(f\"\\nTitle: {title}\\n\")\n",
    "        print(table_df.head())\n",
    "        tabs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b2f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRAVEYARD\n",
    "\n",
    "# page = doc[1] #not index, its page\n",
    "# for page in reversed(doc):\n",
    "# for page in doc.pages(start, stop, step):\n",
    "locationn= \"analysis/hope1\"\n",
    "alldfs,thetexts , linkstotal , linkstotalnext , annotstotal , html = [] , [] , [] , [] , [] , []\n",
    "def dumpshit(doc):\n",
    "    for i,page in enumerate(doc): # iterate the document pages\n",
    "        text = page.get_text() # get plain text encoded as UTF-8\n",
    "        thetexts.append(text)\n",
    "        links = page.get_links()\n",
    "        linkstotal.append(links)\n",
    "        link = page.first_link  # a `Link` object or `None`\n",
    "        linktemp =[]\n",
    "        while link: \n",
    "            link = link.next # get next link, last one has `None` in its `next`\n",
    "            linktemp.append(link)\n",
    "        linkstotalnext.append(linktemp)\n",
    "        # for annot in page.annots():\n",
    "            # print(f'Annotation on page: {page.number} with type: {annot.type} and rect: {annot.rect}')\n",
    "        # for field in page.widgets():\n",
    "            # print(f'Widget on page: {page.number} with type: {field.type} and rect: {field.rect}')\n",
    "        text = page.get_text(\"html\")\n",
    "            # Use one of the following strings for opt to obtain different formats [2]:\n",
    "            # “text”: (default) plain text with line breaks. No formatting, no text position details, no images.\n",
    "            # “blocks”: generate a list of text blocks (= paragraphs).\n",
    "            # “words”: generate a list of words (strings not containing spaces).\n",
    "            # “html”: creates a full visual version of the page including any images. This can be displayed with your internet browser.\n",
    "            # “dict” / “json”: same information level as HTML, but provided as a Python dictionary or resp. JSON string. See TextPage.extractDICT() for details of its structure.\n",
    "            # “rawdict” / “rawjson”: a super-set of “dict” / “json”. It additionally provides character detail information like XML. See TextPage.extractRAWDICT() for details of its structure.\n",
    "            # “xhtml”: text information level as the TEXT version but includes images. Can also be displayed by internet browsers.\n",
    "            # “xml”: contains no images, but full position and font information down to each single text character. Use an XML module to interpret.\n",
    "        # with open(\"output.html\", \"w\") as f:\n",
    "        text = re.sub(r'<img[^>]*>', '<image>', text)\n",
    "        html.append(text)\n",
    "        with open(f\"analysis/hope1/f{i}.html\",'w',encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    " \n",
    "        # Font\n",
    "        # Alignment\n",
    "        # cases\n",
    "        #indentation, new pages, \n",
    "        # 1 1.0\n",
    "        #toc/index - \n",
    "        # blocks of code\n",
    "        # \n",
    "# for i in range(12)\n",
    "page= doc[0]\n",
    "text = page.get_text(\"html\")\n",
    "# pprint(text)\n",
    "print(doc[1].get_text(\"blocks\") == doc[1].get_text(\"blocks\"))\n",
    "htmx=(doc[0].get_text(\"html\"))\n",
    "with open (\"html.html\", \"w\") as f:\n",
    "    f.write(htmx)\n",
    "htmx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d236ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Depreciated \n",
    "def clean_pages_of_even_odd_repeats(dflist, even_df, odd_df, tolerance=1.0):\n",
    "    \"\"\"\n",
    "    Cleans even-indexed pages using even_df and odd-indexed pages using odd_df.\n",
    "    If even_df == odd_df, treats all pages the same.\n",
    "    \"\"\"\n",
    "    cleaned_pages = []\n",
    "    # Check if both are the same\n",
    "    same_repeats = ( set((r['text'].strip(), round(r['top'], 1)) for _, r in even_df.iterrows()) == set((r['text'].strip(), round(r['top'], 1)) for _, r in odd_df.iterrows()) )\n",
    "    for idx, df in enumerate(dflist):\n",
    "        if same_repeats:\n",
    "            compare_df = even_df  # or odd_df, doesn't matter\n",
    "        else:\n",
    "            compare_df = even_df if idx % 2 == 0 else odd_df\n",
    "        mask = df.apply(\n",
    "            lambda row: not any(\n",
    "                (abs(row['top'] - rep['top']) <= tolerance)\n",
    "                and (row['text'].strip() == rep['text'].strip())\n",
    "                for _, rep in compare_df.iterrows()\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        cleaned_df = df[mask].reset_index(drop=True)\n",
    "        cleaned_pages.append(cleaned_df)\n",
    "    return cleaned_pages\n",
    "\n",
    "def clean_pages_of_repeats(dflist, repeated_df, tolerance=1.0): ##################depreciated\n",
    "    \"\"\"\n",
    "    Removes rows from each page in dflist that match (in position and text) any row in repeated_df.\n",
    "    Useful for cleaning common headers/footers.\n",
    "    \"\"\"\n",
    "    cleaned_pages = []\n",
    "\n",
    "    for df in dflist:\n",
    "        mask = df.apply(\n",
    "            lambda row: not any(\n",
    "                (abs(row['top'] - rep['top']) <= tolerance) and (row['text'].strip() == rep['text'].strip())\n",
    "                for _, rep in repeated_df.iterrows()\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        cleaned_df = df[mask].reset_index(drop=True)\n",
    "        cleaned_pages.append(cleaned_df)\n",
    "\n",
    "    return cleaned_pages\n",
    "\n",
    "\n",
    "def find_repeating_rows(dflist, center_page, offsets=[1, 2], tolerance=1.0):#########depriciated\n",
    "    \"\"\"\n",
    "    Finds text lines (rows) in the center page that repeat in neighboring pages at given offsets.\n",
    "    Returns a DataFrame of repeated rows likely to be headers/footers.\n",
    "    \"\"\"\n",
    "    repeated_all = []\n",
    "    df_mid = dflist[center_page]\n",
    "\n",
    "    for offset in offsets:\n",
    "        before, after = center_page - offset, center_page + offset\n",
    "        if before < 0 or after >= len(dflist):\n",
    "            continue\n",
    "\n",
    "        df_before, df_after = dflist[before], dflist[after]\n",
    "\n",
    "        for _, row in df_mid.iterrows():\n",
    "            text, top = row['text'].strip(), row['top']\n",
    "\n",
    "            def is_match(df):\n",
    "                return any(\n",
    "                    (abs(top - r['top']) <= tolerance) and (r['text'].strip() == text)\n",
    "                    for _, r in df.iterrows()\n",
    "                )\n",
    "\n",
    "            if is_match(df_before) and is_match(df_after):\n",
    "                repeated_all.append(row)\n",
    "\n",
    "    # Drop duplicates and return as DataFrame\n",
    "    return pd.DataFrame(repeated_all).drop_duplicates(subset=[\"text\", \"top\", \"font_size\"])\n",
    "\n",
    "def find_alternate_page_repeats_splitold(dflist, mid):\n",
    "    def get_matches(df_base, df1, df2):\n",
    "        matched = []\n",
    "        for _, row in df_base.iterrows():\n",
    "            text = row['text'].strip()\n",
    "            top = row['top']\n",
    "\n",
    "            def match(df):\n",
    "                return any(\n",
    "                    (abs(top - r['top']) <= 1.0) and (r['text'].strip() == text)\n",
    "                    for _, r in df.iterrows()\n",
    "                )\n",
    "            if match(df1) and match(df2):\n",
    "                matched.append(row)\n",
    "        return pd.DataFrame(matched)\n",
    "    even_matches, odd_matches = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # Mid page even\n",
    "    if mid % 2 == 0:\n",
    "        page_even = dflist[mid]\n",
    "        before_even = dflist[mid - 2] if mid - 2 >= 0 else None\n",
    "        after_even = dflist[mid + 2] if mid + 2 < len(dflist) else None\n",
    "\n",
    "        if before_even is not None and after_even is not None:\n",
    "            even_matches = get_matches(page_even, before_even, after_even)\n",
    "        elif before_even is not None:\n",
    "            even_matches = get_matches(page_even, before_even)\n",
    "        elif after_even is not None:\n",
    "            even_matches = get_matches(page_even, after_even)\n",
    "\n",
    "    # Mid - 1 page odd\n",
    "    if mid - 1 >= 0 and (mid - 1) % 2 == 1:\n",
    "        odd_page_idx = mid - 1\n",
    "        page_odd = dflist[odd_page_idx]\n",
    "        before_odd = dflist[odd_page_idx - 2] if odd_page_idx - 2 >= 0 else None\n",
    "        after_odd = dflist[odd_page_idx + 2] if odd_page_idx + 2 < len(dflist) else None\n",
    "\n",
    "        if before_odd is not None and after_odd is not None:\n",
    "            odd_matches = get_matches(page_odd, before_odd, after_odd)\n",
    "        elif before_odd is not None:\n",
    "            odd_matches = get_matches(page_odd, before_odd)\n",
    "        elif after_odd is not None:\n",
    "            odd_matches = get_matches(page_odd, after_odd)\n",
    "    return even_matches, odd_matches\n",
    "\n",
    "def merge_paragraphs_by_font0(df, font_tolerance=1, line_gap=2.0):\n",
    "    \"\"\"\n",
    "    Groups consecutive lines with similar font size and small vertical gap into paragraphs.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): A single-page DataFrame containing 'top', 'font_size', 'text' columns.\n",
    "        font_tolerance (float): Allowed deviation in font size to consider lines as same style.\n",
    "        line_gap (float): Max vertical gap between lines to be considered part of the same paragraph.\n",
    "\n",
    "    Returns:\n",
    "        List of dicts: Each dict represents a merged paragraph block.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return []\n",
    "\n",
    "    # Sort by vertical position\n",
    "    # df_sorted = df.sort_values(by='top').reset_index(drop=True)\n",
    "    df_sorted = df\n",
    "    paragraphs = []\n",
    "    current_para = {\n",
    "        'top': df_sorted.loc[0, 'top'],\n",
    "        'font_size': df_sorted.loc[0, 'font_size'],\n",
    "        'text': df_sorted.loc[0, 'text']\n",
    "    }\n",
    "\n",
    "    for i in range(1, len(df_sorted)):\n",
    "        prev = df_sorted.loc[i - 1]\n",
    "        curr = df_sorted.loc[i]\n",
    "\n",
    "        same_font = (abs(curr['font_size'] - prev['font_size']) <= font_tolerance)\n",
    "        # close_enough = abs(curr['top'] - prev['top']) <= (prev['line_height'] + line_gap)\n",
    "\n",
    "        if same_font : #and close_enough:\n",
    "            current_para['text'] += ' ' + curr['text']\n",
    "        else:\n",
    "            paragraphs.append(current_para)\n",
    "            current_para = {\n",
    "                'top': curr['top'],\n",
    "                'font_size': curr['font_size'],\n",
    "                'text': curr['text']\n",
    "            }\n",
    "\n",
    "    paragraphs.append(current_para)\n",
    "    return \n",
    "\n",
    "\n",
    "def merge_consecutive_same_font_and_leftoldnew(dflist, target_fonts):\n",
    "    \"\"\"\n",
    "    Merges lines in-place within each DataFrame in dflist where:\n",
    "    - font_size ∈ target_fonts\n",
    "    - consecutive lines share same font_size and left\n",
    "    Adds a 'parabool' column to mark whether a line is a merged paragraph.\n",
    "    \n",
    "    Parameters:\n",
    "        dflist (list of pd.DataFrame): Each DataFrame is a page.\n",
    "        target_fonts (list): List of font sizes to consider for paragraph merging.\n",
    "    \n",
    "    Returns:\n",
    "        list of pd.DataFrame: Modified DataFrames with merged paragraphs and 'parabool' flag.\n",
    "    \"\"\"\n",
    "    updated_dflist = []\n",
    "\n",
    "    for df in dflist:\n",
    "        if df.empty or 'left' not in df.columns or 'font_size' not in df.columns:\n",
    "            df['parabool'] = False\n",
    "            updated_dflist.append(df)\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df['parabool'] = False\n",
    "        drop_indices = set()\n",
    "\n",
    "        for left_value in sorted(df['left'].unique()):\n",
    "            # Get indices where left matches\n",
    "            group_df = df[(df['left'] == left_value) & (df['font_size'].isin(target_fonts))]\n",
    "            indices = group_df.index.tolist()\n",
    "            i = 0\n",
    "\n",
    "            while i < len(indices):\n",
    "                current_idx = indices[i]\n",
    "                current_font = df.loc[current_idx, 'font_size']\n",
    "                current_text = df.loc[current_idx, 'text']\n",
    "\n",
    "                merged = False\n",
    "                j = i + 1\n",
    "\n",
    "                while j < len(indices):\n",
    "                    next_idx = indices[j]\n",
    "                    next_font = df.loc[next_idx, 'font_size']\n",
    "\n",
    "                    if next_font == current_font:\n",
    "                        # Merge text\n",
    "                        df.at[current_idx, 'text'] += ' ' + df.loc[next_idx, 'text']\n",
    "                        drop_indices.add(next_idx)\n",
    "                        merged = True\n",
    "                        j += 1\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                if merged:\n",
    "                    df.at[current_idx, 'parabool'] = True\n",
    "                else:\n",
    "                    df.at[current_idx, 'parabool'] = False\n",
    "\n",
    "                i = j  # skip merged rows\n",
    "\n",
    "        # Drop merged lines\n",
    "        df = df.drop(index=list(drop_indices)).reset_index(drop=True)\n",
    "        updated_dflist.append(df)\n",
    "\n",
    "    return updated_dflist\n",
    "\n",
    "\n",
    "def extract_table_with_titleold(page, max_title_distance=50): #################Old\n",
    "    \"\"\"\n",
    "    Extracts the first table and its potential title from a PyMuPDF page.\n",
    "\n",
    "    Parameters:\n",
    "        page (fitz.Page): The PDF page object.\n",
    "        max_title_distance (float): Max vertical distance (in pt) to search above the table for a title.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: The extracted table (if found), else None.\n",
    "            - str: The possible table title (if found), else None.\n",
    "    \"\"\"\n",
    "    tables = page.find_tables()\n",
    "    print(f\"{len(tables.tables)} table(s) found.\")\n",
    "\n",
    "    if not tables.tables:\n",
    "        return None, None\n",
    "\n",
    "    table = tables[0]\n",
    "    table_data = table.extract()\n",
    "    table_df = pd.DataFrame(table_data)\n",
    "\n",
    "    print(\"First table extracted:\")\n",
    "    pprint(table_data)\n",
    "\n",
    "    # Look for text just above the table\n",
    "    bbox = table.bbox\n",
    "    text_dict = page.get_text(\"dict\")\n",
    "    possible_titles = []\n",
    "\n",
    "    for block in text_dict.get(\"blocks\", []):\n",
    "        if \"lines\" in block and block[\"bbox\"][3] < bbox[1]:  # Above the table\n",
    "            if abs(block[\"bbox\"][3] - bbox[1]) <= max_title_distance:\n",
    "                text = \" \".join(\n",
    "                    span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"]\n",
    "                ).strip()\n",
    "                if text:\n",
    "                    possible_titles.append((block[\"bbox\"][1], text))\n",
    "\n",
    "    # Closest first\n",
    "    possible_titles.sort(key=lambda x: -x[0])\n",
    "\n",
    "    if possible_titles:\n",
    "        title = possible_titles[0][1]\n",
    "        print(\"Possible table title:\", title)\n",
    "    else:\n",
    "        title = None\n",
    "        print(\"No clear title found above the table.\")\n",
    "\n",
    "    return table_df, title\n",
    "\n",
    "\n",
    "def filter_and_clean_gibberish(df, text_column='text', min_alnum_ratio=0.1, min_length=1):\n",
    "    \"\"\"\n",
    "    Replaces gibberish parts of the text with spaces. If the entire string is gibberish,\n",
    "    the row is removed.\n",
    "    Replaces gibberish parts of the text with spaces. Keeps tokens like '2.1', 'a.', '1.' etc.\n",
    "    Removes purely symbolic tokens. Drops rows if nothing meaningful remains.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame with a text column.\n",
    "        text_column (str): Column name that contains the text.\n",
    "        min_alnum_ratio (float): Minimum ratio of alphanumeric chars to keep a token.\n",
    "        min_length (int): Minimum total length of cleaned string to keep the row.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with gibberish removed or cleaned.\n",
    "    \"\"\"\n",
    "\n",
    "    def is_meaningful(token):\n",
    "        token = token.strip()\n",
    "        if not token:\n",
    "            return False\n",
    "\n",
    "        # Keep tokens like: 2.1, a., 1., v2.0, etc.\n",
    "        if re.match(r'^[a-zA-Z0-9]+\\.$', token):     # a. or 1.\n",
    "            return True\n",
    "        if re.match(r'^[a-zA-Z]?[0-9]*\\.[0-9]+$', token):  # 2.1, 0.3, v2.0\n",
    "            return True\n",
    "\n",
    "        alnum_ratio = sum(c.isalnum() for c in token) / (len(token) + 1e-6)\n",
    "        return alnum_ratio >= min_alnum_ratio\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = str(text).strip()\n",
    "        if len(text) < min_length:\n",
    "            return None\n",
    "\n",
    "        # Split tokens but keep punctuation and symbols separated\n",
    "        tokens = re.split(r'(\\W+)', text)\n",
    "        cleaned_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if is_meaningful(token):\n",
    "                cleaned_tokens.append(token)\n",
    "            else:\n",
    "                cleaned_tokens.append(' ')\n",
    "\n",
    "        cleaned = ''.join(cleaned_tokens).strip()\n",
    "        if len(cleaned) < min_length or all(c.isspace() for c in cleaned):\n",
    "            return None\n",
    "        return cleaned\n",
    "\n",
    "    df[text_column] = df[text_column].apply(clean_text)\n",
    "    df = df[df[text_column].notna()].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_consecutive_same_font_and_leftv44(dflist, target_fonts, acceptable_spacing_range=[0.0, 2.0], min_words_required=3):\n",
    "    min_spacing, max_spacing = acceptable_spacing_range\n",
    "    updated_dflist = []\n",
    "\n",
    "    for df in dflist:\n",
    "        if df.empty or 'left' not in df.columns or 'font_size' not in df.columns:\n",
    "            df['parabool'] = False\n",
    "            updated_dflist.append(df)\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df['parabool'] = False\n",
    "        drop_indices = set()\n",
    "\n",
    "        for left_value in sorted(df['left'].unique()):\n",
    "            group_df = df[(df['left'] == left_value) & (df['font_size'].isin(target_fonts))]\n",
    "            indices = group_df.index.tolist()\n",
    "            i = 0\n",
    "\n",
    "            while i < len(indices):\n",
    "                current_idx = indices[i]\n",
    "                current_font = df.loc[current_idx, 'font_size']\n",
    "                current_bold = df.loc[current_idx, 'bold']\n",
    "                current_italic = df.loc[current_idx, 'italic']\n",
    "                current_top = df.loc[current_idx, 'top']\n",
    "                current_word_count = df.loc[current_idx, 'word_count']\n",
    "\n",
    "                j = i + 1\n",
    "                merged = False\n",
    "\n",
    "                while j < len(indices):\n",
    "                    next_idx = indices[j]\n",
    "                    next_font = df.loc[next_idx, 'font_size']\n",
    "                    next_bold = df.loc[next_idx, 'bold']\n",
    "                    next_italic = df.loc[next_idx, 'italic']\n",
    "                    next_top = df.loc[next_idx, 'top']\n",
    "\n",
    "                    if (\n",
    "                        next_font != current_font or\n",
    "                        (current_word_count > min_words_required and (\n",
    "                            next_bold != current_bold or\n",
    "                            next_italic != current_italic))\n",
    "                    ):\n",
    "                        break\n",
    "\n",
    "                    top_diff = abs(next_top - current_top)\n",
    "                    if not (min_spacing <= top_diff <= max_spacing):\n",
    "                        break\n",
    "\n",
    "                    df.at[current_idx, 'text'] += ' ' + df.loc[next_idx, 'text']\n",
    "                    df.at[current_idx, 'word_count'] += df.loc[next_idx, 'word_count']\n",
    "                    drop_indices.add(next_idx)\n",
    "                    merged = True\n",
    "                    j += 1\n",
    "                    current_top = next_top\n",
    "                    current_word_count = df.at[current_idx, 'word_count']\n",
    "\n",
    "                df.at[current_idx, 'parabool'] = merged\n",
    "                i = j\n",
    "\n",
    "        df = df.drop(index=list(drop_indices)).reset_index(drop=True)\n",
    "        updated_dflist.append(df)\n",
    "\n",
    "    return updated_dflist\n",
    "\n",
    "\n",
    "def merge_paragraphs_by_font(df, font_min=8.0, font_max=14.0, font_tolerance=1.0,line_gap=2.0): #maybe add a center align criteria\n",
    "    \"\"\"\n",
    "    Groups consecutive lines (in original order) with similar font size and small vertical gap into paragraphs.\n",
    "    No sorting is applied — assumes df is already in reading order.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame with 'top', 'font_size', 'line_height', 'text' columns.\n",
    "        font_min (float): Minimum font size to allow.\n",
    "        font_max (float): Maximum font size to allow.\n",
    "        font_tolerance (float): Allowed deviation in font size between lines.\n",
    "        line_gap (float): Maximum allowed vertical gap to group lines.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: List of merged paragraph dictionaries.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return []\n",
    "    # Filter font range\n",
    "    df = df[(df['font_size'] >= font_min) & (df['font_size'] <= font_max)].reset_index(drop=True)\n",
    "    if df.empty:\n",
    "        return []\n",
    "\n",
    "    paragraphs = []\n",
    "    current_para = {\n",
    "        'top': df.loc[0, 'top'],\n",
    "        'font_size': df.loc[0, 'font_size'],\n",
    "        'text': df.loc[0, 'text'],\n",
    "        'line_count': 1\n",
    "    }\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        prev = df.loc[i - 1]\n",
    "        curr = df.loc[i]\n",
    "    \n",
    "        same_font = abs(curr['font_size'] - prev['font_size']) <= font_tolerance\n",
    "        small_gap = abs(curr['top'] - prev['top']) <= (prev['line_height'] + line_gap)\n",
    "\n",
    "        if same_font and small_gap:\n",
    "            current_para['text'] += ' ' + curr['text']\n",
    "            current_para['line_count'] += 1\n",
    "        else:\n",
    "            paragraphs.append(current_para)\n",
    "            current_para = {\n",
    "                'top': curr['top'],\n",
    "                'font_size': curr['font_size'],\n",
    "                'text': curr['text'],\n",
    "                'line_count': 1\n",
    "            }\n",
    "\n",
    "    paragraphs.append(current_para)\n",
    "    return paragraphs\n",
    "\n",
    "def filter_gibberish_rows(df, text_column='text', min_alpha_ratio=0.1, min_length=1):\n",
    "    \"\"\"\n",
    "    Removes rows from df where the text is mostly non-alphabetic (e.g., --------, ...., ====),\n",
    "    or is too short to be meaningful.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame with a text column.\n",
    "        text_column (str): Column name that contains the text.\n",
    "        min_alpha_ratio (float): Minimum ratio of alphabetic chars to keep the row.\n",
    "        min_length (int): Minimum total length of string to keep it.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with gibberish lines removed.\n",
    "    \"\"\"\n",
    "    def is_gibberish(text):\n",
    "        if len(text.strip()) < min_length:\n",
    "            return True\n",
    "        alpha_count = sum(c.isalpha() for c in text)\n",
    "        return (alpha_count / len(text)) < min_alpha_ratio\n",
    "\n",
    "    mask = df[text_column].apply(lambda t: not is_gibberish(t))\n",
    "    return df[mask].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "def extract_table_with_titleold(page, max_title_distance=50): #################Old\n",
    "    \"\"\"\n",
    "    Extracts the first table and its potential title from a PyMuPDF page.\n",
    "\n",
    "    Parameters:\n",
    "        page (fitz.Page): The PDF page object.\n",
    "        max_title_distance (float): Max vertical distance (in pt) to search above the table for a title.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - pd.DataFrame: The extracted table (if found), else None.\n",
    "            - str: The possible table title (if found), else None.\n",
    "    \"\"\"\n",
    "    tables = page.find_tables()\n",
    "    print(f\"{len(tables.tables)} table(s) found.\")\n",
    "\n",
    "    if not tables.tables:\n",
    "        return None, None\n",
    "\n",
    "    table = tables[0]\n",
    "    table_data = table.extract()\n",
    "    table_df = pd.DataFrame(table_data)\n",
    "\n",
    "    print(\"First table extracted:\")\n",
    "    pprint(table_data)\n",
    "\n",
    "    # Look for text just above the table\n",
    "    bbox = table.bbox\n",
    "    text_dict = page.get_text(\"dict\")\n",
    "    possible_titles = []\n",
    "\n",
    "    for block in text_dict.get(\"blocks\", []):\n",
    "        if \"lines\" in block and block[\"bbox\"][3] < bbox[1]:  # Above the table\n",
    "            if abs(block[\"bbox\"][3] - bbox[1]) <= max_title_distance:\n",
    "                text = \" \".join(\n",
    "                    span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"]\n",
    "                ).strip()\n",
    "                if text:\n",
    "                    possible_titles.append((block[\"bbox\"][1], text))\n",
    "\n",
    "    # Closest first\n",
    "    possible_titles.sort(key=lambda x: -x[0])\n",
    "\n",
    "    if possible_titles:\n",
    "        title = possible_titles[0][1]\n",
    "        print(\"Possible table title:\", title)\n",
    "    else:\n",
    "        title = None\n",
    "        print(\"No clear title found above the table.\")\n",
    "\n",
    "    return table_df, title\n",
    "\n",
    "\n",
    "\n",
    "def merge_consecutive_same_font_and_leftv2(dflist, target_fonts):\n",
    "    \"\"\"\n",
    "    Merges lines within each DataFrame in dflist where:\n",
    "    - font_size ∈ target_fonts\n",
    "    - consecutive lines share same font_size, left alignment, and bold/italic state\n",
    "    Adds a 'parabool' column to mark whether a line is a merged paragraph.\n",
    "\n",
    "    Parameters:\n",
    "        dflist (list of pd.DataFrame): Each DataFrame is a page.\n",
    "        target_fonts (list): List of font sizes to consider for paragraph merging.\n",
    "\n",
    "    Returns:\n",
    "        list of pd.DataFrame: Modified DataFrames with merged paragraphs and 'parabool' flag.\n",
    "    \"\"\"\n",
    "    updated_dflist = []\n",
    "\n",
    "    for df in dflist:\n",
    "        if df.empty or 'left' not in df.columns or 'font_size' not in df.columns:\n",
    "            df['parabool'] = False\n",
    "            updated_dflist.append(df)\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df['parabool'] = False\n",
    "        drop_indices = set()\n",
    "\n",
    "        for left_value in sorted(df['left'].unique()):\n",
    "            group_df = df[(df['left'] == left_value) & (df['font_size'].isin(target_fonts))]\n",
    "            indices = group_df.index.tolist()\n",
    "            i = 0\n",
    "\n",
    "            while i < len(indices):\n",
    "                current_idx = indices[i]\n",
    "                current_font = df.loc[current_idx, 'font_size']\n",
    "                current_bold = df.loc[current_idx, 'bold']\n",
    "                current_italic = df.loc[current_idx, 'italic']\n",
    "\n",
    "                j = i + 1\n",
    "                merged = False\n",
    "\n",
    "                while j < len(indices):\n",
    "                    next_idx = indices[j]\n",
    "                    next_font = df.loc[next_idx, 'font_size']\n",
    "                    next_bold = df.loc[next_idx, 'bold']\n",
    "                    next_italic = df.loc[next_idx, 'italic']\n",
    "\n",
    "                    # Stop if font or bold/italic style mismatch\n",
    "                    if (\n",
    "                        next_font != current_font or\n",
    "                        next_bold != current_bold or\n",
    "                        next_italic != current_italic\n",
    "                    ):\n",
    "                        break\n",
    "\n",
    "                    # Merge text\n",
    "                    df.at[current_idx, 'text'] += ' ' + df.loc[next_idx, 'text']\n",
    "                    drop_indices.add(next_idx)\n",
    "                    merged = True\n",
    "                    j += 1\n",
    "\n",
    "                df.at[current_idx, 'parabool'] = merged\n",
    "                i = j\n",
    "\n",
    "        df = df.drop(index=list(drop_indices)).reset_index(drop=True)\n",
    "        updated_dflist.append(df)\n",
    "\n",
    "    return updated_dflist\n",
    "\n",
    "\n",
    "def merge_consecutive_same_font_and_leftv3(dflist, target_fonts, acceptable_spacing_range=[0.0, 2.0]):\n",
    "    \"\"\"\n",
    "    Merges lines within each DataFrame in dflist where:\n",
    "    - font_size ∈ target_fonts\n",
    "    - consecutive lines share same font_size, left alignment, and bold/italic state\n",
    "    - vertical spacing (top difference) is within acceptable_spacing_range\n",
    "\n",
    "    Adds:\n",
    "    - 'parabool' column: True if line is a merged paragraph.\n",
    "    - 'end' column: set to its own top if no merge, or the last merged line's top.\n",
    "\n",
    "    Parameters:\n",
    "        dflist (list of pd.DataFrame): Each DataFrame is a page.\n",
    "        target_fonts (list): Font sizes to consider for merging.\n",
    "        acceptable_spacing_range (list): [min_spacing, max_spacing] range to allow merging lines.\n",
    "\n",
    "    Returns:\n",
    "        list of pd.DataFrame: Modified DataFrames with merged paragraphs and 'parabool'/'end' flags.\n",
    "    \"\"\"\n",
    "    min_spacing, max_spacing = acceptable_spacing_range\n",
    "    updated_dflist = []\n",
    "\n",
    "    for df in dflist:\n",
    "        if df.empty or 'left' not in df.columns or 'font_size' not in df.columns:\n",
    "            df['parabool'] = False\n",
    "            df['end'] = df['top'] if 'top' in df.columns else 0\n",
    "            updated_dflist.append(df)\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df['parabool'] = False\n",
    "        df['end'] = df['top']\n",
    "        drop_indices = set()\n",
    "\n",
    "        for left_value in sorted(df['left'].unique()):\n",
    "            group_df = df[(df['left'] == left_value) & (df['font_size'].isin(target_fonts))]\n",
    "            indices = group_df.index.tolist()\n",
    "            i = 0\n",
    "\n",
    "            while i < len(indices):\n",
    "                current_idx = indices[i]\n",
    "                current_font = df.loc[current_idx, 'font_size']\n",
    "                current_bold = df.loc[current_idx, 'bold']\n",
    "                current_italic = df.loc[current_idx, 'italic']\n",
    "                current_top = df.loc[current_idx, 'top']\n",
    "                final_end = current_top\n",
    "\n",
    "                j = i + 1\n",
    "                merged = False\n",
    "\n",
    "                while j < len(indices):\n",
    "                    next_idx = indices[j]\n",
    "                    next_font = df.loc[next_idx, 'font_size']\n",
    "                    next_bold = df.loc[next_idx, 'bold']\n",
    "                    next_italic = df.loc[next_idx, 'italic']\n",
    "                    next_top = df.loc[next_idx, 'top']\n",
    "\n",
    "                    # Stop if font or style mismatch\n",
    "                    if (\n",
    "                        next_font != current_font or\n",
    "                        next_bold != current_bold or\n",
    "                        next_italic != current_italic\n",
    "                    ):\n",
    "                        break\n",
    "\n",
    "                    # Check top spacing constraint\n",
    "                    top_diff = abs(next_top - current_top)\n",
    "                    if not (min_spacing <= top_diff <= max_spacing):\n",
    "                        break\n",
    "\n",
    "                    # Merge text\n",
    "                    df.at[current_idx, 'text'] += ' ' + df.loc[next_idx, 'text']\n",
    "                    final_end = next_top\n",
    "                    drop_indices.add(next_idx)\n",
    "                    merged = True\n",
    "                    j += 1\n",
    "                    current_top = next_top  # update for cascading merge\n",
    "\n",
    "                df.at[current_idx, 'parabool'] = merged\n",
    "                df.at[current_idx, 'end'] = final_end\n",
    "                i = j\n",
    "\n",
    "        df = df.drop(index=list(drop_indices)).reset_index(drop=True)\n",
    "        updated_dflist.append(df)\n",
    "\n",
    "    return updated_dflist\n",
    "\n",
    "\n",
    "#spandan\n",
    "\n",
    "def merge_consecutive_same_font_and_leftv0(dflist, target_fonts):\n",
    "    \"\"\"\n",
    "    Merges lines in-place within each DataFrame in dflist where:\n",
    "    - font_size ∈ target_fonts\n",
    "    - consecutive lines share same font_size and left\n",
    "    - lines are NOT bold or italic\n",
    "    Adds a 'parabool' column to mark whether a line is a merged paragraph.\n",
    "\n",
    "    Parameters:\n",
    "        dflist (list of pd.DataFrame): Each DataFrame is a page.\n",
    "        target_fonts (list): List of font sizes to consider for paragraph merging.\n",
    "\n",
    "    Returns:\n",
    "        list of pd.DataFrame: Modified DataFrames with merged paragraphs and 'parabool' flag.\n",
    "    \"\"\"\n",
    "    updated_dflist = []\n",
    "\n",
    "    for df in dflist:\n",
    "        if df.empty or 'left' not in df.columns or 'font_size' not in df.columns:\n",
    "            df['parabool'] = False\n",
    "            updated_dflist.append(df)\n",
    "            continue\n",
    "\n",
    "        df = df.copy()\n",
    "        df['parabool'] = False\n",
    "        drop_indices = set()\n",
    "\n",
    "        for left_value in sorted(df['left'].unique()):\n",
    "            group_df = df[(df['left'] == left_value) & (df['font_size'].isin(target_fonts))]\n",
    "            indices = group_df.index.tolist()\n",
    "            i = 0\n",
    "\n",
    "            while i < len(indices):\n",
    "                current_idx = indices[i]\n",
    "                current_font = df.loc[current_idx, 'font_size']\n",
    "\n",
    "                # Skip bold/italic rows\n",
    "                if df.loc[current_idx, 'bold'] : #or df.loc[current_idx, 'italic']: #ignore the italics for now\n",
    "                    df.at[current_idx, 'parabool'] = False\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                j = i + 1\n",
    "                merged = False\n",
    "\n",
    "                while j < len(indices):\n",
    "                    next_idx = indices[j]\n",
    "                    next_font = df.loc[next_idx, 'font_size']\n",
    "\n",
    "                    # Stop if font differs or next is bold/italic\n",
    "                    if next_font != current_font or df.loc[next_idx, 'bold'] or df.loc[next_idx, 'italic']:\n",
    "                        break\n",
    "\n",
    "                    # Merge\n",
    "                    df.at[current_idx, 'text'] += ' ' + df.loc[next_idx, 'text']\n",
    "                    drop_indices.add(next_idx)\n",
    "                    merged = True\n",
    "                    j += 1\n",
    "\n",
    "                df.at[current_idx, 'parabool'] = merged\n",
    "                i = j\n",
    "\n",
    "        # Drop merged rows\n",
    "        df = df.drop(index=list(drop_indices)).reset_index(drop=True)\n",
    "        updated_dflist.append(df)\n",
    "\n",
    "    return updated_dflist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ec8f68fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div id=\"page0\" style=\"width:612.0pt;height:792.0pt\">\n",
      "<p style=\"top:94.6pt;left:90.0pt;line-height:20.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:20.0pt;color:#000000\">Ontario&#x2019;s Digital Library </span></p>\n",
      "<p style=\"top:122.0pt;left:90.0pt;line-height:16.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:16.0pt;color:#000000\">A Critical Component for Implementing Ontario&#x2019;s Road Map to </span></p>\n",
      "<p style=\"top:140.3pt;left:90.0pt;line-height:16.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:16.0pt;color:#000000\">Prosperity Strategy </span></p>\n",
      "<p style=\"top:199.3pt;left:96.0pt;line-height:12.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:12.0pt;color:#000000\">Summary </span></p>\n",
      "<p style=\"top:229.7pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">The purpose of this </span><b><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Request for Proposal</span></b><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\"> (</span><b><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">RFP</span></b><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">) is to invite firms and/or consultants to </span></p>\n",
      "<p style=\"top:242.3pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">present a proposal for developing the business plan for the </span><b><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Ontario Digital Library (ODL). </span></b></p>\n",
      "<p style=\"top:255.0pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">The ODL will deliver high-quality library electronic content to all Ontario residents in order to </span></p>\n",
      "<p style=\"top:267.8pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">assist people as they learn, work, and enhance their quality of life. The business plan to be </span></p>\n",
      "<p style=\"top:280.4pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">developed is to document and clearly communicate: </span></p>\n",
      "<p style=\"top:305.1pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">how the ODL will be implemented, including the timeline </span></p>\n",
      "<p style=\"top:317.7pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">the financial plan for the implementation </span></p>\n",
      "<p style=\"top:330.3pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">the financial plan for the first 2 operating years, including capital and operating costs, </span></p>\n",
      "<p style=\"top:343.0pt;left:132.6pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">revenues, etc. </span></p>\n",
      "<p style=\"top:355.6pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">a financial forecast for the succeeding 2 operating years </span></p>\n",
      "<p style=\"top:368.3pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">the services and products to be delivered by the ODL </span></p>\n",
      "<p style=\"top:380.9pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">how the ODL will operate and be managed following the implementation </span></p>\n",
      "<p style=\"top:393.5pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">who will be involved, and what their role/responsibility will be, for both the </span></p>\n",
      "<p style=\"top:406.2pt;left:132.6pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">implementation and operational stages </span></p>\n",
      "<p style=\"top:418.8pt;left:132.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">the marketing and communications plan for the ODL </span></p>\n",
      "<p style=\"top:444.1pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">This business plan must be completed and approved by the ODL Steering Committee no </span></p>\n",
      "<p style=\"top:456.7pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">later than September 30, 2003 </span></p>\n",
      "<p style=\"top:481.5pt;left:96.0pt;line-height:11.0pt\"><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Timeline: </span></i></p>\n",
      "<p style=\"top:506.0pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Those firms/consultants intended to submit a proposal to this RFP must indicate their </span></p>\n",
      "<p style=\"top:518.8pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">intention to do so in an e-mail to Michael Ridley (</span><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#0000ff\">mridley@uoguelph.ca</span><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">) by </span><b><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">April 11</span></i></b><sup><b><i><span style=\"font-family:Arial,sans-serif;font-size:7.0pt;color:#000000\">th</span></i></b></sup><b><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">. </span></i></b></p>\n",
      "<p style=\"top:543.4pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Proposals may be e-mailed, mailed, couriered or faxed to: Larry Moore </span></p>\n",
      "<p style=\"top:556.0pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">(</span><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#0000ff\">lmoore@accessola.com</span><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">), Executive Director, The Ontario Library Association, </span><span style=\"font-family:Arial,sans-serif;font-size:10.0pt;color:#000000\">100 Lombard </span></p>\n",
      "<p style=\"top:569.6pt;left:96.0pt;line-height:10.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:10.0pt;color:#000000\">St., Suite 303, Toronto, ON  M5C 1M3. </span><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Proposals must be received by </span><b><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Noon on Monday, </span></i></b></p>\n",
      "<p style=\"top:581.3pt;left:96.0pt;line-height:11.0pt\"><b><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">April 21, 2003. </span></i></b></p>\n",
      "<p style=\"top:606.0pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Those proposals that are short-listed will be invited to discuss their proposal during the week </span></p>\n",
      "<p style=\"top:618.6pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">of </span><b><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">April 28, 2003</span></i></b><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">. No presentation will be expected. Firms or consultants invited to an </span></p>\n",
      "<p style=\"top:631.4pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">interview will be expected to discuss the project and their approach with the selection </span></p>\n",
      "<p style=\"top:644.0pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">committee</span><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">. </span></i></p>\n",
      "<p style=\"top:668.6pt;left:96.0pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">Contracts with the firm/consultant will be signed the week of </span><b><i><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">May 5, 2003</span></i></b><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\"> with the work to </span></p>\n",
      "<p style=\"top:681.3pt;left:96.1pt;line-height:11.0pt\"><span style=\"font-family:Arial,sans-serif;font-size:11.0pt;color:#000000\">commence as soon as possible thereafter. </span></p>\n",
      "<p style=\"top:734.8pt;left:90.0pt;line-height:7.6pt\"><b><i><span style=\"font-family:Arial,sans-serif;font-size:7.6pt;color:#000000\">RFP: To Develop the Ontario Digital Library Business Plan</span></i></b></p>\n",
      "<p style=\"top:734.8pt;left:296.7pt;line-height:7.6pt\"><b><i><span style=\"font-family:Arial,sans-serif;font-size:7.6pt;color:#000000\"> March 2003 </span></i></b></p>\n",
      "<p style=\"top:733.7pt;left:516.6pt;line-height:9.0pt\"><i><span style=\"font-family:Arial,sans-serif;font-size:9.0pt;color:#000000\">2</span></i><span style=\"font-family:Times New Roman,serif;font-size:9.0pt;color:#000000\"> </span></p>\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page = doc[1]\n",
    "print(page.get_text('html'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
